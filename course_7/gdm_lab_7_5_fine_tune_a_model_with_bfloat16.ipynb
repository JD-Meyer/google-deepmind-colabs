{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X3cgmmhcGXX"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C6-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm5kp5gPcQgk"
      },
      "source": [
        "# Lab: Fine-Tune a Model with bfloat16\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_7/gdm_lab_7_5_fine_tune_a_model_with_bfloat16.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Learn how to successfully train a large model on a single GPU using the bfloat16 data type.\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acC8A1A0crE2"
      },
      "source": [
        "##Overview\n",
        "\n",
        "In the previous fine-tuning lab \"Hitting a Wall,\" you encountered an out-of-memory error when attempting to load Gemma-4B on a standard 16GB GPU.\n",
        "\n",
        "In this lab, you will learn how to load the model parameters using the bfloat16 data type. Combining this technique with LoRA, you can then fine-tune a 4B model on the GPU that is available to you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykoxsIkLctnM"
      },
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Implement the loading of parameters in bfloat16 in Keras.\n",
        "* Explain how bfloat16 significantly reduces the memory footprint of a model.\n",
        "* Fine-tune a large model on a resource-constrained GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGHgDenJcuJT"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "**In this lab, you will**:\n",
        "\n",
        "* Load the Gemma-4B model using bfloat16 precision.\n",
        "* Fine-tune the model to produce flashcards as in course 05 Fine-tune Your Model.\n",
        "* Reflect on the performance of the fine-tuned model.\n",
        "\n",
        "**This lab needs to be run on a GPU. Choose a T4 GPU.** See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this.\n",
        "\n",
        "**You also need a Kaggle account** to download the weights of the Gemma 3 model. See the section \"Set up a Kaggle account\" for instructions on how to do this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are excuted on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the **order in which you run the cells matters**. When you are working through a lab, make sure to always run all cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose _Runtime â†’ Run before_  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click on **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware accelerator**, select **T4 GPU**.\n",
        "4.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running."
      ],
      "metadata": {
        "id": "kN3yE5jSg9zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up a Kaggle account\n"
      ],
      "metadata": {
        "id": "n6-dSJ-o_JT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To run this notebook, you will have to sign up for [Kaggle](https://www.kaggle.com), a platform that hosts datasets and models for machine learning. You will also need to sign the agreement for using the Gemma 3 model. This is required so that you can download the weights of the Gemma model for fine-tuning.\n",
        "\n",
        "### Step 1: Create your Kaggle account\n",
        "\n",
        "* Go to the Kaggle website: https://www.kaggle.com\n",
        "\n",
        "* Click the \"Register\" button in the top-right corner.\n",
        "\n",
        "* You can sign up using your Google account (recommended for easy Colab integration) or by entering an email and password.\n",
        "\n",
        "* Follow the on-screen prompts to complete your registration and verify your email.\n",
        "\n",
        "### Step 2: Sign the Gemma 3 model agreement\n",
        "\n",
        "* Make sure you are logged into your new Kaggle account.\n",
        "\n",
        "* Go directly to the Gemma 3 model card page: https://www.kaggle.com/models/keras/gemma3/keras/\n",
        "\n",
        "* You should see a \"Request Access\" button.\n",
        "\n",
        "* Click the button, read through the license agreement, and if you are happy to click \"Accept\" to gain access to the model. You must do this before the API will let you download the model.\n",
        "\n",
        "### Step 3: Generate your Kaggle API key\n",
        "\n",
        "* From any Kaggle page, click on your profile picture or icon in the top-right corner.\n",
        "\n",
        "* Select \"Account\" from the drop-down menu.\n",
        "\n",
        "* Scroll down to the \"API\" section.\n",
        "\n",
        "* Click the \"Create New API Token\" button.\n",
        "\n",
        "* This will immediately download a file named `kaggle.json` to your computer. This file contains your username and your secret API key. Keep it safe.\n",
        "\n",
        "### Step 4: Set your API Key in  Colab\n",
        "\n",
        "* Click the \"key\" icon ðŸ”‘ in the left-hand sidebar.\n",
        "\n",
        "* You will see the \"Secrets\" panel.\n",
        "\n",
        "* Now, open the kaggle.json file you downloaded on your computer. It's a simple text file and will look like this:\n",
        "\n",
        "   ```json\n",
        "   {\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_API_KEY\"}\n",
        "   ```\n",
        "* In the Colab Secrets panel, create two new secrets:\n",
        "\n",
        "   1. Name: `KAGGLE_USERNAME`\n",
        "\n",
        "      Value: Copy and paste `YOUR_KAGGLE_USERNAME` from your `kaggle.json` file.\n",
        "\n",
        "   2. Name: `KAGGLE_KEY`\n",
        "\n",
        "      Value: Copy and paste `YOUR_KAGGLE_API_KEY` from your `kaggle.json` file.\n",
        "\n",
        "* For both secrets, make sure the \"Notebook access\" toggle is switched on.\n"
      ],
      "metadata": {
        "id": "9iS70YEC_Hx8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrxGIe_mc3Uo"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will use the Keras package for loading and fine-tuning a Gemma model and the Pandas package, for loading the dataset.\n",
        "\n",
        "Run the following cell to import the required packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrgQ_n_ldZde"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import os # For setting system variables.\n",
        "\n",
        "from google.colab import userdata # For using Colab secrets.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Set the Keras backend to JAX.\n",
        "\n",
        "# Disable the command buffer pre-allocation to free up memory.\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_enable_command_buffer=\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.9\"\n",
        "\n",
        "import keras # For defining and training models.\n",
        "import keras_nlp # For loading the Keras implementation of Gemma.\n",
        "import pandas as pd # For loading the dataset.\n",
        "from textwrap import fill # For formatting long paragraphs.\n",
        "from ai_foundations import formatting # For formatting the training data.\n",
        "\n",
        "keras.utils.set_random_seed(812)  # For Keras layers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and process data"
      ],
      "metadata": {
        "id": "5my4qxRUihwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in previous labs, the following cell defines a function `format_question` that formats a prompt. It also loads the Africa Galore QA dataset and processes the individual questions so that the data can be used to fine-tune a model to generate answers for flashcards, as in previous courses."
      ],
      "metadata": {
        "id": "XngEInKtii-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QZLk2s3dxuo"
      },
      "outputs": [],
      "source": [
        "def format_question(\n",
        "    question: str,\n",
        "    sot = \"<start_of_turn>\",\n",
        "    eot = \"<end_of_turn>\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Formats a question for prompting the model and adds special delimiters at\n",
        "    the start and end of the question.\n",
        "\n",
        "    Args:\n",
        "      text: The question to be formatted.\n",
        "      sot: The token to mark the start of a turn.\n",
        "      eot: The token to mark the end of a turn.\n",
        "\n",
        "    Returns:\n",
        "      Formatted string of the question.\n",
        "    \"\"\"\n",
        "\n",
        "    formatted_q = f\"{sot}user\\n{question}{eot}\\n\"\n",
        "\n",
        "    return formatted_q\n",
        "\n",
        "# Load the question-answer dataset.\n",
        "africa_galore_qa = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_qa_v2.json\"\n",
        ")\n",
        "\n",
        "questions = []  # List of formatted questions.\n",
        "answers = []  # List of formatted answers.\n",
        "\n",
        "for idx, row in africa_galore_qa.iterrows():\n",
        "    # Run the format_qa function from the previous lab to format the question\n",
        "    # and the answer.\n",
        "    question, answer = formatting.format_qa(row)\n",
        "    questions.append(question)\n",
        "    answers.append(answer)\n",
        "\n",
        "# Show the first set of input and output.\n",
        "print(questions[0])\n",
        "print(fill(answers[0], replace_whitespace=False))\n",
        "\n",
        "# Prepare the data dictionary for fine-tuning Gemma.\n",
        "data = {\n",
        "    \"prompts\": questions,\n",
        "    \"responses\": answers\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0CZHtCvh5JC"
      },
      "source": [
        "## Coding Activity 1: Load Gemma-4B parameters\n",
        "\n",
        "In the previous fine-tuning lab \"Hitting a Wall,\" the attempt to load Gemma-4B failed because the model's parameters (stored in the default 32-bit format) were too large to fit in the GPU's memory.\n",
        "\n",
        "The solution is to load the model using a more memory-efficient number format. You will use bfloat16, which uses 16 bits instead of 32, effectively halving the memory required for the model's weights.\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        "> In the following cell, load the Gemma-4B model parameters in the `bfloat16` format.\n",
        ">\n",
        "> In Keras, you can load the parameters as `bfloat16` as follows:\n",
        ">\n",
        "> ```\n",
        "> model = keras_nlp.models.Gemma3CausalLM.from_preset(preset=<MODEL_NAME>, dtype=\"bfloat16\")\n",
        "> ```\n",
        ">\n",
        "> For the Gemma-4B model, the `<MODEL_NAME>` value is `\"gemma3_4b_text\"`.\n",
        "------\n",
        "\n",
        "<br>\n",
        "\n",
        "It should take 2-3 minutes to load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaCHPQyyb68v"
      },
      "outputs": [],
      "source": [
        "# Load the Gemma3-4B Keras model with bfloat16 precision.\n",
        "model = ... # Add your code here.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model has been loaded in with bfloat16, inspect some of the model weights to verify that its parameters are represented as bfloat16 by running the following cell."
      ],
      "metadata": {
        "id": "e7Ujtedk99Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the first transformer block.\n",
        "first_transformer_block = model.backbone.get_layer(\"decoder_block_0\")\n",
        "\n",
        "# Access the attention layer.\n",
        "attention_layer = first_transformer_block.attention\n",
        "\n",
        "# Get the weight matrix for the query projections, and check its dtype.\n",
        "query_weights = attention_layer.query_dense.kernel\n",
        "\n",
        "print(f\"The model's weight precision is {query_weights.dtype}.\")"
      ],
      "metadata": {
        "id": "6v9ZmF2l-GD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NG5SBs9eOd_"
      },
      "source": [
        "### Activate LoRA\n",
        "\n",
        "Even when representing model parameters as bfloat16, you will not be able to perform full-parameter fine-tuning on the available GPU. You will therefore again have to train the model using LoRA. Recall that in course 05 Fine-Tune Your Model, when fine-tuning with LoRA, only the parameters of the low-rank matrices were updated during training. These low-rank matrices constitute a small fraction of the total parameters. This means that the GPU memory needs to store gradients, activations, and optimizer states for only a small number of parameters. This drastically reduces the overall memory requirement. The lower memory requirement is why you can fine-tune a 4B model on a T4 GPU.\n",
        "\n",
        "Run the following code to enable LoRA for Gemma-4B with a rank of 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDGMgiI9eNir"
      },
      "outputs": [],
      "source": [
        "model.backbone.enable_lora(rank=4)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Gemma-4B with bfloat16"
      ],
      "metadata": {
        "id": "_I32dlOBk73-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that LoRA is enabled, you need to configure the training process. This involves setting the hyperparameters that will guide the learning.\n",
        "\n",
        "Training with a low-precision format like bfloat16 often requires more careful hyperparameter selection than standard 32-bit training. As the numbers are less precise, the learning process can be more sensitive, so it is important to use settings that result in a stable training process.\n",
        "\n",
        "The following cell below sets up the AdamW optimizer with a set of values that have been found to work well for stable bfloat16 fine-tuning. Run the following cell to configure your model for training."
      ],
      "metadata": {
        "id": "w13gZ5ct-ySr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs2VVQA0eZaH"
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters.\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.95,\n",
        "    epsilon=1e-6,\n",
        "    clipnorm=0.5,\n",
        ")\n",
        "\n",
        "# Determine the number of epochs.\n",
        "num_epochs = 4\n",
        "\n",
        "# Set the maximum length.\n",
        "model.preprocessor.sequence_length = 400\n",
        "\n",
        "# Compile the optimizer.\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor training progress\n",
        "\n",
        "As for previous fine-tuning runs, you need a method to evaluate the training progress beyond the loss. As before, you will do this by sampling generations from the model after each epoch.\n",
        "\n",
        "The following cell creates a Keras callback. This is a helper function that will automatically run code at the end of each epoch. In this case, it will generate and print outputs for three specific test prompts. This allows you to monitor the model's progress in real-time.\n",
        "\n",
        "The three test prompts are designed to check different aspects of learning:\n",
        "\n",
        "* \"What is Kente cloth?\": This tests if the model is learning the specific knowledge from the fine-tuning dataset.\n",
        "* \"What is Kilimanjaro?\": This tests generalization, as \"Mount Kilimanjaro\" features in the fine-tuning dataset but the word \"Mount\" is omitted here to make it more difficult.\n",
        "* \"What is Tokyo?\": Since there is nothing about Tokyo or Japan in the fine-tuning dataset, this checks if the model retains its pre-trained knowledge while learning the new task.\n",
        "\n",
        "Run the following cell to define this callback."
      ],
      "metadata": {
        "id": "ilPyfAtJAeiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of prompts you want to check after each epoch.\n",
        "test_prompts = [\n",
        "    \"What is Kente cloth?\",\n",
        "    \"What is Kilimanjaro?\",\n",
        "    \"What is Tokyo?\"\n",
        "]\n",
        "\n",
        "class GenerationMonitor(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"\\n--- Generations after epoch {epoch + 1} ---\")\n",
        "        for prompt in test_prompts:\n",
        "            # Format the prompt correctly for the model.\n",
        "            formatted_prompt = format_question(prompt)\n",
        "\n",
        "            # Generate and print the output.\n",
        "            output = self.model.generate(formatted_prompt, max_length=150)\n",
        "            print(output)\n",
        "            print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "lpcH6wAAe5wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the following cells to fine-tune the model using LoRA and the bfloat16 number format. Notice how this does not lead to an out-of-memory error.\n",
        "\n",
        "Compare the outputs after each epoch. How do they look?\n",
        "\n",
        "The training will take about five minutes per epoch on a T4 GPU. In total, it should take around 20 minutes for four epochs."
      ],
      "metadata": {
        "id": "vu3hD1lzk-uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the monitoring callback.\n",
        "generation_callback = GenerationMonitor()\n",
        "\n",
        "# Train the model.\n",
        "history = model.fit(\n",
        "    data,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=1,\n",
        "    callbacks=[generation_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "c-7F17BLe6YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you observe?\n",
        "\n",
        "In the previous fine-tuning lab \"Hitting a Wall,\" already attempting to load the model with full 32-bit precision resulted in an immediate out-of-memory error. Now, by making a single change - that is, loading the model with bfloat16 instead of 32-bit floating point numbers - you can not only load the model but also fine-tune it with LoRA.\n",
        "\n",
        "The output of the training process also shows how the model learned to respond to the questions in the desired flashcard format after training for a few epochs. Notice the initial instability in Epoch 1. For the \"Kente\" prompt, the model generates a repeated, random token (\"KMnO4\"). For the \"Tokyo\" prompt, the model keeps generating \"Category:\" lists. However, after this, the generations improve dramatically. The model produces increasingly coherent text matching the flashcard format. The final generations in Epoch 4 are high-quality, factually accurate, and stylistically correct. This shows that using bfloat16 does not compromise the quality of the trained model.\n",
        "\n",
        "You have now successfully trained a 4-billion parameter model on a single GPU, a task that was impossible with standard precision. You have also seen that the final generations are high-quality, indicating that bfloat16 did not significantly harm the model's performance on this task."
      ],
      "metadata": {
        "id": "xB9ae9oAlo9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Loading the model parameters with a lower precision number format like bfloat16 allows you to use and fine-tune bigger models with limited GPU resources. As you observed in this lab, by using bfloat16 representations, you were able to load all parameters of the Gemma-4B into memory and even fine-tune the model on a T4 GPU available through Colab. This resulted in a model that is able to generate high-quality responses thanks to the powerful 4-billion parameter foundation model that served as the basis for fine-tuning."
      ],
      "metadata": {
        "id": "IXwi82sWlpJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ],
      "metadata": {
        "id": "OChZEs7AZBxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "D_rtDcZ9ZIWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras_nlp.models.Gemma3CausalLM.from_preset(\"gemma3_4b_text\", dtype=\"bfloat16\")"
      ],
      "metadata": {
        "id": "_B8aCZuAZCSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "n6-dSJ-o_JT9",
        "D_rtDcZ9ZIWg"
      ],
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
