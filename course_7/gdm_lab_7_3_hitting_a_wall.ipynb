{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "n6-dSJ-o_JT9"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C6-white-bg.png\">"
      ],
      "metadata": {
        "id": "O6E5AAk2l8c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Hitting a Wall\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_7/gdm_lab_7_3_hitting_a_wall.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Investigate the hardware limitations of training large models and experience first-hand out-of-memory errors.\n",
        "\n",
        "15 minutes"
      ],
      "metadata": {
        "id": "5cNGZ1qSl9Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Earlier in this course, you saw that larger models like Gemma-4B can produce higher-quality generations than smaller models like Gemma-1B. What would it take to fine-tune a large model like Gemma-4B yourself?\n",
        "\n",
        "In the previous course, you successfully fine-tuned Gemma-1B using LoRA. In this lab, you will attempt to apply the same fine-tuning process to the larger Gemma-4B model. This exercise is designed to give you a first-hand experience of the \"memory wall\". This describes a critical hardware limitation, which, as you will observe, motivates the need for the efficiency techniques you are learning in this course."
      ],
      "metadata": {
        "id": "4_3eiCnvptt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Describe why even with LoRA you cannot fine-tune bigger models.\n",
        "\n",
        "* The motivation for using memory-saving techniques."
      ],
      "metadata": {
        "id": "hz1L39mgrBuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks\n"
      ],
      "metadata": {
        "id": "RBrY9KVfrPV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this lab, you will**:\n",
        "\n",
        "* Attempt to fine-tune the base Gemma-4B model.\n",
        "\n",
        "* Encounter and reflect on the resulting out-of-memory (OOM) error.\n",
        "\n",
        " **This lab needs to be run on a GPU. Choose a T4 GPU.** See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this.\n",
        "\n",
        " **You also need a Kaggle account** to download the weights of the Gemma 3 model. See the section \"Set up a Kaggle account\" for instructions on how to do this.\n"
      ],
      "metadata": {
        "id": "_PE2ozNbrYvy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are excuted on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the **order in which you run the cells matters**. When you are working through a lab, make sure to always run all cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose _Runtime â†’ Run before_  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click on **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware accelerator**, select **T4 GPU**.\n",
        "4.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running."
      ],
      "metadata": {
        "id": "kN3yE5jSg9zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up a Kaggle account\n"
      ],
      "metadata": {
        "id": "n6-dSJ-o_JT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To run this notebook, you will have to sign up for [Kaggle](https://www.kaggle.com), a platform that hosts datasets and models for machine learning. You will also need to sign the agreement for using the Gemma 3 model. This is required so that you can download the weights of the Gemma model for fine-tuning.\n",
        "\n",
        "### Step 1: Create your Kaggle account\n",
        "\n",
        "* Go to the Kaggle website: https://www.kaggle.com\n",
        "\n",
        "* Click the \"Register\" button in the top-right corner.\n",
        "\n",
        "* You can sign up using your Google account (recommended for easy Colab integration) or by entering an email and password.\n",
        "\n",
        "* Follow the on-screen prompts to complete your registration and verify your email.\n",
        "\n",
        "### Step 2: Sign the Gemma 3 model agreement\n",
        "\n",
        "* Make sure you are logged into your new Kaggle account.\n",
        "\n",
        "* Go directly to the Gemma 3 model card page: https://www.kaggle.com/models/keras/gemma3/keras/\n",
        "\n",
        "* You should see a \"Request Access\" button.\n",
        "\n",
        "* Click the button, read through the license agreement, and if you are happy with the terms click \"Accept\" to gain access to the model. You must do this before the API will let you download the model.\n",
        "\n",
        "### Step 3: Generate your Kaggle API key\n",
        "\n",
        "* From any Kaggle page, click on your profile picture or icon in the top-right corner.\n",
        "\n",
        "* Select \"Account\" from the drop-down menu.\n",
        "\n",
        "* Scroll down to the \"API\" section.\n",
        "\n",
        "* Click the \"Create New API Token\" button.\n",
        "\n",
        "* This will immediately download a file named `kaggle.json` to your computer. This file contains your username and your secret API key. Keep it safe.\n",
        "\n",
        "### Step 4: Set your API Key in  Colab\n",
        "\n",
        "* Click the \"key\" icon ðŸ”‘ in the left-hand sidebar.\n",
        "\n",
        "* You will see the \"Secrets\" panel.\n",
        "\n",
        "* Now, open the kaggle.json file you downloaded on your computer. It is a simple text file and will look like this:\n",
        "\n",
        "   ```json\n",
        "   {\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_API_KEY\"}\n",
        "   ```\n",
        "* In the Colab Secrets panel, create two new secrets:\n",
        "\n",
        "   1. Name: `KAGGLE_USERNAME`\n",
        "\n",
        "      Value: Copy and paste `YOUR_KAGGLE_USERNAME` from your `kaggle.json` file.\n",
        "\n",
        "   2. Name: `KAGGLE_KEY`\n",
        "\n",
        "      Value: Copy and paste `YOUR_KAGGLE_API_KEY` from your `kaggle.json` file.\n",
        "\n",
        "* For both secrets, make sure the \"Notebook access\" toggle is switched on.\n"
      ],
      "metadata": {
        "id": "9iS70YEC_Hx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will use the Keras package for loading a Gemma model and the Pandas package, for loading the dataset.\n",
        "\n",
        "Run the following cell to import the required packages.\n",
        "\n"
      ],
      "metadata": {
        "id": "ynNQ7wwks3v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import os # For setting system variables.\n",
        "\n",
        "from google.colab import userdata # For using Colab secrets.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Set the Keras backend to JAX.\n",
        "# Disables the command buffer pre-allocation to free up memory.\n",
        "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_enable_command_buffer=\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"0.9\"\n",
        "\n",
        "import keras # For defining and training models.\n",
        "import keras_nlp # For loading the Keras implementation of Gemma.\n",
        "import pandas as pd # For loading the dataset.\n",
        "from textwrap import fill # For formatting long paragraphs.\n",
        "from ai_foundations import formatting # For formatting the training data.\n",
        "\n",
        "keras.utils.set_random_seed(812)  # For Keras layers."
      ],
      "metadata": {
        "id": "TfUQTROPs-a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Gemma-4B\n",
        "\n",
        "In the previous course, you learned to fine-tune Gemma-1B using LoRA for building a revison study flashcard generator. In this lab, you will apply the same fine-tuning techniques to fine-tune Gemma-4B.\n",
        "\n",
        "Run the following cell to:\n",
        "\n",
        "* Define the `format_question` function to format model inputs.\n",
        "* Prepare the Africa Galore dataset for fine-tuning."
      ],
      "metadata": {
        "id": "y7WNKIVWs1Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_question(\n",
        "    question: str,\n",
        "    sot = \"<start_of_turn>\",\n",
        "    eot = \"<end_of_turn>\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Formats a question for prompting the model and adds special delimiters at\n",
        "    the start and end of the question.\n",
        "\n",
        "    Args:\n",
        "      text: The question to be formatted.\n",
        "      sot: The token to mark the start of a turn.\n",
        "      eot: The token to mark the end of a turn.\n",
        "\n",
        "    Returns:\n",
        "      Formatted string of the question.\n",
        "    \"\"\"\n",
        "\n",
        "    formatted_q = f\"{sot}user\\n{question}{eot}\\n\"\n",
        "\n",
        "    return formatted_q\n",
        "\n",
        "# Load the question-answer dataset.\n",
        "africa_galore_qa = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_qa_v2.json\"\n",
        ")\n",
        "\n",
        "questions = []  # List of formatted questions.\n",
        "answers = []  # List of formatted answers.\n",
        "\n",
        "for idx, row in africa_galore_qa.iterrows():\n",
        "    # Run the format_qa function from the previous lab to format the question\n",
        "    # and the answer.\n",
        "    question, answer = formatting.format_qa(row)\n",
        "    questions.append(question)\n",
        "    answers.append(answer)\n",
        "\n",
        "# Show the first set of input and output.\n",
        "print(questions[0])\n",
        "print(fill(answers[0], replace_whitespace=False))\n",
        "\n",
        "# Prepare the data dictionary for fine-tuning Gemma.\n",
        "data = {\n",
        "    \"prompts\": questions,\n",
        "    \"responses\": answers\n",
        "}"
      ],
      "metadata": {
        "id": "Chd0kZqPtNAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity 1: Load Gemma-4B\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        "> Run the next cell to load in the base Gemma-4B model with full 32-bit precision, where all numbers are represented as 32bit floating point numbers.\n",
        ">\n",
        "> Does the cell run successfully? Reflect on the output and why this might be happening.\n",
        ">\n",
        "------"
      ],
      "metadata": {
        "id": "PT2eWi4Wtyx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Gemma3-4B Keras model.\n",
        "model = keras_nlp.models.Gemma3CausalLM.from_preset(preset=\"gemma3_4b_text\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hN_7KvaXt-8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you observe?\n",
        "\n",
        "You should have encountered an error saying `RESOURCE_EXHAUSTED` as well a message indicating how much memory the process attempted to allocate.\n",
        "\n",
        "------\n",
        "> **ðŸ’­ Reflection:**\n",
        ">\n",
        ">Given this output, briefly reflect on these questions:\n",
        ">\n",
        ">1. What do you think the error means at a high level?\n",
        ">\n",
        ">2. Based on what you have read so far about GPU memory consumers, write down a hypothesis for why this happens.\n",
        ">\n",
        "------"
      ],
      "metadata": {
        "id": "H_MfGYCx4m5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Out-of-memory errors\n",
        "\n",
        "The `RESOURCE_EXHAUSTED` error is the technical name for an out-of-memory (OOM) error. It means that the GPU ran out of memory while trying to load the model.\n",
        "\n",
        "This error highlights that for a model of this size, you cannot even load the model with standard settings. However, this issue is not insurmountable. It is the exact problem that modern efficiency techniques are designed to solve. In the next article, you will learn what takes up so much memory in the GPU. Later on, you will also observe how using techniques like mixed precision can help you to overcome this memory wall.\n",
        "\n",
        "In case you are wondering why it was possible to load the Gemma-4B model in the first lab of this course, note that that lab already used some memory efficiency techniques in the background."
      ],
      "metadata": {
        "id": "snEknum9TIkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This lab provided a practical demonstration of the hardware limitations involved in training large models. You experienced what it is like to hit the \"memory wall\" again. You further observed that when attempting to fine-tune a large model, it can fail with an out-of-memory error even at the point of loading the parameter weights. In the next activity, you will learn more about the main components that consume of GPU memory."
      ],
      "metadata": {
        "id": "mtPvGi_97gcS"
      }
    }
  ]
}
