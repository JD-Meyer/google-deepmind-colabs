{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "yTor1GtNseI9",
        "x4LBUVnwsjxX",
        "qkkdRMrBsl-B"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C6-white-bg.png\">"
      ],
      "metadata": {
        "id": "NSb3Wg3gYDVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Compare Models of Different Sizes\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_7/gdm_lab_7_1_compare_models_of_different_sizes.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Explore generations for models of different sizes to understand trade-offs between model performance and efficiency.\n",
        "\n",
        "15 minutes"
      ],
      "metadata": {
        "id": "UH8whVZrYNg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overview\n",
        "This activity explores how generations vary between models of different sizes, both in terms of performance and efficiency. It should help you to gain an intuition for the trade-offs you will need to consider when deciding which model to use."
      ],
      "metadata": {
        "id": "IkkJIC0ZZI9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Describe how generations differ between models of different sizes.\n",
        "* Recall the factors to consider when choosing a model."
      ],
      "metadata": {
        "id": "E-CPergYZnsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Write code to inspect generations for the Gemma-1B and Gemma-4B transformer models.\n",
        "* Compare the generation times and outputs for each model to gain a better understanding of the trade-offs between smaller and larger models.\n",
        "\n",
        "**This lab needs to be run on a GPU. Choose a T4 GPU.** See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this."
      ],
      "metadata": {
        "id": "Ejf1f0hXaC2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are excuted on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "wlNG_jg-39Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the **order in which you run the cells matters**. When you are working through a lab, make sure to always run all cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose _Runtime â†’ Run before_  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "pbtgZxrpjm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click on **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware accelerator**, select **T4 GPU**.\n",
        "4.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running."
      ],
      "metadata": {
        "id": "wjDDOKxEFLRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily interact with the `ai_foundations` package, which has been specifically developed for this course. In the background, this package uses the [`gemma`](https://github.com/google-deepmind/gemma) package to load and prompt the Gemma-1B and Gemma-4B models.\n"
      ],
      "metadata": {
        "id": "1WognQQca4dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install orbax-checkpoint==0.11.21\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import os # For setting system variables.\n",
        "import time # For timing how long generations take.\n",
        "\n",
        "# For formatting the model generations.\n",
        "from IPython.display import display, HTML\n",
        "from ai_foundations import generation # For generating texts with Gemma.\n",
        "\n",
        "# Set the full GPU memory usage for JAX.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ],
      "metadata": {
        "id": "GuXXig43cJAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate outputs with Gemma-1B\n",
        "\n",
        "As preparation for comparing the generations between the Gemma-1B and Gemma-4B models, run the following cell. This will load the Gemma-1B model and generate an output for the prompt \"Jide was hungry so she went looking for\", which you encountered in previous courses in the curriculum. Note that this is the original pre-trained variant of Gemma-1B, not a version of Gemma that has been instruction-tuned and optimized for dialog."
      ],
      "metadata": {
        "id": "glNTTtnilC1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Gemma-1B model.\n",
        "print(\"Loading Gemma-1B model...\")\n",
        "gemma_1b_model = generation.load_gemma(model_name=\"Gemma-1B\")\n",
        "print(\"Loaded Gemma-1B model.\\n\\n\")\n",
        "\n",
        "# Generate an output.\n",
        "prompt = \"Jide was hungry so she went looking for\"\n",
        "output_text_transformer, next_token_logits, tokenizer = (\n",
        "    generation.prompt_transformer_model(\n",
        "        prompt, max_new_tokens=10, loaded_model=gemma_1b_model\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\")"
      ],
      "metadata": {
        "id": "RLNGv3DGmSaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1: Prompt Gemma-1B\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        "> Complete the code in the following cell to:\n",
        "> 1. Loop through the list of prompts.\n",
        "> 2. For each prompt, generate an output with Gemma-1B.\n",
        "> 3. Add the output to the list `generations_gemma_1b`.\n",
        ">\n",
        "> When generating outputs:\n",
        "> - Set the `max_new_tokens` argument to `50`.\n",
        "> - Set the `sampling_mode` argument to `\"greedy\"`.\n",
        ">\n",
        "> Once you have implemented the code, run the following cell to generate outputs. It will also print the time taken to generate outputs for all ten prompts.\n",
        "------"
      ],
      "metadata": {
        "id": "--BGwdGCnHLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    \"The tallest mountain in Africa is\",\n",
        "    \"The key ingredients and preparation method for making traditional Nigerian Jollof rice are\",\n",
        "    \"Wild coffee plants originated in\",\n",
        "    \"The national flower of South Africa is\",\n",
        "    \"The significance of the city of Marrakesh is\",\n",
        "    \"The most populous African nation is\",\n",
        "    \"The world's deepest river is\",\n",
        "    \"The Maasai ethnic group originate from \",\n",
        "    \"The African proverb 'A roaring lion kills no game' means\",\n",
        "    \"Braai is\",\n",
        "]\n",
        "\n",
        "generations_gemma_1b = []\n",
        "\n",
        "before_loop = time.time_ns()\n",
        "\n",
        "# Add your code here.\n",
        "\n",
        "after_loop = time.time_ns()\n",
        "\n",
        "# Calculate duration in nanoseconds.\n",
        "duration_ns = after_loop - before_loop\n",
        "\n",
        "# Convert to total seconds.\n",
        "total_seconds = (after_loop - before_loop) / 1_000_000_000\n",
        "\n",
        "# Calculate minutes and remaining seconds.\n",
        "minutes = int(total_seconds // 60)\n",
        "seconds = total_seconds % 60\n",
        "\n",
        "if minutes > 0:\n",
        "    if minutes == 1:\n",
        "        minutes_str = \"minute\"\n",
        "    else:\n",
        "        minutes_str = \"minutes\"\n",
        "    if seconds == 1:\n",
        "        seconds_str = \"second\"\n",
        "    else:\n",
        "        seconds_str = \"seconds\"\n",
        "    print(f\"Duration: {minutes} {minutes_str} {seconds:.2f} {seconds_str}\")\n",
        "else:\n",
        "    print(f\"Duration: {seconds:.2f} seconds\")"
      ],
      "metadata": {
        "id": "8uSHYGYxnLZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now inspect the generations by running the following cell."
      ],
      "metadata": {
        "id": "8sKoStRzyxc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(list_of_prompts)):\n",
        "    display(HTML(f\"<h3>Prompt:</h3><p>{list_of_prompts[i]}</p>\"))\n",
        "    display(\n",
        "        HTML(\n",
        "            f\"<blockquote><b>Gemma-1B generation:</b><br>{generations_gemma_1b[i]}</blockquote>\"\n",
        "        )\n",
        "    )\n",
        "    display(HTML(\"<hr>\"))"
      ],
      "metadata": {
        "id": "JFq4nbF3v459"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate outputs with Gemma-4B\n",
        "\n",
        "Run the following cell to load the Gemma-4B model and generate an output for the prompt \"Jide was hungry so she went looking for\"."
      ],
      "metadata": {
        "id": "Xh67WqCXqC9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the Gemma-1B model to free up memory.\n",
        "del gemma_1b_model\n",
        "\n",
        "# Load the Gemma-4B model.\n",
        "print(\"Loading Gemma-4B model...\")\n",
        "gemma_4b_model = generation.load_gemma(model_name=\"Gemma-4B\")\n",
        "print(\"Loaded Gemma-4B model.\\n\\n\")\n",
        "\n",
        "# Generate an output.\n",
        "prompt = \"Jide was hungry so she went looking for\"\n",
        "output_text_transformer, next_token_logits, tokenizer = (\n",
        "    generation.prompt_transformer_model(\n",
        "        prompt, max_new_tokens=10, loaded_model=gemma_4b_model\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Generation by Gemma-4B:\\n{output_text_transformer}\")"
      ],
      "metadata": {
        "id": "8ZDMNn9OrhBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2: Prompt Gemma-4B\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        "> Complete the code in the following cell to generate outputs for each prompt for the Gemma-4B model, now adding outputs to the list `generations_gemma_4b`.\n",
        ">\n",
        "> When generating outputs:\n",
        "> - Set the `max_new_tokens` argument to `50`.\n",
        "> - Set the `sampling_mode` argument to `\"greedy\"`.\n",
        ">\n",
        "> Once you have implemented the code, run the following cell to generate outputs.\n",
        "------"
      ],
      "metadata": {
        "id": "lZE7xkqwrqs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_prompts = [\n",
        "    \"The tallest mountain in Africa is\",\n",
        "    \"The key ingredients and preparation method for making traditional Nigerian Jollof rice are\",\n",
        "    \"Wild coffee plants originated in\",\n",
        "    \"The national flower of South Africa is\",\n",
        "    \"The significance of the city of Marrakesh is\",\n",
        "    \"The most populous African nation is\",\n",
        "    \"The world's deepest river is\",\n",
        "    \"The Maasai ethnic group originate from \",\n",
        "    \"The African proverb 'A roaring lion kills no game' means\",\n",
        "    \"Braai is\",\n",
        "]\n",
        "\n",
        "generations_gemma_4b = []\n",
        "\n",
        "before_loop = time.time_ns()\n",
        "\n",
        "# Add your code here.\n",
        "\n",
        "after_loop = time.time_ns()\n",
        "\n",
        "# Calculate duration in nanoseconds.\n",
        "duration_ns = after_loop - before_loop\n",
        "\n",
        "# Convert to total seconds.\n",
        "total_seconds = (after_loop - before_loop) / 1_000_000_000\n",
        "\n",
        "# Calculate minutes and remaining seconds.\n",
        "minutes = int(total_seconds // 60)\n",
        "seconds = total_seconds % 60\n",
        "\n",
        "if minutes > 0:\n",
        "    if minutes == 1:\n",
        "        minutes_str = \"minute\"\n",
        "    else:\n",
        "        minutes_str = \"minutes\"\n",
        "    if seconds == 1:\n",
        "        seconds_str = \"second\"\n",
        "    else:\n",
        "        seconds_str = \"seconds\"\n",
        "    print(f\"Duration: {minutes} {minutes_str} {seconds:.2f} {seconds_str}\")\n",
        "else:\n",
        "    print(f\"Duration: {seconds:.2f} seconds\")"
      ],
      "metadata": {
        "id": "N73N33AvqoQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the generations by running the following cell."
      ],
      "metadata": {
        "id": "dDYrxVk54bhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(list_of_prompts)):\n",
        "    display(HTML(f\"<h3>Prompt:</h3><p>{list_of_prompts[i]}</p>\"))\n",
        "    display(\n",
        "        HTML(\n",
        "            f\"<blockquote><b>Gemma-4B generation:</b><br>{generations_gemma_4b[i]}</blockquote>\"\n",
        "        )\n",
        "    )\n",
        "    display(HTML(\"<hr>\"))"
      ],
      "metadata": {
        "id": "LhL9wB-LvkYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare the generations\n",
        "\n",
        "Now that you have the outputs from both models, compare and evaluate the generations.\n",
        "\n",
        "**Speed**: First, compare the duration it took each model to generate the outputs. Which model was faster?\n",
        "\n",
        "**Quality**: Then, review the outputs for each prompt. Which model generally provides more accurate, detailed, and coherent answers?\n",
        "\n",
        "Run the next cell to display the generations side by side."
      ],
      "metadata": {
        "id": "fis8M5pqtL1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell will display the outputs from both models for each prompt.\n",
        "for i in range(len(list_of_prompts)):\n",
        "    display(HTML(f\"<h3>Prompt:</h3><p>{list_of_prompts[i]}</p>\"))\n",
        "    display(\n",
        "        HTML(\n",
        "            f\"<blockquote><b>Gemma-1B generation:</b>\"\n",
        "            f\"<br>{generations_gemma_1b[i]}</blockquote>\"\n",
        "        )\n",
        "    )\n",
        "    display(\n",
        "        HTML(\n",
        "            f\"<blockquote><b>Gemma-4B generation:</b>\"\n",
        "            f\"<br>{generations_gemma_4b[i]}</blockquote>\"\n",
        "        )\n",
        "    )\n",
        "    display(HTML(\"<hr>\"))"
      ],
      "metadata": {
        "id": "47ZvN9vbvW5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you observe?\n",
        "This exercise highlights the central trade-off in choosing a model. Take a moment to reflect on what you have observed.\n",
        "\n",
        "While considerably slower, the Gemma-4B model tends to be better at factual recall and generating more nuanced text for some of the prompts. For example, you likely observed that the Gemma-4B model provided a better response for the prompt about the meaning of the proverb, and more factually accurate statements about the origin of coffee and the world's deepest river. Such a model, therefore, is generally well-suited for tasks where quality, accuracy, and detail are paramount.\n",
        "\n",
        "Conversely, the smaller Gemma-1B model may be a better choice for applications where speed is critical and \"good-enough\" quality may be acceptable, such as in a real-time chatbot."
      ],
      "metadata": {
        "id": "otHNpbYi3XjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "This lab explored the practical trade-offs between language models of different sizes. The following highlights the key takeaways:\n",
        "\n",
        "1. **Performance versus efficiency**: Larger models like Gemma-4B generally produce higher-quality, more factual, and more coherent text. However, this comes at the cost of being significantly slower.\n",
        "\n",
        "2. **Task-dependent model choice**: Smaller models like Gemma-1B are much faster, making them suitable for applications where speed and efficiency are critical. The \"best\" model is always a balance between the performance you need and the resources you have.\n",
        "\n",
        "3. **Intuition for scaling**: You have now experienced in practice how scaling up a model's parameter count affects both its capabilities and its performance."
      ],
      "metadata": {
        "id": "S2Iql3EE3a8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ],
      "metadata": {
        "id": "yTor1GtNseI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "x4LBUVnwsjxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this code to the cell above.\n",
        "for prompt in list_of_prompts:\n",
        "    output_text_transformer, next_token_logits, tokenizer = (\n",
        "        generation.prompt_transformer_model(\n",
        "            prompt,\n",
        "            max_new_tokens=50,\n",
        "            loaded_model=gemma_1b_model,\n",
        "            sampling_mode=\"greedy\",\n",
        "        )\n",
        "    )\n",
        "    generations_gemma_1b.append(output_text_transformer)"
      ],
      "metadata": {
        "id": "ww4iIKrJsfFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2"
      ],
      "metadata": {
        "id": "qkkdRMrBsl-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this code to the cell above.\n",
        "for prompt in list_of_prompts:\n",
        "    output_text_transformer, next_token_logits, tokenizer = (\n",
        "        generation.prompt_transformer_model(\n",
        "            prompt,\n",
        "            max_new_tokens=50,\n",
        "            loaded_model=gemma_4b_model,\n",
        "            sampling_mode=\"greedy\",\n",
        "        )\n",
        "    )\n",
        "    generations_gemma_4b.append(output_text_transformer)"
      ],
      "metadata": {
        "id": "LqRmK1P5sm3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
