{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "f3Dhr_6R4zaI",
        "9BzWY5vX42ho",
        "HPvF_r6eCt4k",
        "S8-ppO6R45nM",
        "l9NGE-Lm47Fv",
        "ZTsEjPdi48Do"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C6-white-bg.png\">"
      ],
      "metadata": {
        "id": "YQm8YFfZh2Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: Estimate GPU Memory\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_7/gdm_lab_7_4_estimate_gpu_memory.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Learn how to estimate the GPU memory required to train a model.\n",
        "\n",
        "15 minutes"
      ],
      "metadata": {
        "id": "aw0FMei7X1qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the lab \"Estimate Training FLOPs\", you explored the number of computations required to train a model. However, there is another critical resource to consider: GPU memory. Before you can perform a single calculation, all of the necessary data must be loaded into the GPU's memory. This includes the model's parameters, the input data, and temporary values like gradients.\n",
        "\n",
        "In this lab, you will build a memory calculator to estimate the total GPU memory required to train a model."
      ],
      "metadata": {
        "id": "zEXwPwIdZNvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Compute the memory required for training a model.\n",
        "* Describe why training large models is so challenging on resource-constrained hardware."
      ],
      "metadata": {
        "id": "9Y3U7nVH24iL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Write functions to calculate the memory usage for parameters, input data, gradients, optimizer states, and activations.\n",
        "* Use these functions to determine whether a 4-billion parameter model can be trained on a standard 16GB GPU."
      ],
      "metadata": {
        "id": "wveUv1c03Mky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are excuted on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "wlNG_jg-39Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the **order in which you run the cells matters**. When you are working through a lab, make sure to always run all cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose _Runtime â†’ Run before_  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "pbtgZxrpjm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will use functions from the custom `ai_foundations` package for formatting memory estimates and verifying your solutions.\n"
      ],
      "metadata": {
        "id": "41kaZR7N3XpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "from ai_foundations.utils import formatting # For formatting memory estimates.\n",
        "# For checking your solutions.\n",
        "from ai_foundations.feedback.course_7 import memory as feedback\n",
        "\n",
        "# Used to format the results of calculations.\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "PRWu-o8V6jlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell sets up all the necessary constants for this lab. These constants define a hypothetical training scenario for the Gemma-4B model. This includes the `PARAM_COUNT`, the `BYTES_PER_PARAMETER` (for 32-bit precision), and other details like `BATCH_SIZE` and `MAX_LENGTH` that are needed to estimate the size of the activation memory.\n",
        "\n",
        "Run this cell to make these constants available for the rest of the lab."
      ],
      "metadata": {
        "id": "ApNLPg_I6nNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters (4 billion).\n",
        "PARAM_COUNT = 4e9\n",
        "\n",
        "# Precision (bytes per parameter).\n",
        "# For 32-bit (FP32), each parameter requires 4 bytes of storage.\n",
        "BYTES_PER_PARAMETER = 4\n",
        "\n",
        "# Activation formula constants.\n",
        "BATCH_SIZE = 8\n",
        "MAX_LENGTH = 1024 # Maximum sequence length.\n",
        "NUM_LAYERS = 32\n",
        "EMBEDDING_DIM = 2560"
      ],
      "metadata": {
        "id": "o5QQLwm24to_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 1: Calculate parameter memory\n",
        "\n",
        "As a first step, implement a function that computes the amount of memory required to store all the parameters of a model. This and all other functions that you implement as part of this lab will then be used to estimate the total memory requirements for training or fine-tuning a model."
      ],
      "metadata": {
        "id": "gr2fTf7_4K8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        ">Complete the function `calculate_param_memory` in the following cell.\n",
        ">\n",
        ">The memory required is the total number of parameters multiplied by the number of bytes each parameter occupies.\n",
        ">\n",
        "> Once you have implemented this function, run the first cell to define the function and the second cell to test your code.\n",
        "------"
      ],
      "metadata": {
        "id": "ViYHBiJaEAUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_1D-99oXmOg"
      },
      "outputs": [],
      "source": [
        "def calculate_param_memory(param_count: float, bytes_per_param: int) -> float:\n",
        "    \"\"\"Calculates the memory in GB required to store the model parameters.\n",
        "\n",
        "    Args:\n",
        "      param_count: The total number of parameters in the model.\n",
        "      bytes_per_param: The number of bytes used to store a single parameter.\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for parameters, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = ...  # Add your code here.\n",
        "\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your implementation\n",
        "feedback.test_calculate_param_memory(calculate_param_memory)"
      ],
      "metadata": {
        "id": "Xp8kF9NwZoXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 2: Calculate input data memory\n",
        "\n",
        "During training, fine-tuning, or inference, you must also load the entire batch of input data into the GPU's memory. In this activity, compute how much memory is required to store the input for one batch."
      ],
      "metadata": {
        "id": "lFQQs-mGEENw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        ">Complete the function `calculate_input_data_memory` below.\n",
        ">\n",
        "> This function should return the number of bytes required to store the input to a model.\n",
        ">\n",
        "> Hints:\n",
        "> * The GPU processes a whole batch of training examples at once. To find the total memory for the entire input batch, you need to determine the number of bytes in a batch.\n",
        "> * A batch contains `batch_size` examples of length `max_length` tokens. Recall that each sequence has `max_length` because we pad and truncate it to length `max_length`.\n",
        "> * One input tokens requires `bytes_per_token_id` bytes of memory.\n",
        ">\n",
        "> Once you have implemented this function, run the following two cells to define the function and test your code.\n",
        "\n",
        "------"
      ],
      "metadata": {
        "id": "5Iblb-QNEJMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_input_data_memory(\n",
        "    batch_size: int, max_length: int, bytes_per_token_id: int\n",
        ") -> float:\n",
        "    \"\"\"Calculates the memory in GB required to store a batch of input token IDs.\n",
        "\n",
        "    Args:\n",
        "      batch_size: The number of sequences in a single batch.\n",
        "      max_length: The length of each sequence in tokens after padding.\n",
        "      bytes_per_token_id: The number of bytes for each token ID.\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for the input data batch, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = ...  # Add your code here.\n",
        "\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "K7y2aljxFqDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your implementation.\n",
        "feedback.test_calculate_input_data_memory(calculate_input_data_memory)"
      ],
      "metadata": {
        "id": "0oYC99_JFr-m",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 3: Calculate gradient memory\n",
        "\n",
        "Recall that during the backward pass, the optimizer computes a gradient that indicates how to update the parameters. This gradient has one value for each model parameter and since the gradient is computed in the GPU, this also needs to be stored in the GPU memory.\n"
      ],
      "metadata": {
        "id": "jNFtRaxAZsFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        ">Complete the function `calculate_gradient_memory` in the following cell.\n",
        ">\n",
        ">For each parameter in the model, you have to store one number as part of the gradient. The total memory required is therefore the total number of parameters multiplied by the number of bytes each parameter occupies.\n",
        ">\n",
        "> Once you have implemented this function, run the following two cells to define the function and test your code.\n",
        "------"
      ],
      "metadata": {
        "id": "_bwQMSVdEC3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradient_memory(param_count: float, bytes_per_param: int) -> float:\n",
        "    \"\"\"Calculates the memory in GB required to store the gradients.\n",
        "\n",
        "    Args:\n",
        "      param_count: The total number of parameters in the model.\n",
        "      bytes_per_param: The number of bytes used to store a single parameter.\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for gradients, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = ...  # Add your code here.\n",
        "\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "dPNo3a-EZlDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your implementation.\n",
        "feedback.test_calculate_gradient_memory(calculate_gradient_memory)"
      ],
      "metadata": {
        "id": "9ooY--GEZsbR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 4: Calculate optimizer memory\n",
        "\n",
        "Modern optimizers such as Adam also keep information that is used to compute learning rates for each individual parameter. This information also needs to be stored on the GPU.\n",
        "\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        ">Complete the function `calculate_optimizer_memory` in the following cell.\n",
        ">\n",
        ">For each parameter in the model, an optimizer such as Adam stores information that is used to compute the learning rate for that specific parameter.\n",
        "The memory required is therefore the total number of parameters multiplied by the number of bytes this infomation occupies. Adam stores two states per parameter, so therefore the memory requirement for storing the information to compute the learning rate is 2 times the number of bytes required to store a parameter.\n",
        ">\n",
        "> Once you have implemented this function, run the following two cells to define the function and test your code.\n",
        "------"
      ],
      "metadata": {
        "id": "X9eE3A5IZuDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_optimizer_memory(param_count: float, bytes_per_param: int) -> float:\n",
        "    \"\"\"Calculates the memory in GB for Adam optimizer states.\n",
        "\n",
        "    Args:\n",
        "      param_count: The total number of parameters in the model.\n",
        "      bytes_per_param: The number of bytes used to store a single parameter.\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for optimizer states, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = ...  # Add your code here.\n",
        "\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "M9BZJs_QZuJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your optimizer calculation\n",
        "feedback.test_calculate_optimizer_memory(calculate_optimizer_memory)"
      ],
      "metadata": {
        "id": "rl-oJIzgZxQb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 5: Calculate activation memory\n",
        "\n",
        "When doing a forward pass, the model needs to store the results of each computation (also known as **activations**) in the GPU.\n",
        "\n",
        "The activation for a single token after passing through one transformer layer is a vector. The size of this vector is the embedding dimension.\n",
        "* $\\text{memory for one token's activation} = \\text{embedding dimension} \\times \\text{bytes per parameter}$\n",
        "\n",
        "During the forward pass, this activation vector is stored for every single token in the sequence.\n",
        "* $\\text{memory for one sequence} = \\text{maximum sequence length} \\times \\text{memory for one token's activation}$\n",
        "\n",
        "During training, the activations from every layer are kept in memory because they are needed for the backward pass.\n",
        "* $\\text{memory for all layers} = \\text{number of  layers} \\times \\text{memory for one sequence}$\n",
        "\n",
        "Finally, this entire process happens for every example in the batch simultaneously. The GPU must hold the activations for all of them at once.\n",
        "* $\\text{total activation memory} = \\text{batch Size} \\times \\text{memory for all layers}$\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task**:\n",
        ">\n",
        ">Complete the function `calculate_activation_memory` in the following cell.\n",
        ">\n",
        "> Use the information about activiations provided to derive the formula for the activation memory and then implement this computation in the function within the cell.\n",
        ">\n",
        "> Once you have implemented this function, run the following two cells to define the function and test your code.\n",
        ">\n",
        "------"
      ],
      "metadata": {
        "id": "ptTGKbnkZvXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_activation_memory(\n",
        "    batch_size: int,\n",
        "    max_length: int,\n",
        "    num_layers: int,\n",
        "    embedding_dim: int,\n",
        "    bytes_per_param: int,\n",
        ") -> float:\n",
        "    \"\"\"Estimates the memory in GB required for activations using a simplified\n",
        "    formula.\n",
        "\n",
        "    Args:\n",
        "      batch_size: The number of sequences in the batch.\n",
        "      max_length: The length of each sequence in tokens after padding.\n",
        "      num_layers: The number of transformer layers in the model.\n",
        "      embedding_dim: The hidden dimension size of the model.\n",
        "      bytes_per_param: The number of bytes used for the activation values.\n",
        "\n",
        "    Returns:\n",
        "      The estimated total memory for activations, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = ... # Add your code here.\n",
        "\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "-iUn9JOyZvc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your implementation.\n",
        "feedback.test_calculate_activation_memory(calculate_activation_memory)"
      ],
      "metadata": {
        "id": "7q3gEgwjZ5dP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing total memory\n",
        "\n",
        "Now that you have created the functions to calculate memory for each component, run the next cell to compute the total GPU memory required for Gemma-4B. The cell uses all the indiviual functions that you implemented in this lab to output a breakdown of the memory requirements of the invidiual components."
      ],
      "metadata": {
        "id": "Kw4Z6lahZ88c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate memory for each component using your functions.\n",
        "param_mem = calculate_param_memory(PARAM_COUNT, BYTES_PER_PARAMETER)\n",
        "input_data_mem = calculate_input_data_memory(\n",
        "    BATCH_SIZE, MAX_LENGTH, BYTES_PER_PARAMETER\n",
        ")\n",
        "grad_mem = calculate_gradient_memory(PARAM_COUNT, BYTES_PER_PARAMETER)\n",
        "optim_mem = calculate_optimizer_memory(PARAM_COUNT, BYTES_PER_PARAMETER)\n",
        "activ_mem = calculate_activation_memory(\n",
        "    BATCH_SIZE, MAX_LENGTH, NUM_LAYERS, EMBEDDING_DIM, BYTES_PER_PARAMETER\n",
        ")\n",
        "\n",
        "total_inference_memory = param_mem + input_data_mem + activ_mem\n",
        "\n",
        "# Sum them.\n",
        "total_training_memory = total_inference_memory + grad_mem + optim_mem\n",
        "\n",
        "# Display the results.\n",
        "display(HTML(\"<h3>--- GPU memory consumption breakdown ---</h3>\"))\n",
        "display(HTML(\"<h4>------ During training and inference ------</h4>\"))\n",
        "\n",
        "# Use the existing function for each component.\n",
        "formatting.display_memory(\"model parameters\", param_mem)\n",
        "formatting.display_memory(\"input data batch\", input_data_mem, decimal_places=6)\n",
        "formatting.display_memory(\"activations\", activ_mem)\n",
        "display(HTML(\"<h4>------ During training only ------</h4>\"))\n",
        "formatting.display_memory(\"gradients\", grad_mem)\n",
        "formatting.display_memory(\"optimizer states (Adam)\", optim_mem)\n",
        "\n",
        "# Display the formatted separator and total.\n",
        "display(\n",
        "    HTML(\n",
        "        f\"<h3>Total estimated GPU memory required during inference: \"\n",
        "        f\"{total_inference_memory:.2f} GB</h3>\"\n",
        "    )\n",
        ")\n",
        "display(\n",
        "    HTML(\n",
        "        f\"<h3>Total estimated GPU memory required during training or \"\n",
        "        f\"fine-tuning: {total_training_memory:.2f} GB</h3>\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Gw2UnnpBZ9AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the total estimated GPU memory from the cell above. The Google Colab environment typically provides a GPU which has slightly less than **16 GB of memory**.\n",
        "\n",
        "As your calculation shows, the required memory to both perform inference using the 4B Gemma model and to train or fine-tune the 4-billion parameter Gemma model with these settings is greater than 16 GB. In its current form, it's impossible to fit all of the required data onto this GPU. As you observed in the previous lab, attempting to load or train this model with these settings or perform full-parameter fine-tuning would immediately fail with an \"out-of-memory\" error.\n",
        "\n",
        "Fortunately, this memory limitation is a common problem for which a number of solutions have been developed. In the following activities, you will learn about several techniques to improve hardware efficiency. These will allow you to fine-tune larger models using the GPU you have available."
      ],
      "metadata": {
        "id": "u5SLK2T64yzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducing memory requirements with bfloat16\n",
        "\n",
        "In the article \"Smart numbers,\" you learned about the **bfloat16** format that was developed specifically for reducing memory requirements of deep learning models without significantly sacrificing model performance. In the final activity of this lab, compute the memory requirements of the Gemma-4B model when the parameters, gradients, and activations are stored as bfloat16 numbers instead of 32-bit floating point numbers."
      ],
      "metadata": {
        "id": "l61h-KKFzAqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of bytes per parameter to 2 as bfloat16 uses only two bytes\n",
        "# to store each parameter, gradient, and activation.\n",
        "BYTES_PER_PARAMETER = 2\n",
        "\n",
        "param_mem = calculate_param_memory(PARAM_COUNT, BYTES_PER_PARAMETER)\n",
        "input_data_mem = calculate_input_data_memory(\n",
        "    BATCH_SIZE, MAX_LENGTH, BYTES_PER_PARAMETER\n",
        ")\n",
        "grad_mem = calculate_gradient_memory(PARAM_COUNT, BYTES_PER_PARAMETER)\n",
        "optim_mem = calculate_optimizer_memory(PARAM_COUNT, BYTES_PER_PARAMETER)\n",
        "activ_mem = calculate_activation_memory(\n",
        "    BATCH_SIZE, MAX_LENGTH, NUM_LAYERS, EMBEDDING_DIM, BYTES_PER_PARAMETER\n",
        ")\n",
        "\n",
        "total_inference_memory = param_mem + input_data_mem + activ_mem\n",
        "\n",
        "# Sum them.\n",
        "total_training_memory = total_inference_memory + grad_mem + optim_mem\n",
        "\n",
        "# Display the results.\n",
        "display(HTML(\"<h3>--- GPU memory consumption breakdown ---</h3>\"))\n",
        "display(HTML(\"<h4>------ During training and inference ------</h4>\"))\n",
        "\n",
        "# Use the existing function for each component.\n",
        "formatting.display_memory(\"model parameters\", param_mem)\n",
        "formatting.display_memory(\"input data batch\", input_data_mem, decimal_places=6)\n",
        "formatting.display_memory(\"activations\", activ_mem)\n",
        "display(HTML(\"<h4>------ During training only ------</h4>\"))\n",
        "formatting.display_memory(\"gradients\", grad_mem)\n",
        "formatting.display_memory(\"optimizer states (Adam)\", optim_mem)\n",
        "\n",
        "# Display the formatted separator and total.\n",
        "display(\n",
        "    HTML(\n",
        "        f\"<h3>Total estimated GPU memory required during inference: \"\n",
        "        f\"{total_inference_memory:.2f} GB</h3>\"\n",
        "    )\n",
        ")\n",
        "display(\n",
        "    HTML(\n",
        "        f\"<h3>Total estimated GPU memory required during training or \"\n",
        "        f\"fine-tuning: {total_training_memory:.2f} GB</h3>\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "oII7fF-O0zyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this computation shows, representing numbers using bfloat16 halves the memory requirements. This, for example, allows you to perform inference with a 4 billion parameter model on a GPU with 16GB. You will make use of this technique in the next lab. Here you will load and fine-tune the Gemma-4B model using this more efficient number representation for storing parameters, gradients, and optimizer states."
      ],
      "metadata": {
        "id": "Za18_uQn127G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This lab provided a practical guide to estimating the memory required to train a language model. The five main components that need to be stored in the GPU's memory are:\n",
        " * Model parameters\n",
        " * Input data\n",
        " * Gradients\n",
        " * Optimizer states\n",
        " * Activations\n",
        "\n",
        "A model can fail to train because these components together require more memory than is available on a GPU. In the next activity, you will apply techniques to overcome these memory limitations."
      ],
      "metadata": {
        "id": "GzDiC3Vi9Ioq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong."
      ],
      "metadata": {
        "id": "f3Dhr_6R4zaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "9BzWY5vX42ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_param_memory(param_count: int, bytes_per_param: int) -> float:\n",
        "    \"\"\"Calculates the memory in GB required to store the model parameters.\n",
        "\n",
        "    Args:\n",
        "      param_count: The total number of parameters in the model.\n",
        "      bytes_per_param: The number of bytes used to store a single parameter.\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for parameters, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = param_count * bytes_per_param\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "tJV5BSIN44fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2"
      ],
      "metadata": {
        "id": "HPvF_r6eCt4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_input_data_memory(\n",
        "    batch_size: int, max_length: int, bytes_per_token_id: int\n",
        ") -> float:\n",
        "    \"\"\"Calculates the memory in GB required to store a batch of input token IDs.\n",
        "\n",
        "    Args:\n",
        "      batch_size: The number of sequences in a single batch.\n",
        "      max_length: The length of each sequence in tokens after padding.\n",
        "      bytes_per_token_id: The number of bytes for each token ID (default is 4).\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for the input data batch, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = batch_size * max_length * bytes_per_token_id\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "jTm6FqcpC1Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 3"
      ],
      "metadata": {
        "id": "S8-ppO6R45nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradient_memory(param_count: int, bytes_per_param: int) -> float:\n",
        "    \"\"\"Calculates the memory in GB required to store the gradients.\n",
        "\n",
        "    Args:\n",
        "      param_count: The total number of parameters in the model.\n",
        "      bytes_per_param: The number of bytes used to store a single parameter.\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for gradients, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = param_count * bytes_per_param\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "g0cl0GUO46_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 4"
      ],
      "metadata": {
        "id": "l9NGE-Lm47Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_optimizer_memory(param_count: int, bytes_per_param: int) -> float:\n",
        "    \"\"\"Calculates the memory in GB for Adam optimizer states.\n",
        "\n",
        "    Args:\n",
        "      param_count: The total number of parameters in the model.\n",
        "      bytes_per_param: The number of bytes used to store a single parameter.\n",
        "\n",
        "    Returns:\n",
        "      The total memory required for optimizer states, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = 2 * param_count * bytes_per_param\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "79kpr3f447-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 5"
      ],
      "metadata": {
        "id": "ZTsEjPdi48Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_activation_memory(\n",
        "    batch_size: int,\n",
        "    max_length: int,\n",
        "    num_layers: int,\n",
        "    embedding_dim: int,\n",
        "    bytes_per_param: int,\n",
        ") -> float:\n",
        "    \"\"\"Estimates the memory in GB required for activations using a simplified formula.\n",
        "\n",
        "    Args:\n",
        "      batch_size: The number of sequences in the batch.\n",
        "      max_length: The length of each sequence in tokens after padding.\n",
        "      num_layers: The number of transformer layers in the model.\n",
        "      embedding_dim: The hidden dimension size of the model.\n",
        "      bytes_per_param: The number of bytes used for the activation values.\n",
        "\n",
        "    Returns:\n",
        "      The estimated total memory for activations, in gigabytes.\n",
        "    \"\"\"\n",
        "    total_bytes = batch_size * max_length * num_layers * embedding_dim * bytes_per_param\n",
        "    return formatting.bytes_to_gb(total_bytes)"
      ],
      "metadata": {
        "id": "SAUZRjfx49Mw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
