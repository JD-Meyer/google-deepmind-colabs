{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Tokenize Texts into Subword Tokens\n",
    "## Purpose:\n",
    "- Apply subword tokenization to address the OOV problem\n",
    "- Use special tokens\n",
    "\n",
    "### Topics:\n",
    "- Subword Tokenization\n",
    "- Special tokens\n",
    "\n",
    "### Steps\n",
    "- Experiment with Gemma's tokenizer to explore subword tokenization.\n",
    "- Implement a function to tokenize the made-up word \"Clusterophonexia\".\n",
    "- Inspect how Gemma handles emojis and the purpose of its special tokens.\n",
    "\n",
    "Date: 2026-02-20\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_3_tokenize_texts_into_subword_tokens.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "aff3ab6020cf355b"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "\n",
    "# Install the custom package for this course. This also installs the gemma\n",
    "# package.\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "from gemma import gm # For interacting with the Gemma tokenizer.\n",
    "# For providing feedback on your implementations.\n",
    "from ai_foundations.feedback.course_2 import subword_tokens as feedback"
   ],
   "id": "aa764f7ffcc32a26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Subword Tokenization\n",
    "A compromise between character-level and word-level tokenization.\n",
    "* Frequent words (like \"the\" or \"is\") are kept as single, complete tokens.\n",
    "* Rare or complex words (like \"Baobab\") are broken down into smaller, meaningful sub-units."
   ],
   "id": "d154a6c6af34ea88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load and experiment with the Gemma tokenizer\n",
    "\n",
    "To gain a better intuition of how the Gemma tokenizer works, run the following cell to load it.\n",
    "\n",
    "Gemma has a vocabulary of more than 260,000 tokens."
   ],
   "id": "fe8eccdf6522de95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer.\n",
    "gemma_tokenizer = gm.text.Gemma3Tokenizer()\n",
    "\n",
    "# Inspect the vocabaulary size.\n",
    "print(f\"Gemma's vocabulary consists of {gemma_tokenizer.vocab_size:,} tokens.\")"
   ],
   "id": "ea1354fd7801f021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Encoding and decoding\n",
    "\n",
    "Use the `encode()` function of the tokenizer to translate arbitrary input text to token IDs Gemma can process."
   ],
   "id": "6be8ae83d2feddb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode a text into token IDs.\n",
    "text = \"The Baobab (genus Adansonia) is one of the most iconic trees.\"\n",
    "\n",
    "gemma_tokens = gemma_tokenizer.encode(text)\n",
    "print(f\"Result of tokenizing the text \\\"{text}\\\":\")\n",
    "print(gemma_tokens)"
   ],
   "id": "1dfae268d51bb29f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Decode the tokens back to a text.\n",
    "decoded_text = gemma_tokenizer.decode(gemma_tokens)\n",
    "print(f\"Decoded sentence from tokens: {decoded_text}\\n\")\n",
    "\n",
    "# Check whether this results in the same text as the original one.\n",
    "is_equal = \"✅\" if text == decoded_text else \"❌\"\n",
    "print(\n",
    "    f\"Decoding the tokens results in the same text as the original one:\"\n",
    "    f\" {is_equal}\\n\"\n",
    ")\n",
    "\n",
    "# Decode individual tokens.\n",
    "for token in gemma_tokens:\n",
    "    decoded_token = gemma_tokenizer.decode(token)\n",
    "    print(f\"Token {token}:\\t{decoded_token}\")"
   ],
   "id": "39135bdde90d2c9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenize a made-up word",
   "id": "21c10a121a3fd440"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set the following two variables as described in the instructions above.\n",
    "clusterophonexia = \"Clusterophonexia\"\n",
    "clusterophonexia_tokens = gemma_tokenizer.encode(clusterophonexia)\n",
    "first_token_as_text = gemma_tokenizer.decode(clusterophonexia_tokens[0])\n",
    "print(first_token_as_text)"
   ],
   "id": "4af50bb38a3a51fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizing Unicode characters",
   "id": "29fdbf11985e2d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gemma_tokens = gemma_tokenizer.encode(\"I am smiling ☺️!\")\n",
    "\n",
    "for i, token in enumerate(gemma_tokens):\n",
    "    decoded_token = gemma_tokenizer.decode(token)\n",
    "    print(f\"Token {token}:\\t{decoded_token}\")"
   ],
   "id": "8da6755d504f1106"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Special tokens\n",
    "* **`<BOS>`** and **`<EOS>`**:\n",
    "\n",
    "  Mark the start and end of a distinct piece of text. Advantages:\n",
    "\n",
    "  * Efficient batching: Feed multiple documents to the model in a single batch without extensive padding.\n",
    "\n",
    "  * Dynamic generation: During text generation, the `<EOS>` token serves as a stop signal. Instead of generating a fixed number of tokens, the model can generate text until it produces an `<EOS>` token, allowing it to decide when a response is complete.\n",
    "\n",
    "* **`<PAD>`**:\n",
    "\n",
    "  Transformer models require inputs to have a fixed size, so shorter sequences are \"padded\" with this token until they match the length of the longest sequence in the batch.\n",
    "\n",
    "* **`<UNK>`**:\n",
    "\n",
    "  Placeholder for a character or symbol not in the tokenizer's vocabulary.\n",
    "\n",
    "### Special tokens in Gemma\n",
    "\n",
    "Can be accessed through `gemma_tokenizer.special_tokens`.\n",
    "\n",
    "**Expected output**\n",
    "```\n",
    "<_Gemma3SpecialTokens.BOS: 2>\n",
    "<_Gemma3SpecialTokens.EOS: 1>\n",
    "```"
   ],
   "id": "cdb7fbb5b6af06e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Beginning of sentence (BOS) token.\n",
    "gemma_tokenizer.special_tokens.BOS\n",
    "\n",
    "# End of sentence (EOS) token.\n",
    "gemma_tokenizer.special_tokens.EOS"
   ],
   "id": "4a37b6403f5e2eb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer also supports automatically adding the BOS and EOS tokens to a sequence. Useful when prepping data for finetuning a chatbot on prompts and model answers, to get the model to learn when it should stop generating.\n",
    "\n",
    "**Expected output**\n",
    "```\n",
    "[<_Gemma3SpecialTokens.BOS: 2>,\n",
    " 9259,\n",
    " 1902,\n",
    " 236888,\n",
    " <_Gemma3SpecialTokens.EOS: 1>]\n",
    "```"
   ],
   "id": "a1a26c40b0d98aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "token_ids = gemma_tokenizer.encode(\"Hello world!\", add_bos=True, add_eos=True)\n",
    "token_ids"
   ],
   "id": "6f9270a6173e7163"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
