{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Experiment with Embeddings\n",
    "## Purpose:\n",
    "- Compute similarities between the embeddings in a Gemma model\n",
    "- Experiment with embedding visualization techniques\n",
    "- Refreshing my memory of matrix multiplication\n",
    "- Fun with numpy\n",
    "\n",
    "### Topics:\n",
    "- Token Embeddings\n",
    "- Cosine similarity\n",
    "- Vectors\n",
    "- Matrices\n",
    "- NumPy\n",
    "\n",
    "### Steps\n",
    "* Load a part of the Gemma embedding matrix.\n",
    "* Implement functions to extract embeddings from the embedding matrix and compute dot products.\n",
    "* Implement a function that prints similarities for pairs of tokens.\n",
    "* Visualize individual embedding dimensions.\n",
    "* Experiment with dimensionality-reduction techniques such as t-SNE.\n",
    "\n",
    "Date: 2026-02-20\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_5_experiment_with_embeddings.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "6e46f15fe6ce0142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "# Install the custom package for this course.\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "import numpy as np # For working with vectors and matrices.\n",
    "# For loading and projecting embeddings.\n",
    "from ai_foundations import embeddings as emb\n",
    "# For providing feedback.\n",
    "from ai_foundations.feedback.course_2 import embeddings as emb_feedback"
   ],
   "id": "9e3d64884b0ab13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Gemma embeddings for 24 tokens.\n",
    "\n",
    "Gemma's tokenizer uses a vocabulary of more than 260,000 tokens, which means its embedding table also has entries for more than 260,000 tokens.\n",
    "This lab works with 24 token embeddings to reduce the memory requirements and speed up computations.\n",
    "```\n",
    "The token labels are:\n",
    "  king\n",
    "  queen\n",
    "  man\n",
    "  woman\n",
    "  apple\n",
    "  etc.\n",
    "```"
   ],
   "id": "f749392bae3b64de"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load Gemma embeddings.\n",
    "embeddings, labels = emb.load_gemma_embeddings(\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/gemma_embeddings.npz\")\n",
    "\n",
    "print(f\"The number of tokens are {len(labels)}.\")\n",
    "print(f\"The token labels are:\\n  {'\\n  '.join(labels)}\")"
   ],
   "id": "4d7b20206659f863"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Working with matrices and vectors using `numpy`\n",
    "\n",
    "------\n",
    ">The table that stores the embeddings is usually a matrix where each row is a vector that contains the embedding for one token.\n",
    ">Machine learning models involve a lot of vector and matrix operations.\n",
    ">\n",
    "\n",
    "> Define a 3-dimensional vector `v` with the elements 1 2 3.\n",
    ">\n",
    ">v = np.array([1, 2, 3])\n",
    ">\n",
    ">\n",
    "> Define a 2x3 dimensional matrix `M` (2 rows, 3 columns) with the following elements:\n",
    ">\n",
    ">   6 1 4\n",
    ">\n",
    ">   9 0 2\n",
    ">\n",
    ">M = np.array( [[ 6,  1,  4 ], [ 9,  0,  2 ]] )\n",
    ">\n",
    ">```\n",
    "\n",
    "### 1: Define vectors and matrices\n",
    "------\n",
    "> Define the following vectors and matrices using `np.array`.\n",
    ">\n",
    "> $$\\mathbf{a} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 1 \\\\ 4  \\end{pmatrix} \\ \\ \\  \\ \\ \\ \\ \\mathbf{b} = \\begin{pmatrix} 1.5 \\\\ -2.5 \\end{pmatrix} \\ \\ \\ \\ \\ \\ \\mathbf{c} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix}$$\n",
    ">\n",
    "> <br>\n",
    "> $$P = \\begin{pmatrix} 7 & 4\\\\ 3 & 5 \\\\ 1 & 6 \\\\ 4 & 7  \\end{pmatrix} \\ \\ \\  \\ \\ \\ \\ Q = \\begin{pmatrix} 7 & 3 & 1 & 4 \\\\ 4 & 5 & 6 & 7 \\end{pmatrix} \\ \\ \\ \\ \\ \\ R = \\begin{pmatrix} 4 & 4 & 4 \\end{pmatrix}$$\n",
    "-----"
   ],
   "id": "ba127736a549b053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a = np.array([7, 3, 1, 4])\n",
    "b = np.array([1.5, -2.5])\n",
    "c = np.array([4, 4, 4])\n",
    "\n",
    "P = np.array([[7,4], [3,5], [1,6], [4,7]])\n",
    "Q = np.array([[7, 3, 1, 4], [4, 5, 6, 7]])\n",
    "R = np.array([[4, 4, 4]])"
   ],
   "id": "1d8070ea6252a075"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2: The shape of vectors and matrices\n",
    "\n",
    "**Shape**: the dimension of a vector or a matrix.\n",
    "- For vectors, it's the number of elements in the vector.\n",
    "- For matrices it's the number of rows and columns in the matrix.\n",
    "\n",
    "`numpy.shape` returns a tuple with the number of rows and number of columns (if it is a matrix)."
   ],
   "id": "5a2f6a750ed3fa60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print the shapes of the above vectors & matrices.\n",
    "print(f\"a = {a}\")\n",
    "print(f\"Shape of a: {a.shape}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"b = {b}\")\n",
    "print(f\"Shape of b: {b.shape}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"c = {c}\")\n",
    "print(f\"Shape of c: {c.shape}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"P =\\n{P}\")\n",
    "print(f\"Shape of P: {P.shape}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"Q =\\n{Q}\")\n",
    "print(f\"Shape of Q: {Q.shape}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"R =\\n{R}\")\n",
    "print(f\"Shape of R: {R.shape}\")"
   ],
   "id": "169fc6978fc33ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# get the number of columns in the embedding matrix\n",
    "embedding_dim = embeddings.shape[1]"
   ],
   "id": "582bcbe67539390b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3: Access rows and columns\n",
    "\n",
    "\n",
    "```python\n",
    "M[row_or_rows, column_or_columns]\n",
    "```\n",
    "\n",
    "Use `:` to get all rows/comuns.\n",
    "\n",
    "Ex. all columns of the 3rd row\n",
    "\n",
    "```python\n",
    "M[2, :]\n",
    "```\n",
    "\n",
    "Ex. All rows of the 4th column of `M`:\n",
    "\n",
    "```python\n",
    "M[:, 3]\n",
    "```"
   ],
   "id": "aaeb56c5c0ee1e50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "third_row = embeddings[2, :]\n",
    "seventh_column = embeddings[:, 6]\n",
    "\n",
    "print(f\"Shape of third_row: {third_row.shape}\")\n",
    "print(f\"Shape of seventh_column: {seventh_column.shape}\")"
   ],
   "id": "9b94e08121ecb874"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dot product\n",
    "\n",
    "The dot product between $K$-dimensional vectors $\\mathbf{u} \\in \\mathbb{R}^K$ and $\\mathbf{v} \\in \\mathbb{R}^K$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{k=1}^K u_k v_k\n",
    "\\;=\\;\n",
    "u_1 v_1 + u_2 v_2 + \\cdots + u_K v_K $$\n",
    "\n",
    "\n",
    "It is also sometimes written as $\\mathbf{u}^T \\mathbf{v}$. The superscript ${}^T$ indicates that the vector should be transposed, that is, a column vector should be transformed into a row vector.\n",
    "\n",
    "In Python, you can compute the dot product between the vectors `u` and `v` using either the `np.dot` function, or the more general  `np.matmul` function that is used to multiply matrices:\n",
    "\n",
    "```python\n",
    "dot_product = np.dot(u, v)\n",
    "\n",
    "dot_product = np.matmul(u.T, v)\n",
    "```\n",
    "\n",
    "Note that if you use matmul, you have to make sure that the second dimension of the first argument and the first dimension of the second argument agree. This may involve computing the transpose, which is done here using `u.T`."
   ],
   "id": "f122333c521e3916"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compute the dot product between the third and  fourth rows of embeddings\n",
    "\n",
    "third_row = embeddings[2,:]         # row 3\n",
    "fourth_row = embeddings[3,:]        # row 6\n",
    "\n",
    "dot_product = np.matmul(third_row.T, fourth_row)\n",
    "\n",
    "print(f\"Dot product: {dot_product:.4f}\")\n",
    "\n",
    "# Dot product: 0.4967"
   ],
   "id": "fdfe0ce6fa02731d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dot product indicates similarity of two vectors.\n",
    "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/inner-products.png)\n",
    "\n",
    "- Negative dot product: when $\\mathbf{u}^T \\mathbf{v} < 0$, the angle between them is greater than 90 degrees. The two vectors are pointing in opposite directions and this indicates a high level of dissimilarity.\n",
    "- Zero dot product: when $\\mathbf{u}^T \\mathbf{v} = 0$, they are orthogonal and the angle between them is 90 degrees. Usually the embeddings are unrelated.\n",
    "- Positive dot product: when $\\mathbf{u}^T \\mathbf{v} > 0$, the angle between them is less than 90 degrees. The vectors are pointing in a similar direction, meaning the embeddings are similar.\n",
    "\n",
    "When you placed the embeddings for \"apple\" and \"banana\" on a 2D plane, you most likely placed them so that the angle between the two embeddings is small, and intuitively placed them so that $\\mathbf{u}^T \\mathbf{v} > 0$."
   ],
   "id": "c26b7d900346d80b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cosine similarity\n",
    "\n",
    "The dot product indicates the similarity of two embeddings, but it can become very big or very small when a vector has many dimensions because you are summing over a lot of values. **Normalize** the similarities to make them less dependent on the specific values and the number of dimensions, such that it always returns a value between -1 and +1.\n",
    "\n",
    "The **cosine similarity** does exactly that:\n",
    "\n",
    "$$\n",
    "\\text{cosine}\\ \\bigl(\\mathbf{u},\\mathbf{v}\\bigr)\n",
    "\\;=\\;\n",
    "\\frac{\\mathbf{u}\\,\\cdot\\,\\mathbf{v}}\n",
    "     {\\lVert \\mathbf{u} \\rVert \\,\\lVert \\mathbf{v} \\rVert}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{u}\\cdot\\mathbf{v}$ is the dot product of the two vectors, and ${\\lVert \\mathbf{u} \\rVert \\,\\lVert \\mathbf{v} \\rVert}$\n",
    "are the magnitudes (lengths) of the vectors $\\mathbf{u}$ and $\\mathbf{v}$, respectively.\n",
    "\n",
    "Cosine similarity measures how similar vectors are by computing the dot product, scaled by their lengths. This captures the cosine of the angle between them rather than their magnitude.\n",
    "\n",
    "The cosine similarity is +1 for identical directions, 0 for orthogonal vectors (e.g., embeddings of unrelated tokens), and -1 for opposite vectors (e.g., embeddings of strong antonyms)."
   ],
   "id": "50d7ae6326ca61b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute the cosine similarity between two vectors\n",
    "\n",
    "def cos_sim(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    \"\"\"Computes the cosine similarity between two 1-D numpy arrays u and v.\n",
    "    Args:\n",
    "      u: A vector of dimension (k,).\n",
    "      v: A vector of dimension (k,).\n",
    "    Returns:\n",
    "      The dot product between u and v.\n",
    "    \"\"\"\n",
    "\n",
    "    dot_uv = np.matmul(u.T,v)\n",
    "\n",
    "    # np.linalg.norm(u, 2) computes the length of the vector u (its L2-norm).\n",
    "    len_u = np.linalg.norm(u, 2)\n",
    "    len_v = np.linalg.norm(v, 2)\n",
    "\n",
    "    # u . v / (||u|| * ||v||).\n",
    "    cosine_sim = dot_uv / (len_u * len_v)\n",
    "\n",
    "    # Turn 1x1 numpy array into a float.\n",
    "    cosine_sim = cosine_sim.item()\n",
    "\n",
    "    return cosine_sim"
   ],
   "id": "cf0acaaf5bad8014"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5: Access the embedding for a token\n",
    "Before you can compute the cosine similarity between two token embeddings, you need to write a function that returns the embedding for a specific token, e.g., \"apple\".\n",
    "\n",
    "For this you need to determine the index of the token in the embedding matrix. The list labels that was loaded at the top of this lab contains all tokens with corresponding embeddings in embeddings. The embedding of the first element in the list is the first row of embeddings, the embedding of the second element in the list is the second embedding, etc.\n",
    "\n",
    "To determine the index of the embedding, you can use the .index method of the list. For example, labels.index(\"apple\") returns the index of the row of the embedding for \"apple\"."
   ],
   "id": "7e2a75a6764a8314"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_embedding(\n",
    "    token: str, embeddings: np.ndarray = embeddings, labels: list[str] = labels\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Returns the embedding for `token` from `embeddings`.\n",
    "\n",
    "    Args:\n",
    "      token: The token for which the embedding should be retrieved.\n",
    "      embeddings: The embedding matrix with embeddings for all tokens in\n",
    "        `labels`.\n",
    "      labels: The list of tokens indicating the order of embeddings in\n",
    "        `embeddings`.\n",
    "\n",
    "    Returns:\n",
    "      The token embedding (a vector) for `token`.\n",
    "\n",
    "    Raises:\n",
    "      ValueError if no embeddings for `token` exists.\n",
    "    \"\"\"\n",
    "\n",
    "    if token not in labels:\n",
    "        raise ValueError(f\"No embeddings for {token} exist.\")\n",
    "\n",
    "    token_idx =  labels.index(token)\n",
    "    embedding =  embeddings[token_idx]\n",
    "\n",
    "    return embedding\n"
   ],
   "id": "3585c4f73be4f482"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  6: Compute the cosine similarity\n",
    "Use the implementation of cos_sim to define a function that prints the similarity between the embeddings of two tokens."
   ],
   "id": "fa967aaa35f37bd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_similarity(\n",
    "    token1: str,\n",
    "    token2: str,\n",
    "    embeddings: np.ndarray = embeddings,\n",
    "    labels: list[str] = labels,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes and prints the cosine similarity between the embeddings of `token1`\n",
    "      and `token2`.\n",
    "\n",
    "    Args:\n",
    "      token1: The first token for the similarity computation.\n",
    "      token2: The second token for the similarity computation.\n",
    "      embeddings: The embedding matrix with embeddings for `token1` and\n",
    "        `token2`.\n",
    "      labels: The list of tokens indicating the order of embeddings in\n",
    "        `embeddings`.\n",
    "\n",
    "    Returns:\n",
    "      The cosine similarity between `token1` and `token2`.\n",
    "\n",
    "    Raises:\n",
    "      ValueError if no embedding for `token1` or `token2` exists.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    embedding1 = get_embedding(token1, embeddings, labels)\n",
    "    embedding2 = get_embedding(token2, embeddings, labels)\n",
    "\n",
    "    similarity = cos_sim(embedding1, embedding2)\n",
    "    print(\n",
    "        f'Cosine similarity between \"{token1}\" and \"{token2}\" '\n",
    "        f'\\t= {similarity:.2f}'\n",
    "    )\n",
    "    return similarity"
   ],
   "id": "f4f7de63e87a4fc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b58905896645c3ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print_similarity(\"king\", \"king\")\n",
    "print_similarity(\"king\", \"queen\")\n",
    "print_similarity(\"queen\", \"king\")\n",
    "print_similarity(\"joy\", \"happy\")\n",
    "print_similarity(\"good\", \"bad\")\n",
    "print_similarity(\"sad\", \"happy\")\n",
    "print_similarity(\"king\", \"bus\")\n",
    "print_similarity(\"car\", \"banana\")\n",
    "print()"
   ],
   "id": "d1ae184b44f04373"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Expected output\n",
    "```\n",
    "Cosine similarity between \"king\" and \"king\" \t= 1.00\n",
    "Cosine similarity between \"king\" and \"queen\" \t= 0.42\n",
    "Cosine similarity between \"queen\" and \"king\" \t= 0.42\n",
    "Cosine similarity between \"joy\" and \"happy\" \t= 0.29\n",
    "Cosine similarity between \"good\" and \"bad\" \t= 0.42\n",
    "Cosine similarity between \"sad\" and \"happy\" \t= 0.22\n",
    "Cosine similarity between \"king\" and \"bus\" \t= 0.04\n",
    "Cosine similarity between \"car\" and \"banana\" \t= 0.07\n",
    "```"
   ],
   "id": "5f7313e99227895a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizing high-dimensional embeddings\n",
   "id": "96285330649ad941"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @title Plot individual dimensions\n",
    "# Adjust the numeric values to choose one of 1151 possible dimensions (It doesn't work well)\n",
    "dimension_1 = 0  #@param {type: 'slider', min:0, max:1151}\n",
    "dimension_2 = 1151  #@param {type: 'slider', min:0, max:1151}\n",
    "emb.plot_embeddings_dimensions(embeddings,\n",
    "                           labels,\n",
    "                           dim_x=dimension_1,\n",
    "                           dim_y=dimension_2)"
   ],
   "id": "73494e4a0cd50b90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualizing  embeddings with t-SNE\n",
    "\n",
    "Meaningful word relationships typically arise from combinations of many embedding dimensions.\n",
    "\n",
    "Dimensionality reduction techniques make high-dimensional embedding space more interpretable. They try to compress the embedding space into fewer dimensions with the goal of capturing as much information as possible from the original embedding space.\n",
    "\n",
    "[t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) has been particularly well-suited for projecting embeddings for visualizations. t-SNE is a visualization method that **preserves the pairwise similarities** between data points in a lower-dimensional space. t-SNE provides an \"at a glance\" map of neighborhoods hidden in the high-dimensional space, keeping local distances faithful, so clusters that are closely related pop out as tight clouds that are easy to label and debug.\n",
    "\n",
    "The 2D points can be used to generate a visualization of how the words are distributed in the high-dimensional space. Data points close together in the high-dimensional space will appear closer together in 2D using t-SNE."
   ],
   "id": "de47d128d281a6c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This makes much more sense than the last one.\n",
    "\n",
    "emb.plot_embeddings_tsne(embeddings, labels)"
   ],
   "id": "188dc5b54e882f47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
