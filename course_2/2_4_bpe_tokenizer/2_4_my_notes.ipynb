{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Implement a BPE Tokenizer\n",
    "## Purpose:\n",
    "- Understanding the BPE algorithm\n",
    "- Train a subword tokenizer\n",
    "- Implement and run the BPE algorithm over multiple merge steps\n",
    "\n",
    "### Topics:\n",
    "- Byte-pair encoding\n",
    "\n",
    "### Steps\n",
    "* Write code to convert each word into a list of characters with </w> appended.\n",
    "* Complete the function get_pair_frequencies to count adjacent symbol pairs.\n",
    "* Fill in logic in merge_pair_in_word to replace the most frequent pair.\n",
    "* Apply merges iteratively to update the corpus and vocabulary.\n",
    "* Implement the loop in learn_bpe to perform a fixed number of merge steps and inspect results.\n",
    "\n",
    "* Date: 2026-02-20\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_4_implement_a_bpe_tokenizer.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "c8b2a6c2b34795f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import Counter, defaultdict # For counting token pairs.\n",
    "import pandas as pd # For loading the dataset."
   ],
   "id": "78eee80b2a43dcfa"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"].values\n",
    "print(f\"Dataset contains {dataset.shape[0]:,} paragraphs.\")"
   ],
   "id": "3fec8f870391c2e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The BPE Algorithm\n",
    "\n",
    "1. **Initialization**: Split the dataset into individual characters. Initialize the vocabulary with the set of unique characters. Replaces spaces with `</w>`).\n",
    "2. **Counting**: Count how often each pair of tokens appears in the corpus.\n",
    "3. **Merging**: Choose the most frequently appearing adjacent pair of tokens `(p, q)`. Add a new merged token `pq` to the vocabulary.\n",
    "4. **Replacing**: Replace all adjacent pairs of tokens `(p, q)` with the new token `pq` in the corpus.\n",
    "5. **Repeating**: Repeat steps 2-4 until you reach the target vocabulary size."
   ],
   "id": "55a5e04ef471eb8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Initialization\n",
    "\n",
    "- Convert every space-separated word into a list of characters.\n",
    "- Append \"</w>\" at the end of each word's list of characters.\n",
    "- Add the set of unique characters to the vocabulary.\n",
    "\n",
    "Expected output\n",
    "```\n",
    "First 10 words:\n",
    "  ['T', 'h', 'e', '</w>']\n",
    "  ['L', 'a', 'g', 'o', 's', '</w>']\n",
    "  ['a', 'i', 'r', '</w>']\n",
    "  ['w', 'a', 's', '</w>']\n",
    "  ['t', 'h', 'i', 'c', 'k', '</w>']\n",
    "  ['w', 'i', 't', 'h', '</w>']\n",
    "  ['h', 'u', 'm', 'i', 'd', 'i', 't', 'y', ',', '</w>']\n",
    "  ['b', 'u', 't', '</w>']\n",
    "  ['t', 'h', 'e', '</w>']\n",
    "  ['e', 'n', 'e', 'r', 'g', 'y', '</w>']\n",
    "```"
   ],
   "id": "5b54ded9f4dbc61e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "EOW_SYMBOL = \"</w>\"\n",
    "\n",
    "def bpe_initialize(dataset: list[str]) -> tuple[list[list[str]], set[str]]:\n",
    "    \"\"\"The initialization step.\n",
    "    Args:\n",
    "      dataset: The corpus consisting of a list of raw paragraphs.\n",
    "    Returns:\n",
    "      corpus: A list containing every word in the corpus represented as a list\n",
    "        of characters and the special end-of-word-symbol \"</w>\".\n",
    "      vocabulary: The set of unique tokens in the dataset that serves as the\n",
    "        vocabulary before the first merge.\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = []\n",
    "    vocabulary = {EOW_SYMBOL}\n",
    "    for paragraph in dataset:\n",
    "        for word in paragraph.split(\" \"):\n",
    "            # Convert word into chars and add end-of-word symbol ('</w>').\n",
    "            chars =  list(word)\n",
    "            chars.append(EOW_SYMBOL)\n",
    "            corpus.append(chars)\n",
    "\n",
    "            # Append characters to the vocabulary. Since vocabulary is a set,\n",
    "            # it will ignore characters that have already been added.\n",
    "            vocabulary.update(chars)\n",
    "\n",
    "    return corpus, vocabulary\n",
    "\n",
    "corpus, vocabulary = bpe_initialize(dataset)\n",
    "\n",
    "# Print the first 10 \"words\" in the corpus.\n",
    "print(\"First 10 words: \")\n",
    "for word in corpus[:10]:\n",
    "    print(f\"  {word}\")\n",
    "\n",
    "print(\"\\n\\nVocabulary:\")\n",
    "print(f\"  {vocabulary}\")"
   ],
   "id": "8b88f22d74091c36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2: Counting adjacent token pairs\n",
    "\n",
    "For every word in the corpus, extract every pair of tokens that appear next to each other and increase their count.\n",
    "\n",
    "Ex. \"rice\" becomes `(r,i)`, `(i,c)`, `(c,e)`, `(e, </w>)`."
   ],
   "id": "8ce30f5d8119fc51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_pair_frequencies(\n",
    "    corpus: list[list[str]]\n",
    ") -> Counter[tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of adjacent character pairs in a corpus.\n",
    "    Args:\n",
    "      corpus: A list of tokenized words, where each word is represented as a\n",
    "        list of subword tokens. Before the first merge these are all individual\n",
    "        characters.\n",
    "    Returns:\n",
    "      A Counter object mapping each adjacent character pair (a tuple) to its\n",
    "        frequency in the corpus.\n",
    "    \"\"\"\n",
    "    pairs = Counter()\n",
    "    for word in corpus:\n",
    "        # Loop through the position of every token but the last one.\n",
    "        for i in range(len(word) - 1):\n",
    "            # Create a tuple representing the adjacent pair consisting of the\n",
    "            # i-th and (i+1)-th token in `word`.\n",
    "            pair =  word[i] + \" \" + word[i+1]\n",
    "            pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "\n",
    "pair_freqs = get_pair_frequencies(corpus)\n",
    "print(\"The 10 most common pair frequencies are:\")\n",
    "for freq in pair_freqs.most_common(10):\n",
    "    print(f\" {freq}\")"
   ],
   "id": "133f582444bcfcc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3: Merge the most frequent pair\n",
    "Identify the most frequent pair of adjacent tokens and merge them into a new subword token."
   ],
   "id": "a7c3ea1e0600b21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "most_freq_pair, freq = pair_freqs.most_common(1)[0]\n",
    "print(\n",
    "    f\"The most frequent pair is {most_freq_pair} with a frequency of {freq:,}.\"\n",
    ")\n",
    "\n",
    "new_token = most_freq_pair.replace(\" \", \"\")\n",
    "vocabulary.add(new_token)"
   ],
   "id": "491aa3b0c8f1c6dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_pair_in_word(\n",
    "    word: list[str], pair_to_merge: tuple[str, str]\n",
    ") -> list[str]:\n",
    "    \"\"\"Merges adjacent occurrences of a specified pair of characters in a word.\n",
    "\n",
    "    Given a word represented as a list of subword tokens and a pair of tokens to\n",
    "    merge (represented as a tuple), return a new list where every\n",
    "    instance of the specified pair appearing adjacently in the original word is\n",
    "    replaced by a single string representing the merged pair.\n",
    "\n",
    "    Args:\n",
    "      tokens: A list of subword tokens representing one space separated word.\n",
    "      pair_to_merge: A pair of two subword tokens that should be merged into one\n",
    "        subword token.\n",
    "\n",
    "    Returns:\n",
    "      New list of subword tokens representing the word after applying the merge.\n",
    "    \"\"\"\n",
    "    merged_symbol = pair_to_merge[0] + pair_to_merge[1]\n",
    "    i = 0\n",
    "    new_word = []\n",
    "    while i < len(word):\n",
    "        # If this position and the next match the pair, merge them.\n",
    "        if i < len(word) - 1 and (word[i], word[i + 1]) == pair_to_merge:\n",
    "            new_word.append(merged_symbol)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_word.append(word[i])\n",
    "            i += 1\n",
    "    return new_word\n",
    "\n",
    "print(f\"Before merging: {' '.join(corpus[0])}\")\n",
    "new_word = merge_pair_in_word(corpus[0], most_freq_pair)\n",
    "print(f\"After merging:  {' '.join(new_word)}\")"
   ],
   "id": "516638de678733bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Replace tokens in corpus\n",
    "\n",
    "Use the `merge_pair_in_word` to replace every occurrence of the most frequent pair with the new merged subword token."
   ],
   "id": "e5b8244fb005633e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "new_corpus = []\n",
    "for word in corpus:\n",
    "    new_word = merge_pair_in_word(word, most_freq_pair)\n",
    "    new_corpus.append(new_word)\n",
    "\n",
    "# Each iteration of BPE results in a new corpus with merged pairs.\n",
    "print(\n",
    "    \"The first 10 \\\"words\\\" in the corpus, with \\\"e</w>\\\" being a single token\"\n",
    "    \" now:\"\n",
    ")\n",
    "for word in new_corpus[:10]:\n",
    "    print(f\"  {word}\")\n",
    "\n",
    "print(\"\\n\\nVocabulary:\")\n",
    "print(f\"  {vocabulary}\")"
   ],
   "id": "f7d267eded7017c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Repeat",
   "id": "c9c674782c6e997"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def learn_bpe(\n",
    "    dataset: list[str], num_merges: int\n",
    ") -> tuple[list[tuple[str, str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Implements the basic BPE algorithm by iteratively merging the\n",
    "    most frequent adjacent character pairs in a corpus to create subword units.\n",
    "    It continues merging until the specified number of merges is reached or all\n",
    "    words are represented by a single token.\n",
    "\n",
    "    Args:\n",
    "      dataset: A list of paragraphs, where each sentence is a string.\n",
    "      num_merges: The desired number of BPE merge operations to perform.\n",
    "\n",
    "    Returns:\n",
    "      merges: A list of tuples, where each tuple represents a merged pair of\n",
    "        tokens. (e.g., [('a', 'b'), ('ab', 'c')]).  The order of tuples reflects\n",
    "        the order in which merges were performed.\n",
    "      vocabulary: The final vocabulary of all subword tokens.\n",
    "      corpus: The final corpus after BPE is applied.  Each sentence is\n",
    "        represented as a list of words, and each word is represented as a list\n",
    "        of subword tokens.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize corpus as list of lists of characters + </w>.\n",
    "    corpus, vocabulary = bpe_initialize(dataset)\n",
    "\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        # Step 2: Count all adjacent-pair frequencies.\n",
    "        pair_freqs = get_pair_frequencies(corpus)\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "\n",
    "        # Step 3: Pick the most frequent pair.\n",
    "        most_freq_pair, freq = pair_freqs.most_common(1)[0]\n",
    "        if freq < 1:\n",
    "            break  # No more pairs to merge.\n",
    "        merges.append(most_freq_pair)\n",
    "        new_token = most_freq_pair[0] + most_freq_pair[1]\n",
    "        vocabulary.add(new_token)\n",
    "\n",
    "        # Step 4: Merge that pair in every word of the corpus.\n",
    "        new_corpus = []\n",
    "        for word in corpus:\n",
    "            new_word = merge_pair_in_word(word, most_freq_pair)\n",
    "            new_corpus.append(new_word)\n",
    "        corpus = new_corpus\n",
    "\n",
    "    # Return the list of merges, the vocabulary, and the final tokenized corpus.\n",
    "    return merges, vocabulary, corpus"
   ],
   "id": "af4f09ec338b2554"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_text = [\n",
    "        \"desert\",\n",
    "        \"deserted\",\n",
    "        \"deserts\",\n",
    "        \"desert\",\n",
    "        \"tested\",\n",
    "        \"test\",\n",
    "        \"deserted\",\n",
    "        \"desert\",\n",
    "        \"desertion\",\n",
    "        \"desertion\",\n",
    "        \"function\",\n",
    "]\n",
    "# Learn BPE tokenizer with 10 merge operations.\n",
    "merges, vocabulary, tokenized_corpus = learn_bpe(sample_text, num_merges=10)\n",
    "\n",
    "print(\"Learned merges (in order):\")\n",
    "for i, pair in enumerate(merges, start=1):\n",
    "    print(f\" {i}. Merge {pair[0]!r} + {pair[1]!r}  →  {pair[0]+pair[1]!r}\")\n",
    "\n",
    "print(\"\\nFinal tokenized corpus:\")\n",
    "for orig, tokenized in zip(sample_text, tokenized_corpus):\n",
    "    print(f\"  {orig:8s} → {' '.join(tokenized)}\")"
   ],
   "id": "8d87009ddd3a2216"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
