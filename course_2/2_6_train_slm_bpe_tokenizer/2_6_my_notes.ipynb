{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Train an SLM with Your BPE Tokenizer\n",
    "## Purpose:\n",
    "- Learn merges for a BPE subword tokenizer\n",
    "- Use t-SNE dimensionality reduction to visualize embeddings\n",
    "\n",
    "### Topics:\n",
    "- BPE\n",
    "- Unicode characters\n",
    "\n",
    "### Steps\n",
    "* Load and inspect the Africa Galore dataset.\n",
    "* Train a BPE encoder on the Africa Galore dataset.\n",
    "* Encode and decode example words and sentences (including made-up words) to see how this tokenizer handles out-of-vocabulary (OOV) cases.\n",
    "* Convert the tokenized corpus into padded numerical index sequences required for model training.\n",
    "* Train the transformer model from the previous course on the dataset and observe how.\n",
    "* Visualize the learned embeddings of some of the tokens using the t-SNE algorithm.\n",
    "\n",
    "Date: 2026-02-21\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_6_train_an_slm_with_your_bpe_tokenizer.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "3537cb2c7881092f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "# Install the custom package for this course.\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "from collections import Counter # For counting tokens in the BPE tokenizer.\n",
    "import os # Used for setting Keras configuration variables.\n",
    "import string # For accessing string constants.\n",
    "\n",
    "# The following line provides configuration for Keras.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "import numpy as np # For working with vectors and matrices.\n",
    "import pandas as pd # For loading the Africa Galore dataset.\n",
    "import tqdm # For displaying progress bars.\n",
    "\n",
    "from ai_foundations import training # For defining and training the SLM.\n",
    "from ai_foundations import embeddings as emb # For visualizing embeddings."
   ],
   "id": "a0e722f7fe28f74d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The full BPE Tokenizer class, all in one place",
   "id": "440a74612d5aeafe"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BPEWordTokenizer:\n",
    "    \"\"\"\n",
    "    A Byte Pair Encoding (BPE) based subword tokenizer.\n",
    "\n",
    "    Supports encoding and decoding text to subword tokens using BPE.\n",
    "    Learns merge rules from a corpus or be initialized with a pre-built vocabulary.\n",
    "\n",
    "    Attributes:\n",
    "        vocabulary: List of subword tokens including special tokens.\n",
    "        vocabulary_size : Total number of tokens in vocabulary.\n",
    "        token_to_index: Mapping from tokens to indices.\n",
    "        index_to_token: Mapping from indices to tokens.\n",
    "        pad_token_id: Index of the padding token.\n",
    "        unknown_token_id: Index of the unknown token.\n",
    "        tokenized_corpus: Cached tokenized corpus after BPE training.\n",
    "    \"\"\"\n",
    "\n",
    "    UNKNOWN_TOKEN = \"<UNK>\"\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "    END_WORD = \"</w>\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: list[str] | str,\n",
    "        vocabulary: list[str] | None = None,\n",
    "        num_merges: int = 100,\n",
    "    ):\n",
    "        \"\"\"Initializes the BPEWordTokenizer.\n",
    "\n",
    "        If no vocabulary is specified, it extracts the unique tokens from the\n",
    "        text corpus and learns the BPE merges.\n",
    "\n",
    "        Args:\n",
    "          texts: A list of strings or a string representing the text corpus.\n",
    "          vocabulary: Optional list of strings with unique tokens.\n",
    "          num_merges: Defines how many rounds of merges should be performed\n",
    "            when learning the BPE merges.\n",
    "        \"\"\"\n",
    "\n",
    "        # Normalize to list of strings.\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        if vocabulary is None:\n",
    "            # Learn BPE merges and derive vocabulary from tokenized corpus.\n",
    "            self.merges, tokenized, vocabulary_set = self._learn_bpe(\n",
    "                texts, num_merges\n",
    "            )\n",
    "            self.tokenized_corpus = tokenized\n",
    "\n",
    "            # Ensure that basic alphanumeric characters are always included in\n",
    "            # the vocabulary.\n",
    "            required_chars = set(\n",
    "                string.ascii_lowercase + string.ascii_uppercase + string.digits\n",
    "            )\n",
    "\n",
    "            vocabulary_set.update(required_chars)\n",
    "\n",
    "            # Add special tokens to the vocabulary.\n",
    "            self.vocabulary = (\n",
    "                [self.PAD_TOKEN] + sorted(vocabulary_set) + [self.UNKNOWN_TOKEN]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.vocabulary = vocabulary\n",
    "            self.merges = []  # Skip merge logic when a vocabulary is provided.\n",
    "\n",
    "        # Build mappings and set IDs of special tokens.\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        self.token_to_index = {tok: i for i, tok in enumerate(self.vocabulary)}\n",
    "        self.index_to_token = {i: tok for i, tok in enumerate(self.vocabulary)}\n",
    "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
    "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
    "\n",
    "    def _split_text(self, text: str) -> list[str]:\n",
    "        \"\"\"Split a string into subword tokens using learned BPE merges.\n",
    "\n",
    "        Args:\n",
    "          text: String to split into subword tokens.\n",
    "\n",
    "        Returns:\n",
    "          List of subword tokens that together form the original text.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for word in text.strip().split():\n",
    "            # Split the string into characters and add special END_WORD token.\n",
    "            chars = list(word) + [self.END_WORD]\n",
    "\n",
    "            # Merge individual characters according to learned BPE merges.\n",
    "            for pair in self.merges:\n",
    "                chars = self._merge_pairs_in_word(chars, pair)\n",
    "            tokens.extend(chars)\n",
    "        return tokens\n",
    "\n",
    "    def join_text(self, tokens: list[str]) -> str:\n",
    "        \"\"\"Join subword tokens into full string, preserving word boundaries.\n",
    "\n",
    "        Args:\n",
    "          tokens: List of subword tokens to be joined.\n",
    "\n",
    "        Returns:\n",
    "          String obtained from joining the subword tokens.\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        current_word = []\n",
    "        for token in tokens:\n",
    "            # Check whether token ends with a word boundary marker.\n",
    "            if token.endswith(self.END_WORD):\n",
    "                current_word.append(token.replace(self.END_WORD, \"\"))\n",
    "                words.append(\"\".join(current_word))\n",
    "                current_word = []\n",
    "            else:\n",
    "                current_word.append(token)\n",
    "        if current_word:\n",
    "            words.append(\"\".join(current_word))\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Encode a string into list of token indices.\n",
    "\n",
    "        Args:\n",
    "            text: Input text.\n",
    "\n",
    "        Returns:\n",
    "            List of integers corresponding to tokens.\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        for token in self._split_text(text):\n",
    "            token_id = self.token_to_index.get(token, self.unknown_token_id)\n",
    "            token_ids.append(token_id)\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: int | list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode list of token IDs back to original text.\n",
    "\n",
    "        Args:\n",
    "          token_ids: Single index or list of token IDs.\n",
    "\n",
    "        Returns:\n",
    "          Decoded text string.\n",
    "        \"\"\"\n",
    "        # Covert to list if a single token index is specified.\n",
    "        if isinstance(token_ids, int):\n",
    "            token_ids = [token_ids]\n",
    "\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            tokens.append(\n",
    "                self.index_to_token.get(\n",
    "                    token_id,\n",
    "                    self.UNKNOWN_TOKEN + self.END_WORD,\n",
    "                )\n",
    "            )\n",
    "        return self.join_text(tokens)\n",
    "\n",
    "    def _get_pair_frequencies(self, corpus: list[list[str]]) -> Counter[str]:\n",
    "        \"\"\"Count all adjacent token pairs in corpus.\n",
    "\n",
    "        Args:\n",
    "          corpus: A list of lists of strings representing subword tokens.\n",
    "\n",
    "        Returns:\n",
    "          Counter mapping adjacent pairs of subword tokens to their frequencies.\n",
    "        \"\"\"\n",
    "        pairs = Counter()\n",
    "        for word in corpus:\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                # Increase the count by 1.\n",
    "                pairs[pair] += 1\n",
    "        return pairs\n",
    "\n",
    "    def _merge_pairs_in_word(\n",
    "        self, word: list[str], pair_to_merge: tuple[str, str]\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Merge all occurrences of a token pair inside a word.\n",
    "\n",
    "        Args:\n",
    "          tokens: A list of subword tokens representing one space separated\n",
    "            word.\n",
    "          pair_to_merge: A pair of two subword tokens that should be merged into\n",
    "            one subword token.\n",
    "\n",
    "        Returns:\n",
    "          New list of subword tokens representing the word after applying the\n",
    "            merge.\n",
    "        \"\"\"\n",
    "\n",
    "        merged_symbol = pair_to_merge[0] + pair_to_merge[1]\n",
    "        if pair_to_merge[0] not in word or pair_to_merge[1] not in word:\n",
    "            return word\n",
    "        i = 0\n",
    "        new_word = []\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and (word[i], word[i + 1]) == pair_to_merge:\n",
    "                new_word.append(merged_symbol)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        return new_word\n",
    "\n",
    "    def _learn_bpe(\n",
    "        self, corpus: list[str], num_merges: int\n",
    "    ) -> tuple[list[tuple[str, str]], list[list[list[str]]], set[str]]:\n",
    "        \"\"\"\n",
    "        Learn BPE merges from a corpus of texts.\n",
    "\n",
    "        Args:\n",
    "          corpus: List of input texts.\n",
    "          num_merges: Number of merge operations to perform.\n",
    "\n",
    "        Returns:\n",
    "            merges: List of merges in order they are learned to be performed.\n",
    "            tokenized_corpus: List of list of list of subword tokens where each\n",
    "              paragraph in the corpus is tokenized as a list of list of\n",
    "              subword tokens.\n",
    "            vocabulary_set: Set of subword tokens after performing all merges.\n",
    "        \"\"\"\n",
    "        # List of lists of lists to store tokenized text corpus.\n",
    "        tokenized_corpus = []\n",
    "        vocabulary = set([self.END_WORD])\n",
    "        for paragraph in corpus:\n",
    "            sentence_raw_tokens = []\n",
    "            for word in paragraph.strip().split():\n",
    "                # Split the word into characters and add word boundary marker.\n",
    "                sentence_raw_tokens.append(list(word) + [self.END_WORD])\n",
    "                vocabulary.update(list(word))\n",
    "            tokenized_corpus.append(sentence_raw_tokens)\n",
    "\n",
    "        merges = []\n",
    "        for _ in (pbar := tqdm.tqdm(range(num_merges), unit=\"merges\")):\n",
    "            # Build a one-dimensional list of all tokens in the corpus.\n",
    "            flat_corpus = []\n",
    "            for tokenized_paragraph in tokenized_corpus:\n",
    "                flat_corpus.extend(tokenized_paragraph)\n",
    "\n",
    "            # Find the most frequent pair of adjacent tokens.\n",
    "            pair_freqs = self._get_pair_frequencies(flat_corpus)\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "            most_freq_pair, freq = pair_freqs.most_common(1)[0]\n",
    "            if freq < 1:\n",
    "                break\n",
    "            merges.append(most_freq_pair)\n",
    "\n",
    "            # Apply merge to each token in each paragraph.\n",
    "            new_tokenized_corpus = []\n",
    "            for para_tokens in tokenized_corpus:\n",
    "                new_para_tokens = []\n",
    "                for word_tokens in para_tokens:\n",
    "                    new_para_tokens.append(\n",
    "                        self._merge_pairs_in_word(word_tokens, most_freq_pair)\n",
    "                    )\n",
    "                new_tokenized_corpus.append(new_para_tokens)\n",
    "            tokenized_corpus = new_tokenized_corpus\n",
    "            vocabulary.add(most_freq_pair[0] + most_freq_pair[1])\n",
    "            pbar.set_postfix(vocabulary_size=f\"{len(vocabulary):,}\")\n",
    "\n",
    "        return merges, tokenized_corpus, vocabulary"
   ],
   "id": "53a87ce6ce08e405"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train the BPE tokenizer\n",
    "Uses 3000 merges to completely tokenize the dataset."
   ],
   "id": "483446734d0c7c57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the Africa Galore dataset\n",
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"].values\n",
    "print(\"Loaded dataset with\", dataset.shape[0], \"paragraphs.\")"
   ],
   "id": "d13c0d3adb7dba7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_merges = 3000\n",
    "\n",
    "tokenizer = BPEWordTokenizer(dataset, num_merges=num_merges)\n",
    "print(f\"\\n\\nFinal tokenizer vocabulary size: {tokenizer.vocabulary_size:,}\\n\")"
   ],
   "id": "3babc508a0cf19d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Check the behavior on the first 20 words in the dataset\n",
    "\n",
    "Expected Output\n",
    "```\n",
    "The</w>\n",
    "Lago s</w>\n",
    "air</w>\n",
    "was</w>\n",
    "thick</w>\n",
    "with</w>\n",
    "humid ity,</w>\n",
    "but</w>\n",
    "the</w>\n",
    "energy</w>\n",
    "in</w>\n",
    "the</w>\n",
    "cl ub </w>\n",
    "was</w>\n",
    "electr ic.</w>\n",
    "The</w>\n",
    "band</w>\n",
    "la un ched</w>\n",
    "into</w>\n",
    "a</w>\n",
    "```"
   ],
   "id": "167237b050388a2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "africa_galore_tokenized = tokenizer.tokenized_corpus\n",
    "for tokens in africa_galore_tokenized[0][:20]:\n",
    "    print(\" \".join(tokens))"
   ],
   "id": "74ee9edb45b634e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenize unknown words\n",
    "Expected output\n",
    "```\n",
    "Decoded sentence from tokens: A Zimbabwian dish <UNK>.\n",
    "Token 63:\tA\n",
    "Token 388:\tZimbab\n",
    "Token 3003:\twi\n",
    "Token 500:\tan\n",
    "Token 1026:\tdish\n",
    "Token 3080:\t<UNK>\n",
    "Token 35:\t.\n",
    "```"
   ],
   "id": "ade207dcb91845fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "madeup_tokens = tokenizer.encode(\"A Zimbabwian dish ðŸ˜‹.\")\n",
    "\n",
    "print(f\"Decoded sentence from tokens: {tokenizer.decode(madeup_tokens)}\")\n",
    "for token in madeup_tokens:\n",
    "    decoded_token = tokenizer.decode(token)\n",
    "    print(f\"Token {token}:\\t{decoded_token}\")"
   ],
   "id": "d58ffd8c9cd308fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Convert data to token IDs\n",
    "Prepare dataset to train model."
   ],
   "id": "d2d935d1051828c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoded_tokens = []\n",
    "for paragraph in tqdm.tqdm(dataset, unit=\"paragraphs\"):\n",
    "    encoded_tokens.append(tokenizer.encode(paragraph))"
   ],
   "id": "a2ac246395b5fe03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Truncate * pad the sequences using keras.preprocessing.sequence.pad_sequences\n",
    "Truncate any paragraphs longer than 300 words & pad the rest."
   ],
   "id": "cf14681978467345"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_length = 300\n",
    "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "        encoded_tokens,\n",
    "        maxlen=max_length,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "        value=tokenizer.pad_token_id,\n",
    "    )"
   ],
   "id": "87df8fa617735659"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split the dataset to create the inputs and targets.",
   "id": "bb095b90d4af7479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare input and target for the transformer model.\n",
    "# For each example, extract all tokens except the last one.\n",
    "input_sequences = padded_sequences[:, :-1]\n",
    "# For each example, extract all tokens except the first one.\n",
    "target_sequences = padded_sequences[:, 1:]\n",
    "\n",
    "max_length = input_sequences.shape[1]"
   ],
   "id": "cb8996b311358a61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the model\n",
    "\n",
    "Initialize the model and trains for 100 epochs.\n",
    "Monitor progress by printing a statement every tenth epoch."
   ],
   "id": "fd9447d7fa026dc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set a seed for reproducibility.\n",
    "keras.utils.set_random_seed(3112)\n",
    "\n",
    "model = training.create_model(\n",
    "    max_length=max_length,\n",
    "    vocabulary_size=tokenizer.vocabulary_size,\n",
    "    learning_rate=8e-5\n",
    ")\n",
    "\n",
    "prompt = \"Jide\"\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "text_gen_callback = training.TextGenerator(\n",
    "    max_tokens=11, start_tokens=prompt_ids, tokenizer=tokenizer, print_every=10\n",
    ")\n",
    "\n",
    "num_epochs = 100\n",
    "# verbose=2: Instructs the model.fit method to print one line per\n",
    "# epoch so you can observe loss decreasing and the generated texts improving.\n",
    "history = model.fit(\n",
    "    x=input_sequences,\n",
    "    y=target_sequences,\n",
    "    verbose=2,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=2,\n",
    "    callbacks=[text_gen_callback]\n",
    ")"
   ],
   "id": "85e7fc68abe7d333"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualize embeddings with t-SNE\n",
   "id": "3f0df18c31060256"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract all embeddings from your model as a matrix.\n",
    "embeddings = model.trainable_weights[0].value\n",
    "\n",
    "# Define token categories.\n",
    "food_and_drink = [\n",
    "    \"water\",\n",
    "    \"coffee\",\n",
    "    \"onions\",\n",
    "    \"peanut\",\n",
    "    \"pepper\",\n",
    "    \"pudding\",\n",
    "    \"sauce\",\n",
    "    \"stew\",\n",
    "    \"carrots,\"\n",
    "]\n",
    "prepositions = [\"on\", \"in\", \"with\", \"for\", \"of\"]\n",
    "adjectives = [\"aromatic\", \"hot\"]\n",
    "countries = [\"Egypt\", \"Congo\", \"Ghana,\", \"Tanzania\"]\n",
    "\n",
    "# Define list of tokens and map them to categories for coloring them.\n",
    "tokens = food_and_drink + prepositions + adjectives + countries\n",
    "categories = (\n",
    "    [0] * len(food_and_drink)\n",
    "    + [1] * len(prepositions)\n",
    "    + [2] * len(adjectives)\n",
    "    + [3] * len(countries)\n",
    ")\n",
    "\n",
    "# Convert tokens into token IDs.\n",
    "token_ids = []\n",
    "for token in tokens:\n",
    "    token_ids.extend(tokenizer.encode(token))\n",
    "\n",
    "# Check that each token is represented as a single token in the tokenizer.\n",
    "assert len(token_ids) == len(tokens)\n",
    "\n",
    "# Extract embeddings for the set of tokens of interest.\n",
    "embeddings_subset = embeddings[token_ids, :]\n",
    "\n",
    "# Generate t-SNE plot with embeddings from your model.\n",
    "emb.plot_embeddings_tsne(embeddings_subset, tokens, categories)"
   ],
   "id": "f4f66c701878fbf3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
