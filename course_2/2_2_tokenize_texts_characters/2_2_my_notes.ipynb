{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Tokenize Texts into Characters and Words\n",
    "## Purpose:\n",
    "- Explore Statistical probability of token distribution\n",
    "- Explore tokenization strategies\n",
    "\n",
    "### Topics:\n",
    "- Zipf's Law\n",
    "- Token frequency\n",
    "- Preprocessing alters token frequency\n",
    "- Character-level tokenization\n",
    "- Word-level tokenization\n",
    "\n",
    "### Steps\n",
    "Preprocess and count tokens in the Africa Galore dataset.\n",
    "Visualize word frequency distributions in relation to Zipf's law.\n",
    "Observe the effects of character-level versus word-level tokenization on sequence length and vocabulary size.\n",
    "\n",
    "Date: 2026-02-20\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_2_tokenize_texts_into_characters_and_words.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "79fc652742e287f2"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "# Install the custom package for this course.\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "import re # For defining regular expressions.\n",
    "import pandas as pd # For loading the dataset.\n",
    "import textwrap # For making paragraphs more readable.\n",
    "from collections import Counter # For counting tokens.\n",
    "\n",
    "from ai_foundations import visualizations # For visualizations.\n",
    "from ai_foundations.feedback.course_2 import tokenize # For providing feedback."
   ],
   "id": "988d1a00c3fb879d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the Africa Galore dataset.\n",
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"].values\n",
    "print(\"Loaded dataset with\", dataset.shape[0], \"paragraphs.\\n\")\n",
    "print(f\"The first paragraph is:\\n{textwrap.fill(dataset[0])}\")"
   ],
   "id": "9cf3405ad948640f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocess Text & Count Tokens\n",
    "1.  The `preprocess_text()` removes punctuation so punctuation marks do not interfere with token interpretation.\n",
    "2. The `preprocess_text()` converts all text to lowercase."
   ],
   "id": "41b076502f8b9ae4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_text(paragraphs: list[str]) -> list[str]:\n",
    "    \"\"\"Preprocesses a list of text paragraphs.\n",
    "\n",
    "    Lowercases & tokenizes text, removing punctuation.\n",
    "    Use regex for more precise tokenization than a space tokenizer.\n",
    "    Args:\n",
    "      paragraphs: A list of strings, where each string represents a paragraph\n",
    "        of text.\n",
    "    Returns:\n",
    "      A list of strings, where each string is a lowercase token extracted from\n",
    "        the input paragraphs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text to lower case.\n",
    "    paragraphs = [text.lower() for text in paragraphs]\n",
    "\n",
    "    tokens_list = []\n",
    "    # The regular expression (r'\\b\\w+\\b') splits a paragraph on word boundaries\n",
    "    # to remove punctuation. This breaks the text into individual words while\n",
    "    # handling punctuation and spacing more precisely than `.split()`.\n",
    "    for paragraph in paragraphs:\n",
    "        for token in re.findall(r'\\b\\w+\\b', paragraph):\n",
    "            tokens_list.append(token)\n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "# Process all paragraphs in the dataset.\n",
    "tokens_list = preprocess_text(dataset)\n",
    "print(tokens_list[:10])"
   ],
   "id": "ac573ea21bf48606"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compute token counts\n",
    "\n",
    "Next, you will compute how many times each token appears in the dataset.\n",
    "\n",
    "------\n",
    ">\n",
    "> > Complete the implementation of the `get_token_counts()`.\n",
    "> It should return a [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object where the keys are tokens and the values are the frequency of the token in `tokens_list`.\n",
    ">\n",
    "------"
   ],
   "id": "1de8ba2428da8561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_token_counts(tokens_list: list[str]) -> Counter[str]:\n",
    "    \"\"\"Calculates the frequency of each token in a list of tokens.\n",
    "    Args:\n",
    "      tokens_list: A list of string tokens.\n",
    "\n",
    "    Returns:\n",
    "      A Counter where keys are the unique tokens and values are their\n",
    "        corresponding frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add your code here.\n",
    "    token_counts = Counter(tokens_list)\n",
    "\n",
    "    return token_counts\n",
    "\n",
    "token_counts = get_token_counts(tokens_list)\n",
    "\n",
    "# Print the 10 tokens with the highest counts.\n",
    "list(token_counts.most_common(10))\n",
    "\n",
    "# Print the 10 least common words in the African Galore dataset.\n",
    "# Note that most_common() returns all items in descending order of their count.\n",
    "list(token_counts.most_common())[-10:]"
   ],
   "id": "9ac1a6f35a7164b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Zipf's law\n",
    "\n",
    "A very small number of tokens (like \"the\", \"a\", \"and\") are extremely common, while the vast majority are rare, often appearing only once.\n",
    "This creates a \"long tail\" distribution.\n",
    "\n",
    "**log-log plot**: Solves the problem of the long tail when plotting distribution frequency.\n",
    "- Plots the logarithm of the rank against the logarithm of the frequency."
   ],
   "id": "ecad2961b2ffdd6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_text = dataset[0]\n",
    "print(textwrap.fill(sample_text))"
   ],
   "id": "d3f33f9401d1699f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenization\n",
    "Begin making decisions about breaking up text data into smaller \"units,\" then translate to token IDs. Use the IDs as the input to the language model.\n",
    "\n",
    "**Character-level tokenization**: Solves the out-of vocabulary problem by rendering every character meaningless."
   ],
   "id": "d118a26f6b9eae98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def character_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Splits text on characters.\n",
    "    Args:\n",
    "      text: The text to split.\n",
    "    Returns:\n",
    "      A list of tokens.\n",
    "    \"\"\"\n",
    "    tokens = list(text)\n",
    "    return tokens\n",
    "\n",
    "print(character_tokenize(dataset[0])[:10])"
   ],
   "id": "6491ed9eef91f7ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Word-level tokenization**: Splitting on spaces results in many tokens that are variations on the same word.",
   "id": "ca72891bb58a5d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def space_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Splits text on spaces.\n",
    "\n",
    "    Args:\n",
    "      text: The text to split.\n",
    "\n",
    "    Returns:\n",
    "      A list of tokens.\n",
    "    \"\"\"\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "print(space_tokenize(dataset[0])[:10])"
   ],
   "id": "f2cd6dc7f123c8ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization and Sequence Length\n",
    "- Tokenization method impacts number of tokens and vocabulary size inversely.\n",
    "- Sequence length affects memory and compute demands proportionately."
   ],
   "id": "f455eae5d8447298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# character-based tokenizer\n",
    "tokens_char = character_tokenize(dataset[0])\n",
    "print(tokens_char)\n",
    "print(f\"The length of the sequence is: {len(tokens_char)}\")\n",
    "\n",
    "# word-based tokenizer\n",
    "tokens_word = space_tokenize(dataset[0])\n",
    "print(tokens_word)\n",
    "print(f\"The length of the sequence is: {len(tokens_word)}\")"
   ],
   "id": "10f10166a3931f7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenization & Vocabulary Size\n",
    "* A model can neither process nor generate any token outside its vocabulary.\n",
    "1. Larger vocabularies allow more information to be distributed across tokens.\n",
    "\n",
    "With a **word-level vocabulary**, the distinct meanings of \"time\" and \"the\" are captured in separate, specific token representations. Information is clearly distributed.\n",
    "\n",
    "With a **character-level vocabulary**, each token must contribute to representing every word containing it. For example, \"t\" is part of \"time\", \"the\", and \"train.\" This forces a single token's parameters to hold a vast amount of contextual information, making it difficult for the model to learn representations that capture precise meaning.\n",
    "\n",
    "2. Larger vocabularies **require more parameters for the model**, directly increasing size and computational cost. Each token needs a unique representation stored in the model's parameters. Bigger vocabularies lead to larger models that demand more memory and processing power, making both training and inference slower and more expensive.\n",
    "\n",
    "3. Larger vocabularies contain **more tokens that appear very infrequently** in training data. The model cannot learn a reliable representation for these rare tokens because it lacks sufficient examples of their usage. This \"data sparsity\" problem hinders the model's ability to handle less common words or concepts effectively."
   ],
   "id": "664cacc6bae10c11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Print the unique number of tokens in character and word level tokenization.\n",
    "vocab_char = set(tokens_char)\n",
    "vocab_word = set(tokens_word)\n",
    "print(f\"The character-level vocabulary consists of {len(vocab_char):,} tokens.\")\n",
    "print(f\"The word-level vocabulary consists of {len(vocab_word):,} tokens.\")"
   ],
   "id": "faa45236ceca0baf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
