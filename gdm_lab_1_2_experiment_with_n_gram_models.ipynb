{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Experiment With N-Gram Models\n",
    "## Purpose:\n",
    "- Estimate next-word probabilities\n",
    "- Build a (small) n-gram model on a (tiny) dataset.\n",
    "- Understand n-gram models & their limitations\n",
    "### Topics:\n",
    "- Tokenization\n",
    "- Probability estimation\n",
    "- Token prediction\n",
    "\n",
    "Date: 2026-02-14\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_2_experiment_with_n_gram_models.ipynb#scrollTo=pbtgZxrpjm6j\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GH repo from DeepMind used in AI training courses at the university & college level."
   ],
   "id": "96ce43ec4b3ab3a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the math\n",
    "**N-gram**: A continuous sequence of $n$ words.\n",
    "\n",
    "**Context**: The preceding sequence of $n-1$ words.\n",
    "\n",
    "**How are n-grams related to the context?** N-gram models use n-grams to estimate the probability of the next word based on the context.\n",
    "\n",
    "**Text Corpus**: A dataset consisting of a collection of texts\n",
    "\n",
    "Computing the Probability of the next word\n",
    "---\n",
    "Given $\\mbox{A}$ is the context\n",
    "\n",
    "Given $\\mbox{B}$ is the next word\n",
    "\n",
    "Compute the probability $P(\\mbox{B} \\mid \\mbox{A})$:\n",
    "\n",
    "$$P(\\mbox{B} \\mid \\mbox{A}) = \\frac{\\mbox{Count}(\\mbox{A B})}{\\mbox{Count}(\\mbox{A})}$$\n",
    "\n",
    "The full n-gram counts, $\\mbox{ Count}(\\mbox{A B})$, and the context n-gram counts, $\\mbox{ Count}(\\mbox{A})$, can be computed by counting n-grams in a dataset (**text corpus**)."
   ],
   "id": "ea6e8358739e0bae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Set up the environment\n",
    "- for information only\n",
    "\n",
    "sudo apt update\n",
    "\n",
    "sudo apt install python3.12\n",
    "\n",
    "sudo apt install python3.12-venv\n",
    "\n",
    "python3.12 -m venv .venv\n",
    "\n",
    "source .venv/bin/activate\n",
    "\n",
    "cd .venv/bin"
   ],
   "id": "d7517408a7a3ca33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:58:51.898735300Z",
     "start_time": "2026-02-14T16:58:51.853632900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "! pip3.12 install \"git+https://github.com/google-deepmind/ai-foundations.git@main\""
   ],
   "id": "b1f397b03a5e66d4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T16:59:03.032250400Z",
     "start_time": "2026-02-14T16:59:02.965642500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Packages used.\n",
    "import random # For sampling from probability distributions.\n",
    "from collections import Counter, defaultdict # For counting n-grams.\n",
    "\n",
    "import textwrap # For automatically addding linebreaks to long texts.\n",
    "import pandas as pd # For construction and visualizing tables.\n",
    "\n",
    "# Custom functions for providing feedback on your solutions.\n",
    "from ai_foundations.feedback.course_1 import ngrams"
   ],
   "id": "7271a2fd246bae25",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ai_foundations'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m \u001B[38;5;66;03m# For construction and visualizing tables.\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Custom functions for providing feedback on your solutions.\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mai_foundations\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfeedback\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcourse_1\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ngrams\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'ai_foundations'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Africa Galore dataset\n",
    "Specialized dataset containing information on African culture, history, & geography generated by Gemini. The use of Gemini to create the dataset is supposed to ensure clean data by removing noise and inconsistencies."
   ],
   "id": "a6ff571dbe21f1bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"]\n",
    "print(f\"The dataset consists of {dataset.shape[0]} paragraphs.\")"
   ],
   "id": "b4eaff1acb3cc97f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inspect first 10 paragraphs in dataset\n",
    "for paragraph in dataset[:10]:\n",
    "    # textwrap automatically adds linebreaks to make long texts more readable.\n",
    "    formatted_paragraph = textwrap.fill(paragraph)\n",
    "    print(f\"{formatted_paragraph}\\n\")"
   ],
   "id": "a64361616438845a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### About Tokenization\n",
    "The above paragraphs are a single continuous string. The next function will split the strings on spaces to produce tokens; however, splitting on spaces does not take punctuation into account, therefore, a token may be the same as a word, but not always."
   ],
   "id": "28d93125bea1b859"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def space_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Splits a string into a list of words (tokens).\n",
    "    Splits text on space.\n",
    "    Args:\n",
    "        text: The input text.\n",
    "    Returns:\n",
    "        A list of tokens. Returns empty list if text is empty or all spaces.\n",
    "    \"\"\"\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "# Tokenize an example text with the `space_tokenize` function.\n",
    "space_tokenize(\"Kanga, a colorful printed cloth is more than just a fabric.\")"
   ],
   "id": "bb7e80f79a9d16d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test it\n",
    "space_tokenize(dataset[0])"
   ],
   "id": "d640f5d15f1004a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Coding activity 1\n",
    "space_tokenize() creates a list of tokens; however, the conditional probability of any token $\\mbox{B}$ following the preceding context $\\mbox{A}$, $P(\\mbox{B} \\mid \\mbox{A})$, relies on how often any **n-grams** and **(n-1)-grams** appear in the dataset.\n",
    "\n",
    "---\n",
    "- Use space_tokenize() to create a list of n-grams of length *n* for a text.\n",
    "- Represent each n-gram as a tuple using tuple()."
   ],
   "id": "1c676c4a7693ded0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_unigrams = []\n",
    "all_bigrams = []\n",
    "all_trigrams = []\n",
    "\n",
    "def generate_ngrams(text: str, n: int) -> list[tuple[str]]:\n",
    "    \"\"\"Generates n-grams from a given text.\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "        n: The size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "    Returns:\n",
    "        A list of n-grams, each represented as a list of tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize text.\n",
    "    tokens = ...  # Add your code here.\n",
    "\n",
    "    # Construct the list of n-grams.\n",
    "    ngrams = []\n",
    "\n",
    "    # Add your code here.\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "for paragraph in dataset:\n",
    "    # Calling `generate_ngrams` with n=1 constructs a list of unigrams.\n",
    "    all_unigrams.extend(generate_ngrams(paragraph, n=1))\n",
    "    # Calling `generate_ngrams` with n=2 constructs a list of bigrams (2-grams).\n",
    "    all_bigrams.extend(generate_ngrams(paragraph, n=2))\n",
    "    # Calling `generate_ngrams` with n=2 constructs a list of trigram (3-grams).\n",
    "    all_trigrams.extend(generate_ngrams(paragraph, n=3))\n",
    "\n",
    "print(\"First 10 Unigrams:\", all_unigrams[:10])\n",
    "print(\"First 10 Bigrams:\", all_bigrams[:10])\n",
    "print(\"First 10 Trigrams:\", all_trigrams[:10])"
   ],
   "id": "39f98323011b3466"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
