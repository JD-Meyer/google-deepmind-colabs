{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Prepare The Dataset For Training an SLM\n",
    "## Purpose:\n",
    "- Learn preprocessing steps to train a xformer SLM\n",
    "- Tokenize text\n",
    "- Generate primitive embeddings\n",
    "### Topics:\n",
    "- Tokenization\n",
    "- Vocabulary creation\n",
    "- Embeddings\n",
    "### Steps\n",
    "Load & tokenize the dataset.\n",
    "List tokens in the dataset.\n",
    "List unique tokens in the dataset.\n",
    "Map tokens to token IDs and vice-versa.\n",
    "Translate between tokens and their corresponding IDs w/ functions.\n",
    "Wrap it all in a class that encapsulates all methods necessary for preparing the data for a transformer model.\n",
    "\n",
    "Date: 2026-02-19\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_4_prepare_the_dataset_for_training_a_slm.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "a27475b28befc65b"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "import re # Used for splitting strings on spaces.\n",
    "\n",
    "# Packages used.\n",
    "import pandas as pd # For reading the dataset.\n",
    "import textwrap # For adding linebreaks to paragraphs.\n",
    "\n",
    "# For providing feedback.\n",
    "from ai_foundations.feedback.course_1 import slm"
   ],
   "id": "84d1e0902dcddeab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load & Tokenize\n",
    "w/ Pandas"
   ],
   "id": "79fc0ff12c772760"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"]\n",
    "print(f\"Loaded Africa Galore dataset with {len(dataset)} paragraphs.\")\n",
    "print(f\"\\nFirst paragraph:\")\n",
    "print(textwrap.fill(dataset[0]))"
   ],
   "id": "18b66e95db2f987c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Tokenize\n",
    "def tokenizer(text: str) -> list[str]:\n",
    "    \"\"\"Creates a list of tokens by splitting a string on spaces.\n",
    "    Args:\n",
    "        text: The input text.\n",
    "    Returns:\n",
    "        A list of tokens. Returns empty list if text is empty or all spaces.\n",
    "    \"\"\"\n",
    "    # Use `re` package so that splitting on multiple spaces also works.\n",
    "    tokens = re.split(r\" +\", text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize an example text with tokenizer().\n",
    "tokenizer(\"Kanga, a colorful printed cloth is more than just a fabric.\")"
   ],
   "id": "990fc380c7276c7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### List all tokens",
   "id": "5f3f28d736bd247f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the list of tokens\n",
    "tokens = []\n",
    "\n",
    "# loop through each para in the dataset\n",
    "for paragraph in dataset:\n",
    "    # tokenizer returns a list. Add each item in list to tokens[]\n",
    "    # I could loop through the lists or use extend()\n",
    "    tokens.extend(tokenizer(paragraph))\n",
    "\n",
    "print(f\"Total number of tokens in the Africa Galore dataset: {len(tokens):,}\")\n",
    "\n",
    "# Print first 30 tokens sample\n",
    "tokens[:30]\n",
    "\n",
    "# test\n",
    "slm.test_build_tokens_list(tokens, tokenizer, dataset)"
   ],
   "id": "92ecfe1074138be8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### List unique tokens\n",
    "(5260)"
   ],
   "id": "97ac3a47226199d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_vocabulary(tokens: list[str]) -> list[str]:\n",
    "    # Build a vocabulary list from the set of tokens.\n",
    "    vocabulary = list(set(tokens))\n",
    "    return vocabulary\n",
    "\n",
    "slm.test_build_vocabulary(build_vocabulary)"
   ],
   "id": "c3cb8eb4a21bacc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vocabulary = build_vocabulary(tokens)\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "print(\n",
    "    \"Total number of unique tokens in the Africa Galore dataset:\"\n",
    "    f\" {vocabulary_size:,}\"\n",
    ")"
   ],
   "id": "ad42678c46c7c4ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create an index for each token\n",
    "- IDs should always be consecutive\n",
    "- Use a dictionary for each mapping"
   ],
   "id": "1196bff304ca33b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build the `token_to_index` dictionary.\n",
    "# enumerate() is faster than a for loop\n",
    "token_to_index = {}\n",
    "\n",
    "for index, token in enumerate(vocabulary):\n",
    "    token_to_index[token] = index"
   ],
   "id": "455bb4f101c4d09c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build the `token_to_index` dictionary.\n",
    "index_to_token = {}\n",
    "\n",
    "for index, token in enumerate(vocabulary):\n",
    "    index_to_token[index] = token"
   ],
   "id": "a6ca508bfbbdc0b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Encoding and decoding\n",
    "- encode() takes a string of text and returns the corresponding indices.\n",
    "- decode() takes a list of indices and returns the corresponding text."
   ],
   "id": "e5658e8a6f51858a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"Encodes a text sequence into a list of indices based on the vocabulary.\n",
    "    Args:\n",
    "        text: The input text to be encoded.\n",
    "    Returns:\n",
    "        A list of indices corresponding to the tokens in the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tokens into indices.\n",
    "    indices = []\n",
    "    for token in tokenizer(text):\n",
    "        token_index = token_to_index.get(token)\n",
    "        indices.append(token_index)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def decode(indices: int | list[int]) -> list[str]:\n",
    "    \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
    "    Args:\n",
    "        indices: A single index or a list of indices to be decoded into tokens.\n",
    "    Returns:\n",
    "        str: A string of decoded tokens corresponding to the input indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # If a single integer is passed, convert it into a list.\n",
    "    if isinstance(indices, int):\n",
    "        indices = [indices]\n",
    "\n",
    "    # Map indices to tokens.\n",
    "    tokens = []\n",
    "    for index in indices:\n",
    "        token = index_to_token.get(index)\n",
    "        tokens.append(token)\n",
    "\n",
    "    # Join the decoded tokens with spaces.\n",
    "    return \" \".join(tokens)"
   ],
   "id": "be56d278242746ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text = dataset[0]\n",
    "print(text)"
   ],
   "id": "a14477ed0a827a0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wrap it all into a class",
   "id": "e041a53f4624dcc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleWordTokenizer:\n",
    "    \"\"\"A simple word tokenizer that can be initialized with a corpus of texts\n",
    "       or using a provided vocabulary list.\n",
    "\n",
    "    The tokenizer splits the text on spaces,\n",
    "    encode() converts the text to indices.\n",
    "    decode() converts indices to text.\n",
    "\n",
    "    Typical usage example:\n",
    "        corpus = \"Hello there!\"\n",
    "        tokenizer = SimpleWordTokenizer(corpus)\n",
    "        print(tokenizer.encode('Hello'))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
    "        \"\"\"Initialize tokenizer with texts in corpus or with a vocabulary.\n",
    "        Args:\n",
    "            corpus: Input text dataset.\n",
    "            vocabulary: A pre-defined vocabulary. If None,\n",
    "                the vocabulary is automatically inferred from the texts.\n",
    "        \"\"\"\n",
    "\n",
    "        if vocabulary is None:\n",
    "            # Build the vocabulary from scratch.\n",
    "            if isinstance(corpus, str):\n",
    "                corpus = [corpus]\n",
    "\n",
    "            # Convert text sequence to tokens.\n",
    "            tokens = []\n",
    "            for text in corpus:\n",
    "                for token in self.tokenizer(text):\n",
    "                    tokens.append(token)\n",
    "\n",
    "            # Create a vocabulary of unique tokens.\n",
    "            self.vocabulary = self.build_vocabulary(tokens)\n",
    "\n",
    "        else:\n",
    "            self.vocabulary = vocabulary\n",
    "\n",
    "        # Size of vocabulary.\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "        # Create token-to-index and index-to-token mappings.\n",
    "        self.token_to_index = {}\n",
    "        self.index_to_token = {}\n",
    "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
    "        # assigns a unique index to each token.\n",
    "        for index, token in enumerate(self.vocabulary):\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "\n",
    "    def tokenizer(self, text: str) -> list[str]:\n",
    "        \"\"\"Splits text on space into tokens.\n",
    "        Args:\n",
    "            text: Text to split on space.\n",
    "        Returns:\n",
    "            List of tokens after splitting `text`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use re.split such that multiple spaces are treated as a single\n",
    "        # separator.\n",
    "        return re.split(\" +\", text)\n",
    "\n",
    "    def join_text(self, text_list: list[str]) -> str:\n",
    "        \"\"\"Combines a list of tokens into a single string, with tokens separated\n",
    "           by spaces.\n",
    "        Args:\n",
    "            text_list: List of tokens to be joined.\n",
    "        Returns:\n",
    "            String with all tokens joined with a space.\n",
    "        \"\"\"\n",
    "        return \" \".join(text_list)\n",
    "\n",
    "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
    "        \"\"\"Create a vocabulary list from the list of tokens.\n",
    "        Args:\n",
    "            tokens: The list of tokens in the dataset.\n",
    "        Returns:\n",
    "            List of unique tokens (vocabulary) in the dataset.\n",
    "        \"\"\"\n",
    "        return sorted(list(set(tokens)))\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Encodes a text sequence into a list of indices.\n",
    "        Args:\n",
    "            text: The input text to be encoded.\n",
    "        Returns:\n",
    "            A list of indices corresponding to the tokens in the input text.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert tokens into indices.\n",
    "        indices = []\n",
    "        for token in self.tokenizer(text):\n",
    "            token_index = self.token_to_index.get(token)\n",
    "            indices.append(token_index)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: int | list[int]) -> str:\n",
    "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
    "        Args:\n",
    "            indices: A single index or a list of indices to be decoded into\n",
    "                tokens.\n",
    "        Returns:\n",
    "            str: A string of decoded tokens corresponding to the input indices.\n",
    "        \"\"\"\n",
    "\n",
    "        # If a single integer is passed, convert it into a list.\n",
    "        if isinstance(indices, int):\n",
    "            indices = [indices]\n",
    "\n",
    "        # Map indices to tokens.\n",
    "        tokens = []\n",
    "        for index in indices:\n",
    "            token = self.index_to_token.get(index)\n",
    "            tokens.append(token)\n",
    "\n",
    "        # Join the decoded tokens into a single string.\n",
    "        return self.join_text(tokens)"
   ],
   "id": "fc6f42110b08d4f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
