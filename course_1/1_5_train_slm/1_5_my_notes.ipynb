{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Train Your Own Small Language Model (SLM)\n",
    "## Purpose:\n",
    "- Apply the pre-processing steps from 1_4\n",
    "- Prepare data\n",
    "- Train SLM\n",
    "- Observe results\n",
    "### Topics:\n",
    "- Tokenization\n",
    "- Vocabulary creation\n",
    "- Embeddings\n",
    "- Keras\n",
    "### Steps\n",
    "- Load & tokenize the dataset.\n",
    "- Map tokens to token IDs and vice-versa.\n",
    "- Create same-length sequences by padding them.\n",
    "- Shuffle examples & group into batches\n",
    "- Transform the data into model inputs and model targets.\n",
    "- Train the transformer model.\n",
    "\n",
    "Date: 2026-02-19\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_5_train_your_own_small_language_model.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "4a73c3e3d5944997"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "# Packages used.\n",
    "import os # Used for setting Keras configuration variables.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # Set a parameter for Keras.\n",
    "import re # Used for splitting text on whitespace.\n",
    "\n",
    "import keras # Used for defining an training the model.\n",
    "import pandas as pd # Used for loading the dataset.\n",
    "import tensorflow as tf # Used for shuffling the dataset.\n",
    "\n",
    "# Used for displaying nicer error messages.\n",
    "from IPython.display import display, HTML\n",
    "from ai_foundations import training # For training your model.\n",
    "from ai_foundations import generation # For prompting your model.\n",
    "from ai_foundations import visualizations # For visualizing probabilities.\n",
    "from ai_foundations.feedback.course_1 import slm # For providing feedback.\n",
    "\n",
    "# The following line provides configuration for Keras.\n",
    "keras.utils.set_random_seed(812)  # For Keras layers."
   ],
   "id": "49aab36ed1ef8cd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load dataset",
   "id": "e17de63f59e4a300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"].values\n",
    "print(\"Loaded dataset with\", dataset.shape[0], \"paragraphs.\")"
   ],
   "id": "52050dc6b5a167b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenize dataset",
   "id": "dbee4893e183cc3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleWordTokenizer:\n",
    "    \"\"\"A simple word tokenizer.\n",
    "\n",
    "    Splits text sequence based on whitespace, using\n",
    "    encode() converts text to indices\n",
    "    decode() converts indices to text\n",
    "\n",
    "    Can be initialized w/ a corpus or a vocabulary list.\n",
    "\n",
    "    Typical usage example:\n",
    "        corpus = \"Hello there!\"\n",
    "        tokenizer = SimpleWordTokenizer(text)\n",
    "        print(tokenizer.encode('Hello'))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define constants.\n",
    "    UNKNOWN_TOKEN = \"<UNK>\"\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
    "        \"\"\"Initializes the tokenizer with texts in corpus or with a vocabulary.\n",
    "\n",
    "        Args:\n",
    "          corpus: Input text dataset.\n",
    "          vocabulary: A pre-defined vocabulary. If None,\n",
    "              the vocabulary is automatically inferred from the texts.\n",
    "        \"\"\"\n",
    "\n",
    "        if vocabulary is None:\n",
    "            # Build the vocabulary from scratch.\n",
    "            if isinstance(corpus, str):\n",
    "                corpus = [corpus]\n",
    "\n",
    "            # Convert text sequence to tokens.\n",
    "            tokens = []\n",
    "            for text in corpus:\n",
    "                for token in self.space_tokenize(text):\n",
    "                    tokens.append(token)\n",
    "\n",
    "            # Create a vocabulary comprising of unique tokens.\n",
    "            vocabulary = self.build_vocabulary(tokens)\n",
    "\n",
    "            # Add special unknown and pad tokens to the vocabulary list.\n",
    "            self.vocabulary = (\n",
    "                [self.PAD_TOKEN] + vocabulary + [self.UNKNOWN_TOKEN]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.vocabulary = vocabulary\n",
    "\n",
    "        # Size of vocabulary.\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "        # Create token-to-index and index-to-token mappings.\n",
    "        self.token_to_index = {}\n",
    "        self.index_to_token = {}\n",
    "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
    "        # assigns a unique index to each token.\n",
    "        for index, token in enumerate(self.vocabulary):\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "\n",
    "        # Map the special tokens to their IDs.\n",
    "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
    "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
    "\n",
    "    def space_tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Splits a given text on whitespace into tokens.\n",
    "        Args:\n",
    "            text: Text to split on whitespace.\n",
    "        Returns:\n",
    "            List of tokens after splitting `text`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use re.split such that multiple spaces are treated as a single\n",
    "        # separator.\n",
    "        return re.split(\" +\", text)\n",
    "\n",
    "    def join_text(self, text_list: list[str]) -> str:\n",
    "        \"\"\"Combines a list of tokens into a single string.\n",
    "        The combined tokens, as a single string, are separated by spaces in the\n",
    "        string.\n",
    "        Args:\n",
    "            text_list: List of tokens to be joined.\n",
    "        Returns:\n",
    "            String with all tokens joined with a whitespace.\n",
    "        \"\"\"\n",
    "        return \" \".join(text_list)\n",
    "\n",
    "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
    "        \"\"\"Create a vocabulary list from the list of tokens.\n",
    "        Args:\n",
    "            tokens: The list of tokens in the dataset.\n",
    "        Returns:\n",
    "            List of unique tokens (vocabulary) in the dataset.\n",
    "        \"\"\"\n",
    "        return sorted(list(set(tokens)))\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Encodes a text sequence into a list of indices.\n",
    "        Args:\n",
    "            text: The input text to be encoded.\n",
    "        Returns:\n",
    "            A list of indices corresponding to the tokens in the input text.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert tokens into indices.\n",
    "        indices = []\n",
    "        unk_index = self.token_to_index[self.UNKNOWN_TOKEN]\n",
    "        for token in self.space_tokenize(text):\n",
    "            token_index = self.token_to_index.get(token, unk_index)\n",
    "            indices.append(token_index)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: int | list[int]) -> str:\n",
    "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
    "        Args:\n",
    "            indices: A single index or a list of indices to be\n",
    "                decoded into tokens.\n",
    "        Returns:\n",
    "            A string of decoded tokens corresponding to the input indices.\n",
    "        \"\"\"\n",
    "\n",
    "        # If a single integer is passed, convert it into a list.\n",
    "        if isinstance(indices, int):\n",
    "            indices = [indices]\n",
    "\n",
    "        # Map indices to tokens.\n",
    "        tokens = []\n",
    "        for index in indices:\n",
    "            token = self.index_to_token.get(index, self.unknown_token_id)\n",
    "            tokens.append(token)\n",
    "\n",
    "        # Join the decoded tokens into a single string.\n",
    "        return self.join_text(tokens)\n",
    "\n",
    "\n",
    "# Initialize the tokenizer. This will build the tokenizer's vocabulary with\n",
    "# all the tokens that appear in the dataset.\n",
    "tokenizer = SimpleWordTokenizer(dataset)\n",
    "\n",
    "# Translate all tokens to their corresponding IDs.\n",
    "encoded_tokens = []\n",
    "for text in dataset:\n",
    "    # Split text into tokens and translate the tokens to token IDs.\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    encoded_tokens.append(token_ids)"
   ],
   "id": "561b5446c454e31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pad the dataset\n",
    "Create a matrix containing the indices of each token in the dataset.\n",
    "- Each paragraph in the dataset will constitute a matrix.\n",
    "- All matrices must be of same size, so <PAD> will be used as a token to match length of longest paragraph.\n",
    "    - Alternate methods include\n",
    "        - truncating all paragraphs to length of shortest paragraph\n",
    "        - selecting an arbitrary paragraph length, truncating long ones and padding short ones (most common method)."
   ],
   "id": "21aa496f3ce55308"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compute length statistics\n",
    "Length of the shortest paragraph is: 26\n",
    "Length of the longest paragraph is: 318"
   ],
   "id": "271a02c2d5cd8b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(f\"Length of first paragraph: {len(encoded_tokens[0]):,}\")\n",
    "\n",
    "paragraph_lengths = []\n",
    "for i in range(dataset.shape[0] - 1):\n",
    "  # print(len(encoded_tokens[i]))\n",
    "  paragraph_lengths.append(len(encoded_tokens[i]))\n",
    "\n",
    "paragraph_lengths.sort()\n",
    "# print(paragraph_lengths)\n",
    "\n",
    "# Add your code to compute the length of the shortest paragraph here.\n",
    "shortest_paragraph_length = paragraph_lengths[0]\n",
    "\n",
    "# Add your code to compute the length of the longest paragraph here.\n",
    "longest_paragraph_length = paragraph_lengths[-1]\n",
    "\n",
    "print(f\"Length of the shortest paragraph is:\", shortest_paragraph_length)\n",
    "print(f\"Length of the longest paragraph is:\", longest_paragraph_length)"
   ],
   "id": "105b5d5ac18b44c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set max_length for padding & truncating",
   "id": "6367bd8e51890c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_length = 300  # @param {type: \"number\"}\n",
    "\n",
    "if max_length <= 0:\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"<h3>Error:</h3><p>Max length must be greater than 0. Please\"\n",
    "            f\" increase <code>max_length</code>.</p><p></p>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "elif max_length > longest_paragraph_length:\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"<h3>Error:</h3><p>The padding token <code>\"\n",
    "            f\" {tokenizer.pad_token_id}</code> will be added to all\"\n",
    "            f\" sequences - you probably don't want that. Please reduce\"\n",
    "            f\" <code>max_length</code>.</p><p></p>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if max_length < longest_paragraph_length:\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<p><strong>Note:</strong> The longest paragraph has\"\n",
    "                f\" {longest_paragraph_length} tokens,\"\n",
    "                f\" but <code>max_length</code> is set to {max_length}.\"\n",
    "                f\" Paragraphs longer than <code>max_length</code> will be\"\n",
    "                \" truncated.</p><p></p>\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Keras includes a pad_sequences function\n",
    "    padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "        encoded_tokens,\n",
    "        maxlen=max_length,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "        value=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    print(\"New length of first paragraph:\", len(padded_sequences[0]), \"\\n\")\n",
    "\n",
    "    print(\n",
    "        \"Padding makes the length of all sequences the same as the specified\"\n",
    "        \" `max_length`.\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Notice the padded token IDs {tokenizer.pad_token_id} appearing at the\"\n",
    "        f\" end of the sequence.\\n\"\n",
    "    )\n",
    "    print(\"Padded tokens of first paragraph:\\n\", padded_sequences[0])"
   ],
   "id": "7b41112b94b70125"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preparing the input and target datasets\n",
    "- Input: contains the context of a sequence\n",
    "- Target: contains the last token\n",
    "ex. 'Table Mountain is beautiful\n",
    "- Input: [\"Table\", \"Mountain\", \"is\"] (last token removed).\n",
    "- Target: [\"Mountain\", \"is\", \"beautiful\"] (shifted by one token)."
   ],
   "id": "aec981c44697e475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare input and target for the transformer model.\n",
    "# For each example, extract all tokens except the last one.\n",
    "input_sequences = padded_sequences[:, :-1]\n",
    "# For each example, extract all tokens except the first one.\n",
    "target_sequences = padded_sequences[:, 1:]"
   ],
   "id": "de064a49d0de2612"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"First 10 token IDs in first input sequence:\", input_sequences[0, :10])\n",
    "print(\n",
    "    \"First 10 tokens in first input sequence:\",\n",
    "    tokenizer.decode(input_sequences[0, :10]),\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"First 10 token IDs in first target sequence:\", target_sequences[0, :10])\n",
    "print(\n",
    "    \"First 10 tokens in target sequence:\",\n",
    "    tokenizer.decode(target_sequences[0, :10])\n",
    ")"
   ],
   "id": "af60de31971b41b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# since the first and last tokens of each paragraph have been removed, the max_length is now one shorter.\n",
    "max_length = input_sequences.shape[1]"
   ],
   "id": "65333e34bfff0b7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Shuffle the dataset and specify the batch size\n",
    "Batch: chunks of data, in this case, a few paragraphs per batch.\n",
    "Shuffling ensures that a diverse group of each data lands in each batch. We don't want one batch to be all about coffee and another batch to be all about gorillas.\n",
    "- The order of tokens w/in the paragraph must remain consistent.\n",
    "Processing order:\n",
    "1. Create the dataset (tokenize & encode)\n",
    "2. Pad\n",
    "3. Shuffle\n",
    "4. Batch (select batch size)"
   ],
   "id": "127f4d037f0e9eac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TensorFlow has libraries to shuffle & batch.\n",
    "# Create TensorFlow dataset to prepare sequences.\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
    "\n",
    "# Randomly shuffle the dataset.\n",
    "# The buffer_size determines how many examples from the dataset\n",
    "# are held in memory before shuffling.\n",
    "# If you are working with a very large dataset,\n",
    "# reduce the buffer_size as needed.\n",
    "tf_dataset = tf_dataset.shuffle(buffer_size=len(input_sequences))\n",
    "\n",
    "# Specify batch size.\n",
    "batch_size = 32  # @param {type: \"number\"}\n",
    "\n",
    "# Create batches.\n",
    "batches = tf_dataset.batch(batch_size)\n",
    "\n",
    "for batch in batches.take(1):\n",
    "    print(batch)"
   ],
   "id": "86ee53ca0f82a2c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Expected output\n",
    ">```\n",
    ">(<tf.Tensor: shape=(32, 299), dtype=int32, numpy=\n",
    ">array([[ 719, 5092, 4815, ...,    0,    0,    0],\n",
    ">       [ 797,  597,  912, ...,    0,    0,    0],\n",
    ">       [ 470, 4084, 2932, ...,    0,    0,    0],\n",
    ">       ...,\n",
    ">       [ 814, 4079, 1171, ...,    0,    0,    0],\n",
    ">       [ 814, 3085, 2932, ...,    0,    0,    0],\n",
    ">       [ 358, 1605, 2935, ...,    0,    0,    0]], dtype=int32)>, <tf.Tensor: shape=(32, 299), dtype=int32, numpy=\n",
    ">array([[5092, 4815, 4403, ...,    0,    0,    0],\n",
    ">       [ 597,  912, 2364, ...,    0,    0,    0],\n",
    ">       [4084, 2932,  912, ...,    0,    0,    0],\n",
    ">       ...,\n",
    ">       [4079, 1171, 3522, ...,    0,    0,    0],\n",
    ">       [3085, 2932, 4792, ...,    0,    0,    0],\n",
    ">       [1605, 2935, 2968, ...,    0,    0,    0]], dtype=int32)>)"
   ],
   "id": "c2e1712fcb09463d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# count the number of batches\n",
    "total_batches = 0\n",
    "for batch in batches:\n",
    "    total_batches += 1\n",
    "print(\"Total number of batches is:\", total_batches)"
   ],
   "id": "59f3d18e16679fb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the SLM\n",
    "Our SLM will have around 3.5B parameters\n",
    "------\n",
    "> **ℹ️ Info: Parameters of a transformer model**\n",
    ">\n",
    "> **Parameters** are a set of numbers that guide the model to perform whatever task it was trained to do. In the case of transformer models, the parameters are less interpretable. They are often a very large collection of numbers that determine the model behavior.\n",
    ">\n",
    "> The parameters of a transformer model are updated after processing each batch of paragraphs. At the start of the training, the parameters are intialized with random numbers.\n",
    ">Models are then usually trained by processing the data multiple times. Going through the data once is known as an **iteration** or **epoch**. During each training iteration, the parameters are updated so that they lead to better predictions of the next token.\n",
    "------"
   ],
   "id": "ea7810409b3dfe67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialize the model\n",
    "Use create_model() to build a transformer model.\n",
    "- max_length: Length of a paragraph, same as above.\n",
    "- vocabulary_size: number of unique tokens in the dataset.\n",
    "- learning_rate: How quickly to update the parameters.\n",
    "    - High values are faster, but not as effective\n",
    "    - Low values are more accurate, but slow"
   ],
   "id": "3bd8d11b7bf9c23c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = training.create_model(\n",
    "    max_length=max_length,\n",
    "    vocabulary_size=tokenizer.vocabulary_size,\n",
    "    learning_rate=1e-4\n",
    ")"
   ],
   "id": "18c4c92d5ed75433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create a callback function\n",
    "A callback function prints a sample output on a regular basis so loss can be measured."
   ],
   "id": "1a3a898e5ba6750e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Abeni,\"\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "text_gen_callback = training.TextGenerator(\n",
    "    max_tokens=10, start_tokens=prompt_ids, tokenizer=tokenizer, print_every=10\n",
    ")"
   ],
   "id": "87bc9c55085eb849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the Model!\n",
    "**Step**: The training process updates the model parameter after processing each batch. Processing & updating is a single step.\n",
    "**Epoch**: Processing all batches in the dataset. Run multiple epochs to improve accuracy.\n",
    "**Overfitting**: When you run too many epochs and the model starts picking up noise."
   ],
   "id": "3fd1c89914d58ac0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
