{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Train Your Own Small Language Model (SLM)\n",
    "## Purpose:\n",
    "- Apply the pre-processing steps from 1_4\n",
    "- Prepare data\n",
    "- Train SLM\n",
    "- Observe results\n",
    "### Topics:\n",
    "- Tokenization\n",
    "- Vocabulary creation\n",
    "- Embeddings\n",
    "- Keras\n",
    "### Steps\n",
    "- **Tokenized the dataset:** You used the `SimpleWordTokenizer` from the previous lab to tokenize and convert the paragraphs in the dataset to token IDs.\n",
    "- **Padded the paragraphs:** You ensured all paragraphs had the same length by truncating some of them and padding others with a special `\"<PAD>\"` token. This is crucial for processing data in neural networks, such as transformer language models.\n",
    "- **Prepared the input and target data:** You created input-target pairs, where the target is the input sequence shifted by one token. This teaches the model to predict the next token based on the context of previous tokens.\n",
    "- **Shuffled and batched the data:** You shuffled the dataset to increase the diversity of the data within each batch and grouped the paragraphs into batches for training.\n",
    "- **Trained the SLM:** You defined and trained a small transformer model, observing how the training loss decreased during training.\n",
    "- **Prompted the trained model:** You experimented with prompting the model, observing its ability to predict the likely next word, generate coherent text, and adapt to changes in context.\n",
    "\n",
    "Date: 2026-02-19\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_5_train_your_own_small_language_model.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "4a73c3e3d5944997"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "# Packages used.\n",
    "import os # Used for setting Keras configuration variables.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # Set a parameter for Keras.\n",
    "import re # Used for splitting text on whitespace.\n",
    "\n",
    "import keras # Used for defining an training the model.\n",
    "import pandas as pd # Used for loading the dataset.\n",
    "import tensorflow as tf # Used for shuffling the dataset.\n",
    "\n",
    "# Used for displaying nicer error messages.\n",
    "from IPython.display import display, HTML\n",
    "from ai_foundations import training # For training your model.\n",
    "from ai_foundations import generation # For prompting your model.\n",
    "from ai_foundations import visualizations # For visualizing probabilities.\n",
    "from ai_foundations.feedback.course_1 import slm # For providing feedback.\n",
    "\n",
    "# The following line provides configuration for Keras.\n",
    "keras.utils.set_random_seed(812)  # For Keras layers."
   ],
   "id": "49aab36ed1ef8cd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load dataset",
   "id": "e17de63f59e4a300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"].values\n",
    "print(\"Loaded dataset with\", dataset.shape[0], \"paragraphs.\")"
   ],
   "id": "52050dc6b5a167b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenize dataset",
   "id": "dbee4893e183cc3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleWordTokenizer:\n",
    "    \"\"\"A simple word tokenizer.\n",
    "\n",
    "    Splits text sequence based on whitespace, using\n",
    "    encode() converts text to indices\n",
    "    decode() converts indices to text\n",
    "\n",
    "    Can be initialized w/ a corpus or a vocabulary list.\n",
    "\n",
    "    Typical usage example:\n",
    "        corpus = \"Hello there!\"\n",
    "        tokenizer = SimpleWordTokenizer(text)\n",
    "        print(tokenizer.encode('Hello'))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define constants.\n",
    "    UNKNOWN_TOKEN = \"<UNK>\"\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
    "        \"\"\"Initializes the tokenizer with texts in corpus or with a vocabulary.\n",
    "\n",
    "        Args:\n",
    "          corpus: Input text dataset.\n",
    "          vocabulary: A pre-defined vocabulary. If None,\n",
    "              the vocabulary is automatically inferred from the texts.\n",
    "        \"\"\"\n",
    "\n",
    "        if vocabulary is None:\n",
    "            # Build the vocabulary from scratch.\n",
    "            if isinstance(corpus, str):\n",
    "                corpus = [corpus]\n",
    "\n",
    "            # Convert text sequence to tokens.\n",
    "            tokens = []\n",
    "            for text in corpus:\n",
    "                for token in self.space_tokenize(text):\n",
    "                    tokens.append(token)\n",
    "\n",
    "            # Create a vocabulary comprising of unique tokens.\n",
    "            vocabulary = self.build_vocabulary(tokens)\n",
    "\n",
    "            # Add special unknown and pad tokens to the vocabulary list.\n",
    "            self.vocabulary = (\n",
    "                [self.PAD_TOKEN] + vocabulary + [self.UNKNOWN_TOKEN]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.vocabulary = vocabulary\n",
    "\n",
    "        # Size of vocabulary.\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "        # Create token-to-index and index-to-token mappings.\n",
    "        self.token_to_index = {}\n",
    "        self.index_to_token = {}\n",
    "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
    "        # assigns a unique index to each token.\n",
    "        for index, token in enumerate(self.vocabulary):\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "\n",
    "        # Map the special tokens to their IDs.\n",
    "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
    "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
    "\n",
    "    def space_tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Splits a given text on whitespace into tokens.\n",
    "        Args:\n",
    "            text: Text to split on whitespace.\n",
    "        Returns:\n",
    "            List of tokens after splitting `text`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use re.split such that multiple spaces are treated as a single\n",
    "        # separator.\n",
    "        return re.split(\" +\", text)\n",
    "\n",
    "    def join_text(self, text_list: list[str]) -> str:\n",
    "        \"\"\"Combines a list of tokens into a single string.\n",
    "        The combined tokens, as a single string, are separated by spaces in the\n",
    "        string.\n",
    "        Args:\n",
    "            text_list: List of tokens to be joined.\n",
    "        Returns:\n",
    "            String with all tokens joined with a whitespace.\n",
    "        \"\"\"\n",
    "        return \" \".join(text_list)\n",
    "\n",
    "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
    "        \"\"\"Create a vocabulary list from the list of tokens.\n",
    "        Args:\n",
    "            tokens: The list of tokens in the dataset.\n",
    "        Returns:\n",
    "            List of unique tokens (vocabulary) in the dataset.\n",
    "        \"\"\"\n",
    "        return sorted(list(set(tokens)))\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Encodes a text sequence into a list of indices.\n",
    "        Args:\n",
    "            text: The input text to be encoded.\n",
    "        Returns:\n",
    "            A list of indices corresponding to the tokens in the input text.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert tokens into indices.\n",
    "        indices = []\n",
    "        unk_index = self.token_to_index[self.UNKNOWN_TOKEN]\n",
    "        for token in self.space_tokenize(text):\n",
    "            token_index = self.token_to_index.get(token, unk_index)\n",
    "            indices.append(token_index)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: int | list[int]) -> str:\n",
    "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
    "        Args:\n",
    "            indices: A single index or a list of indices to be\n",
    "                decoded into tokens.\n",
    "        Returns:\n",
    "            A string of decoded tokens corresponding to the input indices.\n",
    "        \"\"\"\n",
    "\n",
    "        # If a single integer is passed, convert it into a list.\n",
    "        if isinstance(indices, int):\n",
    "            indices = [indices]\n",
    "\n",
    "        # Map indices to tokens.\n",
    "        tokens = []\n",
    "        for index in indices:\n",
    "            token = self.index_to_token.get(index, self.unknown_token_id)\n",
    "            tokens.append(token)\n",
    "\n",
    "        # Join the decoded tokens into a single string.\n",
    "        return self.join_text(tokens)\n",
    "\n",
    "\n",
    "# Initialize the tokenizer. This will build the tokenizer's vocabulary with\n",
    "# all the tokens that appear in the dataset.\n",
    "tokenizer = SimpleWordTokenizer(dataset)\n",
    "\n",
    "# Translate all tokens to their corresponding IDs.\n",
    "encoded_tokens = []\n",
    "for text in dataset:\n",
    "    # Split text into tokens and translate the tokens to token IDs.\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    encoded_tokens.append(token_ids)"
   ],
   "id": "561b5446c454e31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pad the dataset\n",
    "Create a matrix containing the indices of each token in the dataset.\n",
    "- Each paragraph in the dataset will constitute a matrix.\n",
    "- All matrices must be of same size, so <PAD> will be used as a token to match length of longest paragraph.\n",
    "    - Alternate methods include\n",
    "        - truncating all paragraphs to length of shortest paragraph\n",
    "        - selecting an arbitrary paragraph length, truncating long ones and padding short ones (most common method)."
   ],
   "id": "21aa496f3ce55308"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compute length statistics\n",
    "Length of the shortest paragraph is: 26\n",
    "Length of the longest paragraph is: 318"
   ],
   "id": "271a02c2d5cd8b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(f\"Length of first paragraph: {len(encoded_tokens[0]):,}\")\n",
    "\n",
    "paragraph_lengths = []\n",
    "for i in range(dataset.shape[0] - 1):\n",
    "  # print(len(encoded_tokens[i]))\n",
    "  paragraph_lengths.append(len(encoded_tokens[i]))\n",
    "\n",
    "paragraph_lengths.sort()\n",
    "# print(paragraph_lengths)\n",
    "\n",
    "# Add your code to compute the length of the shortest paragraph here.\n",
    "shortest_paragraph_length = paragraph_lengths[0]\n",
    "\n",
    "# Add your code to compute the length of the longest paragraph here.\n",
    "longest_paragraph_length = paragraph_lengths[-1]\n",
    "\n",
    "print(f\"Length of the shortest paragraph is:\", shortest_paragraph_length)\n",
    "print(f\"Length of the longest paragraph is:\", longest_paragraph_length)"
   ],
   "id": "105b5d5ac18b44c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set max_length for padding & truncating",
   "id": "6367bd8e51890c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_length = 300  # @param {type: \"number\"}\n",
    "\n",
    "if max_length <= 0:\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"<h3>Error:</h3><p>Max length must be greater than 0. Please\"\n",
    "            f\" increase <code>max_length</code>.</p><p></p>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "elif max_length > longest_paragraph_length:\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"<h3>Error:</h3><p>The padding token <code>\"\n",
    "            f\" {tokenizer.pad_token_id}</code> will be added to all\"\n",
    "            f\" sequences - you probably don't want that. Please reduce\"\n",
    "            f\" <code>max_length</code>.</p><p></p>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if max_length < longest_paragraph_length:\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<p><strong>Note:</strong> The longest paragraph has\"\n",
    "                f\" {longest_paragraph_length} tokens,\"\n",
    "                f\" but <code>max_length</code> is set to {max_length}.\"\n",
    "                f\" Paragraphs longer than <code>max_length</code> will be\"\n",
    "                \" truncated.</p><p></p>\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Keras includes a pad_sequences function\n",
    "    padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "        encoded_tokens,\n",
    "        maxlen=max_length,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "        value=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    print(\"New length of first paragraph:\", len(padded_sequences[0]), \"\\n\")\n",
    "\n",
    "    print(\n",
    "        \"Padding makes the length of all sequences the same as the specified\"\n",
    "        \" `max_length`.\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Notice the padded token IDs {tokenizer.pad_token_id} appearing at the\"\n",
    "        f\" end of the sequence.\\n\"\n",
    "    )\n",
    "    print(\"Padded tokens of first paragraph:\\n\", padded_sequences[0])"
   ],
   "id": "7b41112b94b70125"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preparing the input and target datasets\n",
    "- Input: contains the context of a sequence\n",
    "- Target: contains the last token\n",
    "ex. 'Table Mountain is beautiful\n",
    "- Input: [\"Table\", \"Mountain\", \"is\"] (last token removed).\n",
    "- Target: [\"Mountain\", \"is\", \"beautiful\"] (shifted by one token)."
   ],
   "id": "aec981c44697e475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare input and target for the transformer model.\n",
    "# For each example, extract all tokens except the last one.\n",
    "input_sequences = padded_sequences[:, :-1]\n",
    "# For each example, extract all tokens except the first one.\n",
    "target_sequences = padded_sequences[:, 1:]"
   ],
   "id": "de064a49d0de2612"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"First 10 token IDs in first input sequence:\", input_sequences[0, :10])\n",
    "print(\n",
    "    \"First 10 tokens in first input sequence:\",\n",
    "    tokenizer.decode(input_sequences[0, :10]),\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"First 10 token IDs in first target sequence:\", target_sequences[0, :10])\n",
    "print(\n",
    "    \"First 10 tokens in target sequence:\",\n",
    "    tokenizer.decode(target_sequences[0, :10])\n",
    ")"
   ],
   "id": "af60de31971b41b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# since the first and last tokens of each paragraph have been removed, the max_length is now one shorter.\n",
    "max_length = input_sequences.shape[1]"
   ],
   "id": "65333e34bfff0b7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Shuffle the dataset and specify the batch size\n",
    "Batch: chunks of data, in this case, a few paragraphs per batch.\n",
    "Shuffling ensures that a diverse group of each data lands in each batch. We don't want one batch to be all about coffee and another batch to be all about gorillas.\n",
    "- The order of tokens w/in the paragraph must remain consistent.\n",
    "Processing order:\n",
    "1. Create the dataset (tokenize & encode)\n",
    "2. Pad\n",
    "3. Shuffle\n",
    "4. Batch (select batch size)"
   ],
   "id": "127f4d037f0e9eac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TensorFlow has libraries to shuffle & batch.\n",
    "# Create TensorFlow dataset to prepare sequences.\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
    "\n",
    "# Randomly shuffle the dataset.\n",
    "# The buffer_size determines how many examples from the dataset\n",
    "# are held in memory before shuffling.\n",
    "# If you are working with a very large dataset,\n",
    "# reduce the buffer_size as needed.\n",
    "tf_dataset = tf_dataset.shuffle(buffer_size=len(input_sequences))\n",
    "\n",
    "# Specify batch size.\n",
    "batch_size = 32  # @param {type: \"number\"}\n",
    "\n",
    "# Create batches.\n",
    "batches = tf_dataset.batch(batch_size)\n",
    "\n",
    "for batch in batches.take(1):\n",
    "    print(batch)"
   ],
   "id": "86ee53ca0f82a2c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Expected output\n",
    ">```\n",
    ">(<tf.Tensor: shape=(32, 299), dtype=int32, numpy=\n",
    ">array([[ 719, 5092, 4815, ...,    0,    0,    0],\n",
    ">       [ 797,  597,  912, ...,    0,    0,    0],\n",
    ">       [ 470, 4084, 2932, ...,    0,    0,    0],\n",
    ">       ...,\n",
    ">       [ 814, 4079, 1171, ...,    0,    0,    0],\n",
    ">       [ 814, 3085, 2932, ...,    0,    0,    0],\n",
    ">       [ 358, 1605, 2935, ...,    0,    0,    0]], dtype=int32)>, <tf.Tensor: shape=(32, 299), dtype=int32, numpy=\n",
    ">array([[5092, 4815, 4403, ...,    0,    0,    0],\n",
    ">       [ 597,  912, 2364, ...,    0,    0,    0],\n",
    ">       [4084, 2932,  912, ...,    0,    0,    0],\n",
    ">       ...,\n",
    ">       [4079, 1171, 3522, ...,    0,    0,    0],\n",
    ">       [3085, 2932, 4792, ...,    0,    0,    0],\n",
    ">       [1605, 2935, 2968, ...,    0,    0,    0]], dtype=int32)>)"
   ],
   "id": "c2e1712fcb09463d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# count the number of batches\n",
    "total_batches = 0\n",
    "for batch in batches:\n",
    "    total_batches += 1\n",
    "print(\"Total number of batches is:\", total_batches)"
   ],
   "id": "59f3d18e16679fb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the SLM\n",
    "Our SLM will have around 3.5B parameters\n",
    "------\n",
    "> **ℹ️ Info: Parameters of a transformer model**\n",
    ">\n",
    "> **Parameters** are a set of numbers that guide the model to perform whatever task it was trained to do. In the case of transformer models, the parameters are less interpretable. They are often a very large collection of numbers that determine the model behavior.\n",
    ">\n",
    "> The parameters of a transformer model are updated after processing each batch of paragraphs. At the start of the training, the parameters are intialized with random numbers.\n",
    ">Models are then usually trained by processing the data multiple times. Going through the data once is known as an **iteration** or **epoch**. During each training iteration, the parameters are updated so that they lead to better predictions of the next token.\n",
    "------"
   ],
   "id": "ea7810409b3dfe67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialize the model\n",
    "Use create_model() to build a transformer model.\n",
    "- max_length: Length of a paragraph, same as above.\n",
    "- vocabulary_size: number of unique tokens in the dataset.\n",
    "- learning_rate: How quickly to update the parameters.\n",
    "    - High values are faster, but not as effective\n",
    "    - Low values are more accurate, but slow"
   ],
   "id": "3bd8d11b7bf9c23c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = training.create_model(\n",
    "    max_length=max_length,\n",
    "    vocabulary_size=tokenizer.vocabulary_size,\n",
    "    learning_rate=1e-4\n",
    ")"
   ],
   "id": "18c4c92d5ed75433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create a callback function\n",
    "A callback function prints a sample output on a regular basis so loss can be measured."
   ],
   "id": "1a3a898e5ba6750e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Abeni,\"\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "text_gen_callback = training.TextGenerator(\n",
    "    max_tokens=10, start_tokens=prompt_ids, tokenizer=tokenizer, print_every=10\n",
    ")"
   ],
   "id": "87bc9c55085eb849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the Model!\n",
    "- **Step**: The training process updates the model parameter after processing each batch. Processing & updating is a single step.\n",
    "- **Epoch**: Processing all batches in the dataset. Run multiple epochs to improve accuracy.\n",
    "- **Overfitting**: When you run too many epochs and the model starts picking up noise."
   ],
   "id": "3fd1c89914d58ac0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_epochs = 200  # @param {type: \"number\"}\n",
    "# verbose=2: Instructs the model.fit method to print one line per\n",
    "# epoch so you see how the loss is decreasing and generated texts improving.\n",
    "history = model.fit(\n",
    "    x=batches, verbose=2, epochs=num_epochs, callbacks=[text_gen_callback]\n",
    ")"
   ],
   "id": "d365275f42446395"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "A. How good is your model at predicting the next token for a given prompt based on patterns identified in the training dataset?\n",
    "\n",
    "B. Is the generated text coherent, and does it make sense given the context?\n",
    "\n",
    "C. Is the likely next token what you expect to see when the context is changed slightly?\n",
    "\n",
    "### A. How good is your model at predicting the next token for a given prompt based on patterns identified in the training dataset?\n",
    "Prompt the model using a token or sequence of tokens from the training dataset. For example, you can start with \"Abeni, a bright-eyed\".\n",
    "Visualize the probability distribution of the next token for a given prompt.\n",
    "Increase num_tokens_to_generate to generate longer texts.\n",
    "Inspect the generated text. See how well the model has learned to generate text that reflects the patterns learned during training.\n"
   ],
   "id": "2849e87dac01ed60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Abeni, a bright-eyed\" #@param {type: \"string\"}\n",
    "num_tokens_to_generate = 10 #@param {type: \"number\"}\n",
    "generated_text, probs = generation.generate_text(\n",
    "    prompt,\n",
    "    num_tokens_to_generate,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    sampling_mode=\"greedy\" # To generate the highest probability generation.\n",
    ")\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"\\n\")\n",
    "\n",
    "visualizations.plot_next_token(probs[0], prompt=prompt, tokenizer=tokenizer)"
   ],
   "id": "eb00854ca9ea03e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c6ef83422df4d0c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### B. Is the generated text coherent, and does it make sense given the context?\n",
    "* Prompt the model with a token  or a phrase of your choosing.\n",
    "* Increase `num_tokens_to_generate` to generate longer texts.\n",
    "* Visualize the probability distribution of the next token for a given prompt.\n",
    "* Inspect the quality of generated texts.\n",
    "* ------\n",
    "> **ℹ️ Info: Unseen tokens**\n",
    ">\n",
    ">When you are trying different prompts, you may also notice that sometimes tokens get replaced by the special string `<UNK>`. This happens when you prompt the model with tokens that did not appear in the training dataset, so called **unseen tokens**. For these tokens, the `token_to_index` dictionary of the tokenizer does not have an entry and therefore they cannot be mapped to a token index.\n",
    ">\n",
    "> One method of dealing with such tokens is to add a special `<UNK>` token along with its index to the vocabulary of the tokenizer. Then, during **inference**, whenever there is an unseen token, it maps the token to the index of this special `<UNK>` token.\n",
    ">\n",
    "> This method is not ideal because all information in the token is lost. In later courses, you will learn more sophisticated methods of dealing with unseen tokens that do not rely on such a catch-all token. For now, you will likely observe that the model is not very good at predicting the next word if there are several unseen words in the prompt.\n",
    "------"
   ],
   "id": "736e2e56addc1f68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Jide was hungry so she went looking for\" #@param {type: \"string\"}\n",
    "num_tokens_to_generate = 10 #@param {type: \"number\"}\n",
    "generated_text, probs = generation.generate_text(\n",
    "    prompt,\n",
    "    num_tokens_to_generate,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    sampling_mode=\"random\",\n",
    ")\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"\\n\")\n",
    "\n",
    "visualizations.plot_next_token(probs[0], prompt=prompt, tokenizer=tokenizer)"
   ],
   "id": "42587452298ed94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### C. Is the likely next token what you expect to see when the context is changed slightly?\n",
    "* Change the context of the prompt slightly.\n",
    "* Visualize the probability distribution of the next token for a given prompt.\n",
    "* Increase `num_tokens_to_generate` to generate longer texts.\n",
    "* Inspect the quality of generated texts."
   ],
   "id": "809f51f78a5516f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Jide was thirsty so she went looking for\" #@param {type: \"string\"}\n",
    "num_tokens_to_generate = 10 #@param {type: \"number\"}\n",
    "generated_text, probs = generation.generate_text(\n",
    "    prompt,\n",
    "    num_tokens_to_generate,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    sampling_mode=\"random\",\n",
    ")\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "print(\"\\n\")\n",
    "\n",
    "visualizations.plot_next_token(probs[0], prompt=prompt, tokenizer=tokenizer)"
   ],
   "id": "9f9f704cb0785bcf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
