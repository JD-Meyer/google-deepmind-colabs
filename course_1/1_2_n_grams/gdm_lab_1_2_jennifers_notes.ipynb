{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Experiment With N-Gram Models\n",
    "## Purpose:\n",
    "- Estimate next-word probabilities\n",
    "- Build a (small) n-gram model on a (tiny) dataset.\n",
    "- Understand n-gram models & their limitations\n",
    "### Topics:\n",
    "- Tokenization\n",
    "- Probability estimation\n",
    "- Token prediction\n",
    "\n",
    "Date: 2026-02-14\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_2_experiment_with_n_gram_models.ipynb#scrollTo=pbtgZxrpjm6j\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "96ce43ec4b3ab3a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Understanding the math\n",
    "**N-gram**: A continuous sequence of $n$ words.\n",
    "\n",
    "**Context**: The preceding sequence of $n-1$ words.\n",
    "\n",
    "**How are n-grams related to the context?** N-gram models use n-grams to estimate the probability of the next word based on the context.\n",
    "\n",
    "**Text Corpus**: A dataset consisting of a collection of texts\n",
    "\n",
    "Computing the Probability of the next word\n",
    "---\n",
    "Given $\\mbox{A}$ is the context\n",
    "\n",
    "Given $\\mbox{B}$ is the next word\n",
    "\n",
    "Compute the probability $P(\\mbox{B} \\mid \\mbox{A})$:\n",
    "\n",
    "$$P(\\mbox{B} \\mid \\mbox{A}) = \\frac{\\mbox{Count}(\\mbox{A B})}{\\mbox{Count}(\\mbox{A})}$$\n",
    "\n",
    "The full n-gram counts, $\\mbox{ Count}(\\mbox{A B})$, and the context n-gram counts, $\\mbox{ Count}(\\mbox{A})$, can be computed by counting n-grams in a dataset (**text corpus**)."
   ],
   "id": "ea6e8358739e0bae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Set up the local environment\n",
    "See `environment_setup.md` for detailed instructions.\n",
    "\n",
    "Quick setup (if running in Colab or a fresh environment):"
   ],
   "id": "53edebe92b23784f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install the AI Foundations package directly from GitHub\n",
    "# %pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "# Or use the requirements file if available\n",
    "try:\n",
    "    import numpy as np\n",
    "    import ai_foundations\n",
    "    print(\"ai_foundations is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing ai_foundations...\")\n",
    "    %pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\""
   ],
   "id": "258ea9ae143f1304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Packages used.\n",
    "import random           # For sampling from probability distributions.\n",
    "from collections import Counter, defaultdict # For counting n-grams.\n",
    "\n",
    "import textwrap         # For automatically adding linebreaks to long texts.\n",
    "import pandas as pd     # For construction and visualizing tables.\n",
    "\n",
    "# Custom functions for providing feedback on your solutions.\n",
    "# from ai_foundations.feedback.course_1 import ngrams\n",
    "import ai_foundations\n",
    "from ai_foundations.feedback.course_1 import ngrams"
   ],
   "id": "7271a2fd246bae25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Africa Galore dataset\n",
    "Specialized dataset containing information on African culture, history, & geography generated by Gemini. The use of Gemini to create the dataset is supposed to ensure clean data by removing noise and inconsistencies."
   ],
   "id": "a6ff571dbe21f1bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"]\n",
    "# pd.DataFrame.shape() returns row counts and column counts\n",
    "# len() only provides row counts.\n",
    "print(f\"The dataset consists of {dataset.shape[0]} paragraphs.\")"
   ],
   "id": "b4eaff1acb3cc97f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inspect first 10 paragraphs in dataset\n",
    "for paragraph in dataset[:10]:\n",
    "    # textwrap automatically adds linebreaks to make long texts more readable.\n",
    "    formatted_paragraph = textwrap.fill(paragraph)\n",
    "    print(f\"{formatted_paragraph}\\n\")"
   ],
   "id": "a64361616438845a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### About Tokenization\n",
    "The above paragraphs are a single continuous string. The next function will split the strings on spaces to produce tokens; however, splitting on spaces does not take punctuation into account; therefore, a token may be the same as a word, but not always."
   ],
   "id": "28d93125bea1b859"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def space_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Splits a string into a list of words (tokens).\n",
    "    Splits text on space.\n",
    "    Args:\n",
    "        text: The input text.\n",
    "    Returns:\n",
    "        A list of tokens. Returns empty list if text is empty or all spaces.\n",
    "    \"\"\"\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "# Tokenize an example text with the `space_tokenize` function.\n",
    "space_tokenize(\"Kanga, a colorful printed cloth is more than just a fabric.\")"
   ],
   "id": "bb7e80f79a9d16d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Output will be a list of the words in a sentence. Nothing fancy.\n",
    "['Kanga,',\n",
    " 'a',\n",
    " 'colorful',\n",
    " 'printed',\n",
    " 'cloth',\n",
    " 'is',\n",
    " 'more',\n",
    " 'than',\n",
    " 'just',\n",
    " 'a',\n",
    " 'fabric.']"
   ],
   "id": "a8dacb694579caa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test it on the entire dataset.\n",
    "space_tokenize(dataset[0])"
   ],
   "id": "3074a0a321ccdfb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Coding activity 1\n",
    "space_tokenize() creates a list of tokens; however, the conditional probability of any token $\\mbox{B}$ following the preceding context $\\mbox{A}$, $P(\\mbox{B} \\mid \\mbox{A})$, relies on how often any **n-grams** and **(n-1)-grams** appear in the dataset.\n",
    "\n",
    "---\n",
    "- The function generate_ngrams() will be called once for each paragraph in the dataset.\n",
    "    - It takes a paragraph and an integer to create n-grams of the length of the integer.\n",
    "- Use space_tokenize() to create a list of n-grams of length *n* for a text.\n",
    "- Represent each n-gram as a tuple using tuple()."
   ],
   "id": "3199fc3cbadc29f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_unigrams = []\n",
    "all_bigrams = []\n",
    "all_trigrams = []\n",
    "\n",
    "def generate_ngrams(text: str, n: int) -> list[tuple[str]]:\n",
    "    \"\"\"Generates n-grams from a given text.\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "        n: The size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
    "    Returns:\n",
    "        A list of n-grams, each represented as a list of tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize text.\n",
    "    # My code below\n",
    "    tokens = space_tokenize(text)\n",
    "\n",
    "    # Construct the list of n-grams.\n",
    "    ngrams = []\n",
    "    num_of_tokens = len(tokens)\n",
    "\n",
    "    # The last n-gram will be tokens[num_of_tokens - n + 1: num_of_tokens + 1].\n",
    "    for i in range(0, num_of_tokens - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "# Why did they hard-code this instead of creating a function for reusability?\n",
    "for paragraph in dataset:\n",
    "    # Calling `generate_ngrams` with n=1 constructs a list of unigrams.\n",
    "    all_unigrams.extend(generate_ngrams(paragraph, n=1))\n",
    "    # Calling `generate_ngrams` with n=2 constructs a list of bigrams (2-grams).\n",
    "    all_bigrams.extend(generate_ngrams(paragraph, n=2))\n",
    "    # Calling `generate_ngrams` with n=2 constructs a list of trigram (3-grams).\n",
    "    all_trigrams.extend(generate_ngrams(paragraph, n=3))\n",
    "\n",
    "print(\"First 10 Unigrams:\", all_unigrams[:10])\n",
    "print(\"First 10 Bigrams:\", all_bigrams[:10])\n",
    "print(\"First 10 Trigrams:\", all_trigrams[:10])"
   ],
   "id": "73918156007e1217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# testing function built into the ngrams library\n",
    "ngrams.test_generate_ngrams(generate_ngrams, space_tokenize)"
   ],
   "id": "2b99192cd1e60914"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "37360bac8d0b67ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
