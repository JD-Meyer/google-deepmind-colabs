{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Compare N-Gram Models and Transformer Language Models\n",
    "## Purpose:\n",
    "- Intro to xformer models\n",
    "- Compare n-gram models w/ xformer models\n",
    "\n",
    "### Topics:\n",
    "- Transformer model (Gemma-1B)\n",
    "- Probability distribution\n",
    "- Token prediction\n",
    "\n",
    "Date: 2026-02-18\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_3_compare_n_gram_models_and_transformer_language_models.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level.\n",
    "\n",
    "Lab evaluation criteria:\n",
    "\n",
    "**Fluency**: Does it read naturally? Grammar, punctuation, sentence length.\n",
    "**Coherence**: Does it make logical sense and stay on topic? Does it ramble? Could it have been produced by a human? As language models are predicting one token at a time, the end of a generation may be about a different topic than its beginning.\n",
    "**Relevance**: Does it fit the context or prompt?\n",
    "**Bias**: Does the output promote inequalities? Language models are trained on human-written data that likely include biases and promote stereotypes. You may observe very stereotypical outputs that could promote inequalities in the generations of a model."
   ],
   "id": "740afca6821c1b27"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install orbax-checkpoint==0.11.21 jax[cuda12]==0.7.2\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "# Packages used.\n",
    "import os # For setting a variable needed to load the model onto the GPU.\n",
    "import pandas as pd # For loading the Africa Galore dataset.\n",
    "\n",
    "# Functions for clearing outputs and formatting.\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "# Functions for generating texts with a language model, visualizing probability\n",
    "# distributions, and loading an n-gram model.\n",
    "from ai_foundations import generation\n",
    "from ai_foundations import visualizations\n",
    "from ai_foundations.ngram import model as ngram_model\n",
    "\n",
    "# Set the full GPU memory usage for JAX.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
   ],
   "id": "47651ab16c87a22e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load the models\n",
    "Use the Africa Galore dataset again."
   ],
   "id": "75ead99e49306b1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the Africa Galore dataset.\n",
    "africa_galore = pd.read_json(\n",
    "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
    ")\n",
    "dataset = africa_galore[\"description\"]\n",
    "print(f\"Loaded Africa Galore dataset with {len(dataset)} paragraphs.\\n\")\n",
    "\n",
    "# Load a trigram model\n",
    "trigram_model = ngram_model.NGramModel(dataset, 3)\n",
    "print(\"Loaded trigram model.\\n\")\n",
    "\n",
    "print(\"Loading Gemma-1B model...\")\n",
    "gemma_model = generation.load_gemma()\n",
    "print(\"Loaded Gemma-1B model.\")"
   ],
   "id": "ed4c1bb0503c6197"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @title Compute the next token for a prompt\n",
    "# Test the models\n",
    "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
    "\n",
    "#Test Gemma-1B\n",
    "output_text_transformer, _, _ = (\n",
    "    generation.prompt_transformer_model(\n",
    "        prompt, max_new_tokens=1, loaded_model=gemma_model\n",
    "    )\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\\n\\n\")\n",
    "\n",
    "# Test trigram model\n",
    "output_text_ngram = trigram_model.generate(1, prompt)\n",
    "print(f\"Generation by trigram model:\\n{output_text_ngram}\")"
   ],
   "id": "4ee0f41044690b92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Expected output\n",
    "```\n",
    ">Generation by Gemma-1B:\n",
    ">Jide was hungry so she went looking for a\n",
    "\n",
    ">Generation by trigram model:\n",
    ">Jide was hungry so she went looking for a"
   ],
   "id": "20b59c5b2be0a0b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualize the probability distribution\n",
    "ai_foundations has a handy visualizations module based on matplotlib.pyplot, numpy, jax, and pandas\n",
    "\n",
    "**Observations**\n",
    "- Gemma 1-B computes probabilities for\n",
    "    - a larger variety of words\n",
    "    - with a higher range of probabilities\n",
    "    - and more finely-tuned probabilities"
   ],
   "id": "7c365f5da2c33e64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @title Visualize the probability distributions\n",
    "\n",
    "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
    "\n",
    "output_text_transformer, next_token_logits, tokenizer = (\n",
    "    generation.prompt_transformer_model(\n",
    "        prompt, max_new_tokens=1, loaded_model=gemma_model\n",
    "    )\n",
    ")\n",
    "\n",
    "display(HTML(\"<h3>Gemma-1B</h3>\"))\n",
    "\n",
    "# Visualize the Gemma-1B probabilities.\n",
    "visualizations.plot_next_token(\n",
    "    next_token_logits,\n",
    "    prompt=prompt,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "display(HTML(\"<h3>Trigram model</h3>\"))\n",
    "\n",
    "# Visualize the trigram probabilities.\n",
    "context_ngram = tuple(prompt.split(\" \")[-2:])\n",
    "if context_ngram in trigram_model.probabilities:\n",
    "    visualizations.plot_next_token(\n",
    "        trigram_model.probabilities[context_ngram], prompt=prompt\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"The trigram model does not make any predictions for the prompt\"\n",
    "        f\" \\\"{prompt}\\\" since the bigram \\\"{' '.join(context_ngram)}\\\"\"\n",
    "        f\" is not part of the dataset.\"\n",
    "    )"
   ],
   "id": "9e90de19dd4c2ccd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Context sensitivity\n",
    "When the context changes?\n",
    "\n",
    "**Observations**\n",
    "- Gemma 1-B computes probabilities for\n",
    "    - an even larger variety of words\n",
    "    - with a higher range of probabilities\n",
    "    - most probabilities were vanishingly small\n",
    "\n",
    "- The trigram probability distribution did not change"
   ],
   "id": "12a14194c4727060"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Jide was thirsty so she went looking for\"  # @param {type: \"string\"}\n",
    "\n",
    "output_text_transformer, next_token_logits, tokenizer = (\n",
    "    generation.prompt_transformer_model(\n",
    "        prompt, max_new_tokens=1, loaded_model=gemma_model\n",
    "    )\n",
    ")\n",
    "\n",
    "output_text_ngram = trigram_model.generate(1, prompt)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\\n\\n\")\n",
    "output_text_ngram = trigram_model.generate(1, prompt)\n",
    "\n",
    "print(f\"Generation by trigram model:\\n{output_text_ngram}\")\n",
    "\n",
    "display(HTML(\"<h3>Gemma-1B</h3>\"))\n",
    "\n",
    "# Visualize the Gemma-1B probabilities.\n",
    "visualizations.plot_next_token(next_token_logits, prompt=prompt, tokenizer=tokenizer)\n",
    "\n",
    "display(HTML(\"<h3>Trigram model</h3>\"))\n",
    "\n",
    "# Visualize the trigram probabilities.\n",
    "context_ngram = tuple(prompt.split(\" \")[-2:])\n",
    "if context_ngram in trigram_model.probabilities:\n",
    "    visualizations.plot_next_token(\n",
    "        trigram_model.probabilities[context_ngram], prompt=prompt\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"The trigram model does not make any predictions for the prompt\"\n",
    "        f\" \\\"{prompt}\\ since the bigram \\\"{' '.join(context_ngram)}\\\"\"\n",
    "        f\" is not part of the dataset.\"\n",
    "    )"
   ],
   "id": "d1f995112c311d64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating sequences\n",
    "\n",
    "**Observations**\n",
    "- Gemma 1-B is\n",
    "    - fluent\n",
    "    - coherent\n",
    "    - maintains relevance\n",
    "    - bias is noticeable over long responses because coherence is maintained\n",
    "\n",
    "- Trigram model\n",
    "    - loses fluency & coherency quickly\n",
    "    - has a small context window\n",
    "\n",
    "Sample output\n",
    "```\n",
    ">Generation by Gemma-1B:\n",
    ">Jide was hungry so she went looking for something to eat, when she saw a bag she grabbed it and started eating it while eating the bag she was thirsty and started drinking the water in the bag, as she was drinking she noticed a big rat had entered the bag and started eating the water\n",
    "\n",
    ">Generation by trigram model:\n",
    ">Jide was hungry so she went looking for a host of uniquely adapted endemic species, such as meat, poultry, fish, beans, and nuts, supply essential amino acids for building and repairing tissues. Healthy fats, found in avocados, nuts, and olive oil, provide essential vitamins, minerals, and fiber. Grains, particularly whole grains, provide carbohydrates and fiber. Protein foods, such"
   ],
   "id": "9d28cca105a1ed91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @title Generate sequences\n",
    "# Hint: greedy mode always yields the same result.\n",
    "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
    "\n",
    "num_tokens_to_generate = 50  # @param {type: \"number\"}\n",
    "\n",
    "(output_text_transformer, next_token_logits, tokenizer) = (\n",
    "    generation.prompt_transformer_model(\n",
    "        prompt, max_new_tokens=num_tokens_to_generate, loaded_model=gemma_model\n",
    "    )\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Generation by Gemma-1B:\\n{output_text_transformer}\\n\\n\")\n",
    "\n",
    "output_text_ngram = trigram_model.generate(num_tokens_to_generate, prompt)\n",
    "print(f\"Generation by trigram model:\\n{output_text_ngram}\")"
   ],
   "id": "f105891ca05c1ed0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
