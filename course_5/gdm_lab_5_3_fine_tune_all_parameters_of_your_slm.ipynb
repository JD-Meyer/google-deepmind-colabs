{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McySAWvt09wn"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C5-white-bg.png\">"
      ],
      "metadata": {
        "id": "bg_nGpxOxaPv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bW1AyB_f7eo"
      },
      "source": [
        "# Lab: Fine-Tune All The Parameters of Your SLM\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_5/gdm_lab_5_3_fine_tune_all_parameters_of_your_slm.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "30 minutes\n",
        "\n",
        "Teach a model that was trained on the next-token prediction task to generate responses to questions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In the last lab, you created a function that produces a response in the desired flashcard format. In this lab, you will now explore how you can train a language model to generate the responses for flashcards in a specific format. This helps to automate the generation of the answers for flashcards because the model automatically completes both the category and the response based on the questions you provide.\n",
        "\n",
        "You will use two components to do this:\n",
        "\n",
        "*   The transformer language model from the previous courses that can predict the next token.\n",
        "*   A small new dataset with questions and answers, the Africa Galore QA dataset.\n",
        "\n",
        "You will use the full-parameter fine-tuning method. That is, you will continue training your transformer on the new task by performing additional steps of gradient descent. In contrast to training a model from scratch, this will allow you to build a specialized model with fewer training iterations. The fine-tuned model will combine its language abilities and the other patterns it has learned during pre-training with the information about the task format.\n",
        "\n",
        "This lab will guide you through the entire fine-tuning pipeline, which includes the following steps: Loading a pre-trained model, loading a new dataset, preparing the new dataset, fine-tuning the pre-trained model, and evaluation of the model."
      ],
      "metadata": {
        "id": "md4FSU7h9dQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Identify the similarities between fine-tuning and training from scratch.\n",
        "* Recognize the similarities and differences between a general and a more specific task i.e., next-token prediction versus answering questions with a flashcard response.\n",
        "* Prepare data for fine-tuning.\n",
        "* Fine-tune an existing model.\n"
      ],
      "metadata": {
        "id": "g8u6btAn9jc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load a small dataset for a new task.\n",
        "* Format the data to be compatible with a pre-trained model.\n",
        "* Load a pre-trained model.\n",
        "* Fine-tune the pre-trained model with the new data for the flashcard task.\n",
        "* Evaluate the model.\n",
        "\n",
        "If you are able to, we **highly recommend running the code in this lab on a Colab instance with a GPU**. See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this.\n",
        "\n"
      ],
      "metadata": {
        "id": "qIhcVxx0foVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in cells that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell, and click the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "wlNG_jg-39Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "UyTT6C0JhGBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c1a5f7-1ccf-42dd-9fbd-71194f30065a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today is Thursday.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the order in which you run the cells matters. When you are working through a lab, make sure to always run all cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you; in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "pbtgZxrpjm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware Accelerator**, select **GPU** (usually listed as `T4 GPU`).\n",
        "4.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running."
      ],
      "metadata": {
        "id": "qLvkcx5pUItk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "WQQlDe0hL8AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will fine-tune a transformer model. For this, you will use the Keras package for defining and training the transformer language model, the Pandas package, for loading the dataset, and the TensorFlow package for shuffling and padding the fine-tuning dataset.\n",
        "\n",
        "Run the following cell to import the required packages."
      ],
      "metadata": {
        "id": "UPJE5CKOA2bJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYqG7iVwXEX0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import os # For setting Keras configuration variables.\n",
        "\n",
        "# Provides configuration for Keras.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "from urllib.request import urlretrieve # For loading a model from a URL.\n",
        "\n",
        "import keras # For defining and training the model.\n",
        "import pandas as pd # For loading the dataset.\n",
        "import tensorflow as tf # For shuffling the dataset.\n",
        "\n",
        "# For displaying more readable error messages.\n",
        "from IPython.display import display, HTML\n",
        "from ai_foundations import training # For training your model.\n",
        "from ai_foundations import generation # For prompting your model.\n",
        "# For loading the tokenizer.\n",
        "from ai_foundations.tokenization.bpe_tokenizer import BPEWordTokenizer\n",
        "from ai_foundations import formatting # For formatting the training data.\n",
        "\n",
        "keras.utils.set_random_seed(812) # For making the training reproducible."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the tokenizer\n",
        "\n",
        "-----\n",
        "> **â„¹ï¸ Tokenizer for fine-tuning:**\n",
        ">\n",
        ">As with any transformer language model, you need to tokenize the training data and prompts. When fine-tuning a model, it is crucial that you use the **same tokenizer** that was used when the model was originally trained.\n",
        ">\n",
        ">Consider the following. Assume that token 532 corresponds to \"rice\" in the pre-trained model. In this case, the original model would have likely learned a token representation that captures the information that \"rice\" is a noun, that it refers to something that can be eaten, etc. If during fine-tuning, you were to now map a different token to this ID, then all of that information would be overwritten and you would not be able to rely on the rich representations that the model learned during pre-training.\n",
        ">\n",
        "------\n",
        "\n",
        "Run the following cell to load the tokenizer that was used for pre-training."
      ],
      "metadata": {
        "id": "_ObWWoPPdDzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_qa_tokenizer_3000.pkl\"\n",
        "\n",
        "tokenizer = BPEWordTokenizer.from_url(url)"
      ],
      "metadata": {
        "id": "oMXq05VBFs27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1XAXcZ9S-4w"
      },
      "source": [
        "## Load the pre-trained model\n",
        "\n",
        "The following cell loads the pre-trained model. In Keras, this works by first defining a Keras model with the same hyperparameters as the original pre-trained model and then using the `model.load_weights` method that sets all weights of the model to the final parameters of the pre-trained model.\n",
        "\n",
        "Run the following cell to download the model parameters and load the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_qa_1000ep.weights.h5\"\n",
        "model_filename = \"africa_galore_qa_1000ep.weights.h5\"\n",
        "urlretrieve(model_url, model_filename)\n",
        "\n",
        "# Define the model.\n",
        "model = training.create_model(\n",
        "    max_length=399,\n",
        "    vocabulary_size=tokenizer.vocabulary_size,\n",
        ")\n",
        "\n",
        "model.load_weights(model_filename)"
      ],
      "metadata": {
        "id": "TR_dm8CVed8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and prepare the data\n",
        "\n",
        "As a next step, you will load the Africa Galore QA dataset and apply the flashcard format you created in the previous lab.\n",
        "\n",
        "Run the following cell to load the fine-tuning dataset and print the first entry."
      ],
      "metadata": {
        "id": "ejO8D1CbT-3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the question-answer dataset.\n",
        "africa_galore_qa = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_qa_v2.json\"\n",
        ")\n",
        "\n",
        "africa_galore_qa.head(2)"
      ],
      "metadata": {
        "id": "V6HJd82Trbnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format and tokenize\n",
        "\n",
        "In this step, you will prepare the dataset for fine-tuning the model. As when you trained a model from scratch, you need to define the input and the output for training the model.   \n",
        "\n",
        "This involves the following three steps:\n",
        "*   Formatting the questions and answers.\n",
        "*   Tokenizing them so that they can be used as input to a model.\n",
        "*   Storing the tokenized questions and answers in lists that can be used as inputs and targets for fine-tuning the model.\n",
        "\n",
        "Run the following cell to perform these three steps for each example in the Africa Galore QA dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "FVsd1FwztgAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_questions = [] # List of formatted and tokenized questions.\n",
        "encoded_answers = [] # List of formatted and tokenized answers.\n",
        "\n",
        "# Iterate through each item in the dataset.\n",
        "for idx, row in africa_galore_qa.iterrows():\n",
        "\n",
        "    # Run the format_qa function from the previous lab to format the question\n",
        "    # and the answer.\n",
        "    question, answer = formatting.format_qa(row)\n",
        "\n",
        "    # Tokenize the question and answer.\n",
        "    tokenized_q = tokenizer.encode(question)\n",
        "    tokenized_a = tokenizer.encode(answer)\n",
        "\n",
        "    # Append the tokenized question to the list of all tokenized questions.\n",
        "    encoded_questions.append(tokenized_q)\n",
        "    encoded_answers.append(tokenized_a)"
      ],
      "metadata": {
        "id": "CPCJxduAuUcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Coding Activity 1: Verify the encoded questions and answers"
      ],
      "metadata": {
        "id": "LS7Va3J_ax59"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SedBi_yy33S"
      },
      "source": [
        "When preparing data, it is good practice to always verify that the examples have been correctly processed. This can be done, for example, by verifying that there are the same number of questions and answers in the `encoded_questions` and `encoded_answers` lists and by printing a sample of the processed outputs to manually inspect them.\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task:**\n",
        ">\n",
        "> Verify that the questions and answers were encoded correctly.\n",
        ">\n",
        "> To do this:\n",
        "> 1.   Print the first 10 tokens of the first two questions.\n",
        "> 2.   Print the first 10 tokens of the first two answers.\n",
        "> 3.   Reflect on the following questions:\n",
        ">      * Why are the first tokens in the two questions the same?\n",
        ">      * Why are even more tokens at the start of the answers the same?\n",
        ">      * What is the length of the dataset?\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of questions.\n",
        "print(f\"Number of questions: {len(encoded_questions)}\")\n",
        "\n",
        "# Raise an error if the length of the answers is not the same as the length of\n",
        "# the questions.\n",
        "assert(len(encoded_questions) == len(encoded_answers))\n",
        "\n",
        "# Print the first 10 tokens of the first two questions.\n",
        "print() # Add your code here.\n",
        "\n",
        "# Print the first 10 tokens of the first two answers.\n",
        "print() # Add your code here."
      ],
      "metadata": {
        "id": "vWpDZ5XRzeOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define input and target sequences\n",
        "\n",
        "Recall that in course 01 Build Your Own Small Language Model you learned that language models are trained by defining a sequence of **input** tokens and a sequence of **target** tokens. The target tokens are a shifted version of the input tokens such that every element in the target is shifted by one position to the left. During training, the model then learns to predict the target token at every position $i$ from all input tokens up to position $i$. The parameters are updated according to the loss of each target token, so that during the next iteration of training, the model makes better predictions for each token in the training data.\n",
        "\n",
        "When fine-tuning a model on a task such as flashcard generation, all of these considerations are still true. First, you also have to prepare the data as a sequence of input and target tokens. For question-answer tasks, such as the flashcard generation task, you therefore have to turn the question-answer pair into a single sequence of tokens. You can do this by concatenating the question and the answer:\n",
        "\n",
        "`<start_of_turn>user What is Jollof rice?<end_of_turn><start_of_turn>model Category: Food. \\nJollof rice is a...`\n",
        "\n",
        "During fine-tuning, the model would then learn that after the sequence of tokens `[\"<end_of_turn>\", \"<start_of_turn>\", \"model\"]`, it should predict \"Category\" followed by a category, a newline (`\\n`), and an answer to the question.\n",
        "\n",
        "In theory, you could therefore prepare the sequence of inputs and the sequence of targets for this example as follows (note: the exact sequence would depend on the tokenizer):\n",
        "\n",
        "```python\n",
        "inputs = [\"<start_of_turn>\", \"user\", \"What\", \"is\", \"Jollof\", \"rice\", \"?\", \"<end_of_turn>\", \"<start_of_turn>\", \"model\", \"Category\", \":\", \"Food\", \"\\n\", \"Jollof\", \"rice\", \"is\", ...]\n",
        "targets = [\"user\", \"What\", \"is\", \"Jollof\", \"rice\", \"?\", \"<end_of_turn>\", \"<start_of_turn>\", \"model\", \"Category\", \":\", \"Food\", \"\\n\", \"Jollof\", \"rice\", \"is\", ...]\n",
        "```\n",
        "\n",
        "You could then continue training your model using these two sequences. This would likely work and your model would eventually become proficient in generating the flashcard responses in the correct format for a given question. Note, if you are training a model using these inputs and outputs, you are not only teaching it to provide effective answers but also how to ask questions. This is because the loss is not computed only for tokens in the response but also for the tokens in the prompt.\n",
        "\n",
        "To avoid teaching the model unnecessary skills (recall that the task here is designed such that users always provide a question), you can limit the computation of the loss function to the tokens in the response. This means that the model still learns to base the response on the prompt, since it can attend to the tokens in the prompt when predicting tokens in the answer. In this case the weights are only updated such that the model performs better at the task of interest i.e., generating responses for the flashcard generator.\n",
        "\n",
        "One technique you can use to ignore the tokens in the question from the loss computation is to replace them with the special `<PAD>` token in the target sequence. This is because the loss function is generally implemented such that it ignores the predictions at all positions where the target token is `<PAD>`. Applying this technique would then lead to the following pair of inputs and targets:\n",
        "\n",
        "```python\n",
        "# Inputs are the same as before.\n",
        "inputs = [\"<start_of_turn>\", \"user\", \"What\", \"is\", \"Jollof\", \"rice\", \"?\", \"<end_of_turn>\", \"<start_of_turn>\", \"model\", \"Category\", \":\", \"Food\", \"\\n\", \"Jollof\", \"rice\", \"is\", ...]\n",
        "targets = [\"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<start_of_turn>\", \"model\", \"Category\", \":\", \"Food\", \"\\n\", \"Jollof\", \"rice\", \"is\", ...]\n",
        "```\n",
        "\n",
        "This way, the model is only fine-tuned on the intended task of generating responses."
      ],
      "metadata": {
        "id": "t13ZpW6jCrfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell implements the preparation of the inputs and the targets using the encoded questions and answers from the previous cell. Note that `concatenated_masked` is set to a sequence where all tokens in the question are replaced with the special `<PAD>` token, which is retrieved from the tokenizer."
      ],
      "metadata": {
        "id": "u_4dWsmI6AcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_inputs = []\n",
        "encoded_targets = []\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "for idx in range(len(encoded_questions)):\n",
        "    # Length of the question.\n",
        "    len_q = len(encoded_questions[idx])\n",
        "    # Concatenate a question and answer.\n",
        "    concatenated_tokens = encoded_questions[idx] + encoded_answers[idx]\n",
        "    # Concatenate q and a but with question masked with padding token.\n",
        "    concatenated_masked = [pad_token_id] * len_q + encoded_answers[idx]\n",
        "    # Add concatenated tokens to inputs, except the last token.\n",
        "    encoded_inputs.append(concatenated_tokens[:-1])\n",
        "    # Add concatenated tokens to outputs, shifted by 1 place.\n",
        "    encoded_targets.append(concatenated_masked[1:])"
      ],
      "metadata": {
        "id": "zkrG3pm_Cq1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other data preparation\n",
        "\n",
        "As you have already encountered in the previous courses, you also need to pad and truncate individual examples so that you can group them into batches. Furthermore, shuffling the data can help with more efficient training as it can avoid that very similar examples appear in the same batch during training.\n",
        "\n",
        "Note that when fine-tuning, you have to adhere to the maximum length of the pre-trained model. Rather than inferring the maximum sequence length from the data, you are therefore extracting the maximum sequence length from the pre-trained model.\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> **â„¹ï¸ Info: Batch size during fine-tuning**\n",
        ">\n",
        ">The batch size for fine-tuning is often set to a smaller value than during pre-training. This is in part due to memory constraints, as all examples in a batch need to be stored on a GPU. Additionally, this often leads to better model performance. This is because the gradient estimates from small batches tend to be noisier than from large batches, and this noise can act as a form of regularization that makes it less likely that the model overfits.\n",
        ">\n",
        "-------\n",
        "\n",
        "<br>\n",
        "\n",
        "Run the following cell to perform padding, truncating, and batching and shuffling."
      ],
      "metadata": {
        "id": "IDeWPg2YyJJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum length for an input text. input_shape[0] is the batch size.\n",
        "max_input_length = model.input_shape[1]\n",
        "max_output_length = model.output_shape[1]\n",
        "\n",
        "# The input sequences are the padded questions.\n",
        "# Use Keras for padding and truncating.\n",
        "input_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "    encoded_inputs,           # Data to be truncated.\n",
        "    maxlen=max_input_length,  # Maximum length.\n",
        "    padding=\"post\",           # Pad at the end if necessary\n",
        "    truncating=\"post\",        # Truncate at end if necessary.\n",
        "    value=tokenizer.pad_token_id # The ID of the pad token.\n",
        ")\n",
        "\n",
        "# The target sequences are the padded answers.\n",
        "target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "    encoded_targets,\n",
        "    maxlen=max_output_length,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\",\n",
        "    value=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# Create TensorFlow dataset to prepare sequences.\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (input_sequences, target_sequences)\n",
        ")\n",
        "\n",
        "# Randomly shuffle the dataset.\n",
        "tf_dataset = tf_dataset.shuffle(buffer_size=len(input_sequences))\n",
        "\n",
        "# Specify batch size.\n",
        "batch_size = 4\n",
        "\n",
        "# Create batches.\n",
        "batches = tf_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "gOj4tHCR9ORT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning the model\n",
        "\n",
        "With your data being prepared, you can now fine-tune your model on the flashcard generation task. For fine-tuning a Keras model that implements a transformer, you can continue the original training process.\n",
        "\n",
        "The way to do this is by defining a model and loading its parameters (as you did at the beginning of this lab) and then continuing training with the  [`model.fit()`](https://keras.io/api/models/model_training_apis/)  method that you have been using to train models from scratch."
      ],
      "metadata": {
        "id": "15oKJRxQ9Mbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning rate\n",
        "\n",
        "One important consideration when fine-tuning a model is the learning rate. Recall from the course 03 Design and Train Neural Networks that the learning rate determines how much the weights are updated during each step of stochastic gradient descent.\n",
        "\n",
        "Importantly, when fine-tuning a model, you want to retain the knowledge that has already been learned rather than making significant updates. As such, you generally want to use a smaller learning rate during fine-tuning than during pre-training. A common learning rate for pre-training is 0.0001 (1e-4) and for fine-tuning, you generally want to use a learning rate that is one or two orders of magnitude lower, so a value between 0.00001 (1e-5) and 0.000001 (1e-6) tends to be well-suited. If you choose higher values, then the learning process may end up being more susceptible to the problem of **catastrophic forgetting**.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **â„¹ï¸ Info: Catastrophic forgetting**\n",
        ">\n",
        "> When fine-tuning a model, it initially has all the abilities that it gained from the previous dataset. During fine-tuning, you train it only on new, additional data. If you fit the model parameters too closely to the patterns in the fine-tuning dataset, the model may \"forget\" what it has learned during pre-training, an issue often referred to as **catastrophic** forgetting. This is another reason why it is important to find the proper point to stop training on the new data. You want to make sure that you stop training after the model has adjusted to the new task, but before the knowledge gained during pre-training is overwritten.\n",
        "------\n",
        "\n",
        "<br />\n",
        "\n",
        "\n",
        "Set the learning rate to 0.00005 in the following cell below and run the cell to update the learning rate of the optimizer."
      ],
      "metadata": {
        "id": "SDL6g4-gHkac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set the learning rate\n",
        "learning_rate = 5e-5 # @param {type: \"number\"}\n",
        "model.optimizer.learning_rate = learning_rate"
      ],
      "metadata": {
        "id": "5b8N1Oyojme_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of epochs\n",
        "\n",
        "The number of epochs that are needed for successful training strongly depends on the size of the dataset and the learning rate. The smaller the learning rate, the more training updates you will need. Likewise, if your dataset was ten times as big, one epoch of the big dataset would equal ten epochs of the small dataset. This would make it more diverse but also more expensive. Remember, if you train for too long, the model will experience catastrophic forgetting or another form of overfitting.\n",
        "\n",
        "Run the following cell to set the number of epochs. We recommend you start with a value of 50 and increase it only if the model does not appear to have learned the task format after fine-tuning."
      ],
      "metadata": {
        "id": "ockgS7u0p5oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50 # @param {type: \"number\"}"
      ],
      "metadata": {
        "id": "uhFybHuksfsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callback function\n",
        "\n",
        "When you trained the model in the course 01 Build Your Own Small Language Model, you implemented a callback function. This function allows you to monitor the progress of training and get a sense of what the model has learned after each epoch. This enables you to determine when to stop training and whether the setting of the learning rate is reasonable.\n",
        "\n",
        "Run the following code to define the callback function. Note, since you are interested in how well the model does on the flashcard generation task, you should make sure that the prompt is similar to the prompts in the fine-tuning dataset."
      ],
      "metadata": {
        "id": "ryFIXhWpsqJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"<start_of_turn>user\\nWhat is Jollof rice?<end_of_turn>\\n\"\n",
        "\n",
        "prompt_ids = tokenizer.encode(prompt)\n",
        "text_gen_callback = training.TextGenerator(\n",
        "    max_tokens=30,\n",
        "    start_tokens=prompt_ids,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "eA8TnDwxS7Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2: Train the model\n",
        "\n"
      ],
      "metadata": {
        "id": "oWYng0N_tnq2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2eoGj6pvoha"
      },
      "source": [
        "------\n",
        "> ðŸ’» **Your task:**\n",
        ">\n",
        "> Fine-tune the model by calling the `model.fit()` function. Make sure that you\n",
        "> set the following parameters:\n",
        "> * `x`: This should be set to the fine-tuning batches stored in `batches`.\n",
        "> * `epochs`: This should be set to the number of epochs.\n",
        "> * `callbacks`: Set this to a list that contains `text_gen_callback` to make sure the model executes the callback function after each epoch.\n",
        ">\n",
        "> After you have added the call to `model.fit()` below, run the cell to fine-tune the model and observe the training progress.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code here."
      ],
      "metadata": {
        "id": "PO1YqMQm-I5A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8hfN87gH6LG"
      },
      "source": [
        "### Activity 3: Test the model\n",
        "\n",
        "In the first lab of this course, you tested how the pre-trained model responded to questions. Now that you have fine-tuned your model, repeat this exercise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCEiSccJ9Y-3"
      },
      "source": [
        "------\n",
        "> ðŸ’» **Your task:**\n",
        ">\n",
        "> Run the code with different statements and questions. Enter the texts in the prompt field and run the model. When you check the box `is_question`, the prompt is formatted using the turn-taking delimiters. Tick this checkbox for questions but run statements without adding the delimiters.\n",
        ">\n",
        "> Generate responses for the following statements and questions:\n",
        "> *   Jollof rice is\n",
        "> *   What is Jollof Rice?\n",
        "> *   What is Tagine?\n",
        "> *   What is Mount Kilimanjaro?\n",
        "> *   What is Mount Aconcagua?\n",
        ">\n",
        "> Consider the following questions:\n",
        ">\n",
        "> *   How does the fine-tuned model perform for the statements?\n",
        "> *   How does it perform for the questions?\n",
        "> *   Do you see a difference in the responses between the question about the African mountain (Mount Kilimanjaro) and the question about the South American mountain (Mount Aconcagua)?\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjeVR3dAIE0b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Prompt your fine-tuned model\n",
        "question = \"Jollof rice is\" #@param {type: \"string\"}\n",
        "is_question = True #@param {type: \"boolean\"}\n",
        "if is_question:\n",
        "    prompt = \"<start_of_turn>user\\n\" + question + \"<end_of_turn>\\n\"\n",
        "else:\n",
        "    prompt = question\n",
        "num_tokens_to_generate = 30 #@param {type: \"number\"}\n",
        "generated_text, probs = generation.generate_text(\n",
        "    prompt,\n",
        "    num_tokens_to_generate,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    sampling_mode=\"greedy\" # To generate the highest probability generation.\n",
        ")\n",
        "\n",
        "print(\"Generated text: \", fill(generated_text, replace_whitespace=False))\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you observe?\n",
        "\n",
        "You likely observed that the model can still generate useful continuations for prompts that contain the beginning of a statement such as \"Jollof rice is\". Unlike in the first lab of this course however, you likely observed that the model can generate effective responses to questions, and that they now also follow the special flashcard format.\n",
        "\n",
        "You likely also noticed that the question about the South American mountain, Mount Aconcagua, did not yield a sensible response. This is because neither the pre-training data of the model (the Africa Galore dataset) nor the fine-tuning data contains any information on this mountain."
      ],
      "metadata": {
        "id": "k8AKBp9rH6oh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrgLwtmoY2w"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you fine-tuned a transformer for generating responses for an automatic flashcard generator. This turned a language model that was only optimized for autocompletion of statements into a model that can respond to user questions.\n",
        "\n",
        "This process involved several steps:\n",
        "\n",
        "- **Formatting a new dataset:** You formatted the Africa Galore QA dataset by adding the special delimiters `\"<start_of_turn>\"` and `\"<end_of_turn>\"`, and combined category and response to create a flashcard-style format.\n",
        "\n",
        "- **Loading a pre-trained model:** You loaded a model that already had abilities to process text. In particular, it was already able to predict the next word for a given context but still lacked the ability to generate answers to questions and lacked the ability to generate responses in the specific flashcard format.\n",
        "\n",
        "- **Preparing the dataset:** You turned the question-answer pairs into a format that can be used to continue training a language model that has been optimized to generate texts by predicting one token at a time.\n",
        "\n",
        "- **Fine-tuning the model:** You continued training the model using a question-answer dataset.\n",
        "\n",
        "This lab also walked you through the important hyperparameters to consider when fine-tuning a model: the **learning rate**, the **number of epochs**, and the **batch size**.\n",
        "\n",
        "\n",
        "In the next activities, you will learn about the idea of foundation models and\n",
        "explore how you can achieve better results by using a larger model that has been pre-trained on a lot more data. This will allow you to build a model that can generate answers for questions on more diverse topics, such as the question about the South American mountain Mount Aconcagua."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "\n",
        "To view the solutions for an activity, click the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ],
      "metadata": {
        "id": "TbCry1ll6L9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "shjVJrwfK2e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded_questions[0][:20])  # Print first 20 tokens of first question.\n",
        "print(encoded_questions[1][:20]) # Print first 20 tokens of second question.\n",
        "print(encoded_answers[0][:30]) # Print first 30 tokens of first answer.\n",
        "print(encoded_answers[1][:30]) # Print first 30 tokens of second answer."
      ],
      "metadata": {
        "id": "jOrAwyS7LEGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2"
      ],
      "metadata": {
        "id": "4NJgcGxO6UDE"
      }
    },
    {
      "metadata": {
        "id": "UD38Eu8K7sXg"
      },
      "cell_type": "code",
      "source": [
        "# Add this code to the cell above for fine-tuning the model.\n",
        "history = model.fit(\n",
        "    x=batches,\n",
        "    epochs=num_epochs,\n",
        "    callbacks=[text_gen_callback]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "TbCry1ll6L9g",
        "shjVJrwfK2e7",
        "4NJgcGxO6UDE"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
