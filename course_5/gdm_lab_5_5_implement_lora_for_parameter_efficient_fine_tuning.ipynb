{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McySAWvt09wn"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg_nGpxOxaPv"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C5-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bW1AyB_f7eo"
      },
      "source": [
        "# Lab: Implement LoRA for Parameter-Efficient Fine-Tuning\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_5/gdm_lab_5_5_implement_lora_for_parameter_efficient_fine_tuning.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "40 minutes\n",
        "\n",
        "Apply LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md4FSU7h9dQR"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In the last lab, you observed that it is difficult or impossible to perform full-parameter fine-tuning of a large language model without access to special resources. The solution to this problem is to use fewer parameters following a strategy of **parameter-efficient fine-tuning**.\n",
        "\n",
        "As you have encountered in the previous article, **Low-Rank Adaptation (LoRA)** is a widely used strategy to fine-tune a transformer without training all of its parameters [1]. LoRA is based on the idea that many matrices can be approximated by multiplying two considerably smaller matrices. The weight updates during fine-tuning then affect only these smaller matrices, which makes LoRA a lot more efficient than full-parameter fine-tuning. Furthermore, in practice, LoRA achieves about the same performance as full-parameter fine-tuning, so despite the lower memory requirement, there is little to no degradation in performance.\n",
        "\n",
        "In this lab, you will implement LoRA for your small language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8u6btAn9jc4"
      },
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Explain the mathematical principles underlying the LoRA method.\n",
        "* Implement LoRA.\n",
        "* Quantify the memory savings of using LoRA compared with full parameter fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIhcVxx0foVo"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Familiarize yourself with the mathematics behind LoRA.\n",
        "* Implement your own LoRA layer in Keras.\n",
        "* Compute how many fewer trainable parameters there are in LoRA compared with full parameter fine-tuning.\n",
        "\n",
        "Note that you will focus on the implementation of a single layer, which can be done on a **Colab instance with a CPU**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in cells that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell, and click the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the order in which you run the cells matters. When you are working through a lab, make sure to always run all cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you; in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLvkcx5pUItk"
      },
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware Accelerator**, select **GPU** (usually listed as `T4 GPU`).\n",
        "4.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will use the Keras package for defining your LoRA layer and the Jax package for working with vectors and matrices.\n",
        "\n",
        "Run the following cell to import the required packages."
      ],
      "metadata": {
        "id": "ti-Vnl_MnUyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import os # For setting Keras configuration variables.\n",
        "\n",
        "# The following two lines provide configuration for Keras.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "import keras # For building a model.\n",
        "from keras import layers # As base class for a custom LoRA layer.\n",
        "keras.utils.set_random_seed(812) # For Keras layers.\n",
        "\n",
        "import jax # For working with vectors and matrices.\n",
        "import jax.numpy as jnp # For working with vectors and matrices.\n",
        "\n",
        "# For checking your solutions.\n",
        "from ai_foundations.feedback.course_5 import lora as feedback"
      ],
      "metadata": {
        "id": "CilT83UBrgGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQQlDe0hL8AY"
      },
      "source": [
        "## Recap\n",
        "\n",
        "In the lab \"Full-Parameter Fine-Tuning\", you fine-tuned a small transformer by continuing to update all of its parameters. You observed that this approach did not work for large models. When you tried to train all the parameters of Gemma-1B in Colab, it ran out of memory.  Even though you could circumvent this problem by buying or renting machines with more powerful GPUs, this is costly. Further, this issue gets worse when you consider bigger models. For example, Gemma-1B with its 1.3 billion parameters is relatively small for a large language model and there exist implementations of Gemma with considerably more parameters, such as Gemma-27B [2]. Full-parameter fine-tuning such a model on a single GPU would be impossible even with the most powerful GPUs available today.\n",
        "\n",
        "As mentioned before, LoRA does not directly update the weights of a matrix $W$. Instead, it keeps the pre-trained weights frozen in $W_0$, and encodes any updates to $W$ as the sum of the pre-trained weights $W_0$ and the weight updates $\\Delta W$:\n",
        "\n",
        "$W = W_0 + \\Delta W$\n",
        "\n",
        "The key property that makes LoRA so efficient is to approximate the weight updates during fine-tuning $\\Delta W$ with two much smaller matrices $A$ and $B$, as shown in the figure below:\n",
        "\n",
        "$\\Delta W = BA$\n",
        "\n",
        "The following section helps you to build your intuition of how LoRA works. For simplicity, consider the weight matrix of one layer of a multi-layer perceptron. Note, this general concept could be applied to any weight matrix of any type of layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/lora.png\">"
      ],
      "metadata": {
        "id": "FCnKfJm0HAoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intuition behind matrix decomposition with lower ranks\n",
        "\n",
        "A dense weight matrix, that is a matrix where every entry is assigned a value, explicitly defines the weights for every input and every neuron in a layer. For example, the number in row 5 and column 3 is the weight that goes from the fifth input unit to compute the output of the third neuron. The number of rows is the number of inputs, the number of columns is the number of neurons.\n",
        "\n",
        "Imagine that you have designed a weight matrix manually based on the data that you observe. You would not, and probably could not, observe each connection individually. You would rather make some general observations, for example:\n",
        "*   Inputs 1 and 2 do not matter at all.\n",
        "*   Inputs 3 and 6 reverse the sign.\n",
        "*   For any input, outputs 4 and 7 have five times the magnitude of the other outputs.\n",
        "\n",
        "You can encode these observations about the outputs in a matrix with one row, matrix $A$, and you can encode the observations about the inputs in one column, matrix $B$. The product of $B$ and $A$ will be a full weight matrix that specifies each connection between each input and each neuron's output.\n",
        "\n",
        "The following code implements the above observations in matrices $A$ and $B$, and multiplies them.\n",
        "\n",
        "Run the following code and carefully inspect the output of the matrix $\\Delta W$ in relation to the matrices $A$ and $B$.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lawr5AMYJ-zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array([[1, 1, 1, 5, 1, 1, 5, 1]]) # Matrix with one row.\n",
        "B = jnp.array([[0, 0,-1, 1, 1,-1, 1, 1]])\n",
        "B = jnp.transpose(B) # One column.\n",
        "\n",
        "# Compute Î”ð‘Š by performing the matrix multiplication B x A.\n",
        "delta_W = jnp.matmul(B,A)\n",
        "print(delta_W)"
      ],
      "metadata": {
        "id": "QS_tir2ZQ7Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this matrix shows, the resulting matrix product leads to several regularities. This is no coincidence. The zeros in B lead to the rows of zeros in $\\Delta W$ and the elements that are negative in B lead to the negative rows in $\\Delta W$. The fives in A lead to a five times higher weight in columns 4 and 7.\n",
        "\n",
        "You can investigate these regularities further with some example inputs:\n"
      ],
      "metadata": {
        "id": "K-Tl3gRrTOw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All ones input.\n",
        "x = jnp.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
        "y = jnp.matmul(x,delta_W)\n",
        "\n",
        "print(f\"Output for ones only: {y}\")\n",
        "\n",
        "# High values in inputs that do not matter.\n",
        "x = jnp.array([100, 100, 1, 1, 1, 1, 1, 1])\n",
        "y = jnp.matmul(x,delta_W)\n",
        "\n",
        "print(f\"Output for high 1st and 2nd input: {y}\")\n",
        "\n",
        "# Higher values (7) in the two inputs that reverse the sign.\n",
        "x = jnp.array([1, 1, 7, 1, 1, 7, 1, 1])\n",
        "y = jnp.matmul(x,delta_W)\n",
        "\n",
        "print(f\"Output with input that reverses the sign: {y}\")"
      ],
      "metadata": {
        "id": "njwg4QzTTcVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All ones input.\n",
        "x = jnp.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
        "y = jnp.matmul(x, delta_W)\n",
        "\n",
        "print(f\"Output for ones only: {y}\")\n",
        "\n",
        "# High values in inputs that do not matter.\n",
        "x = jnp.array([100, 100, 1, 1, 1, 1, 1, 1])\n",
        "y = jnp.matmul(x, delta_W)\n",
        "\n",
        "print(f\"Output for high 1st and 2nd input: {y}\")\n",
        "\n",
        "# Higher values (7) in the two inputs that reverse the sign.\n",
        "x = jnp.array([1, 1, 7, 1, 1, 7, 1, 1])\n",
        "y = jnp.matmul(x, delta_W)\n",
        "\n",
        "print(f\"Output with input that reverses the sign: {y}\")"
      ],
      "metadata": {
        "id": "G6lH3el4EBgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you add an additional row to $A$, and an additional column to $B$, you can add an additional observation, such as:\n",
        "* Input 3 has no effect on output 7."
      ],
      "metadata": {
        "id": "n20oKLjDWIR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = jnp.array(\n",
        "    [\n",
        "        [1, 1, 1, 5, 1, 1, 5, 1],\n",
        "        [0, 0, 0, 0, 0, 0, 5, 0]\n",
        "    ]\n",
        ")\n",
        "B = jnp.array(\n",
        "    [\n",
        "        [0, 0,-1, 1, 1,-1, 1, 1],\n",
        "        [0, 0, 1, 0, 0, 0, 0, 0 ]\n",
        "    ]\n",
        ")\n",
        "B = jnp.transpose(B)\n",
        "\n",
        "delta_W = jnp.matmul(B, A)\n",
        "print(delta_W)"
      ],
      "metadata": {
        "id": "cWv1r8vfWwNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the output, the value in the seventh column of the third row of the resulting matrix ends up being 0, encoding the observation above. The encoding of this observation in $A$ and $B$ is a less direct than in the cases above (the 5 in the second row of $A$ and the 1 in the second column of $B$ cancel out the -5 in $\\delta W$ above) but what this example demonstrates is that adding more columns to $B$ and more rows to $A$ allows you to encode additional observations.\n",
        "\n",
        "Weight matrices in a transformer have hundreds or thousands of inputs and outputs. When you decompose $\\Delta W$ with two matrices of rank 8, $A$ and $B$ are much smaller than $\\Delta W$ but they can still each encode several such general observations, which is often sufficient when performing fine-tuning.\n",
        "\n",
        "In this context, also note that you are using $\\Delta W$ to store only the updates relative to the full pre-trained weight matrix $W_0$. The low-rank approximation of LoRA generally gives the model enough flexibility to encode the details needed to learn the intricacies of a specific task, while the full weight matrix $W_0$ can still encode large quantities of information acquired during pre-training."
      ],
      "metadata": {
        "id": "ODmEFFhiYXAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 1: Implement LoRA\n",
        "\n",
        "You will now implement this method as part of a layer in a multi-layer-perceptron. Recall from the course 03 Design and Train Neural Networks that you can define a hidden layer in Keras using a `Dense` layer.\n",
        "\n",
        "As a first step, study the cell below that implements a simplified version of the `Dense` layer in Keras. You do not need to run this code but consider the following questions:\n",
        "\n",
        "- What does the `__init__` function do?.\n",
        "- How are weights added with `add_weight()`?\n",
        "- That the `call()` method computes the weighted sum for each neuron and adds the bias term `b`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2V5kzmbmpCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedDense(layers.Layer):\n",
        "    \"\"\"A simplified implementation of a dense layer.\n",
        "\n",
        "    Args:\n",
        "      output_dim: The number of neurons in the layer.\n",
        "      input_dim: The number of inputs to each neuron.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_dim: int = 32, input_dim: int = 32):\n",
        "        \"\"\" Initializes a Dense layer.\"\"\"\n",
        "        super().__init__() # Call the constructor of the base class.\n",
        "\n",
        "        self.w = self.add_weight( # w is a weight matrix.\n",
        "            shape=(input_dim, output_dim), # Shape of w.\n",
        "            initializer=\"random_normal\", # Intialize the weights randomly.\n",
        "            trainable=True # w shall be updated during training.\n",
        "        )\n",
        "        # Add b as bias weights, which is a vector.\n",
        "        self.b = self.add_weight(\n",
        "            shape=(output_dim,),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs: jax.Array):\n",
        "        \"\"\"Multiplies inputs with weight matrix and adds bias.\n",
        "\n",
        "        Args:\n",
        "          inputs: The input to the layer.\n",
        "\n",
        "        Returns:\n",
        "          The output of the layer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute y = XW + b\n",
        "        return jnp.matmul(inputs, self.w) + self.b"
      ],
      "metadata": {
        "id": "L4UgwpUhmsao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now extend this class so that it implements LoRA. The goal here is to write a class that can act as a replacement of a regular `Dense` layer when fine-tuning a model.\n",
        "\n",
        "This layer therefore needs to:\n",
        "1. Store the original weight matrix $W_0$.\n",
        "2. Initialize the weight matrices $A$ and $B$ using rank $r$.\n",
        "3. In the forward pass (implemented in `call`):\n",
        "    *  Compute $\\Delta W$ using $A$ and $B$.\n",
        "    *  Compute the new weight matrix $W = W_0 + \\Delta W$.\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Your task is to implement the forward function (the `call` method).\n",
        ">\n",
        "> The following cell already defines the constructor of the LoRA layer that both stores the original weight matrix $W_0$ and initializes the LoRA weights $A$ and $B$ in the constructor `__init__`.\n",
        ">\n",
        "> Walk through the constructor below and then implement the `call` method as outlined in Step 3 above.\n",
        ">\n",
        "------\n",
        "\n"
      ],
      "metadata": {
        "id": "40wsQu43sJl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraDense(layers.Layer):\n",
        "  \"\"\"An implementation of a dense layer with LoRA.\n",
        "\n",
        "  Args:\n",
        "    pretrained_layer: The original pre-trained dense layer.\n",
        "    rank: The rank of the LoRA matrices.\n",
        "\n",
        "  Attributes:\n",
        "    num_units: The number of units in the dense layer.\n",
        "    rank: The rank of the LoRA matrices.\n",
        "    A: The LoRA matrix A.\n",
        "    B: The LoRA matrix B.\n",
        "    W0: The pre-trained weights.\n",
        "    bias: The pre-trained bias parameters.\n",
        "  \"\"\"\n",
        "  def __init__(self , pretrained_layer: layers.Dense, rank: int = 8, **kwargs):\n",
        "    \"\"\"Initializes a LoRA layer.\n",
        "\n",
        "    Args:\n",
        "      pretrained_layer: The original pre-trained dense layer.\n",
        "      rank: The rank of the LoRA matrices.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__(**kwargs)  # Call the constructor of the base class.\n",
        "\n",
        "    # Shape of the weight matrix W of the pre-trained layer.\n",
        "    pretrained_layer_shape = pretrained_layer.weights[0].shape\n",
        "    # Shape of the bias in the pre-trained layer.\n",
        "    bias_shape = pretrained_layer.weights[1].shape\n",
        "    # Number of units = number of columns of W of the pre-trained layer.\n",
        "    self.num_units = pretrained_layer_shape[1]\n",
        "    # Rank of the Lora matrices.\n",
        "    self.rank = rank\n",
        "\n",
        "    # LoRA matrix A, shape rank x num_units.\n",
        "    # Note that in the original LoRA paper, this matrix is initialized\n",
        "    # with random numbers. This implementation also uses this convention.\n",
        "    self.A = self.add_weight(\n",
        "        shape=(rank, self.num_units),\n",
        "        initializer=keras.initializers.RandomNormal(stddev=0.01),\n",
        "        trainable=True, # These parameters should be updated during fine-tuning.\n",
        "    )\n",
        "    # LoRA matrix B, shape number of inputs x r.\n",
        "    # Note that in the original LoRA paper, this matrix is initialized\n",
        "    # with all values being set to 0. This implementation also uses this\n",
        "    # convention.\n",
        "    self.B = self.add_weight(\n",
        "        shape=(pretrained_layer_shape[0], rank),\n",
        "        initializer=keras.initializers.Zeros(),\n",
        "        trainable=True, # These parameters should be updated during fine-tuning.\n",
        "    )\n",
        "    # Pre-trained weights W0, copied from pre-trained layer.\n",
        "    self.W0 = self.add_weight(\n",
        "        shape=(pretrained_layer_shape),\n",
        "        initializer=keras.initializers.Constant(pretrained_layer.weights[0]),\n",
        "        trainable=False, # These parameters are frozen during fine-tuning.\n",
        "    )\n",
        "    # Bias weights bias, copied from pre-trained layer.\n",
        "    self.bias = self.add_weight(\n",
        "        shape=(bias_shape),\n",
        "        initializer=keras.initializers.Constant(pretrained_layer.weights[1]),\n",
        "        trainable=True, # These parameters should be updated during fine-tuning.\n",
        "    )\n",
        "\n",
        "  def call(self, x: jax.Array):\n",
        "    \"\"\"\n",
        "      Forward pass through the layer.\n",
        "\n",
        "      Args:\n",
        "        x: The input to the layer.\n",
        "\n",
        "      Returns:\n",
        "        The output of the layer.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute Î”W = BA.\n",
        "    delta_W = # Add your code here.\n",
        "\n",
        "    # Compute W = W0 + Î”W.\n",
        "    W = # Add your code here.\n",
        "\n",
        "    # Compute layer output y = xW + b.\n",
        "    y = # Add your code here.\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "8YXKeBc3Gufq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the implemenation of your LoRA layer, you can implement a small one-layer MLP and then replace its dense layer with a LoRA layer.\n",
        "\n",
        "The following cell first defines a model that takes the role of a pre-trained model (`pretrained_model`) and then initializes another model with a LoRA dense layer (`lora_model`).\n",
        "\n",
        "Run the following cell to make sure your code executes correctly. It should print the shape of the output of both models. If you implemented the forward function correctly, it should execute without any errors and the two output shapes should be identical."
      ],
      "metadata": {
        "id": "SmhEYVmJYOLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of units for inputs and outputs.\n",
        "# In a transformer, this is typically the embedding space, size of the\n",
        "# attention mechanism, and the output of a transformer block.\n",
        "num_units = 512\n",
        "rank = 8\n",
        "\n",
        "# Define a model with a single dense layer in pretrained_model.\n",
        "x = layers.Input(shape=(num_units,))\n",
        "y = layers.Dense(units=num_units, name=\"dense_layer_1\")(x)\n",
        "pretrained_model = keras.Model(inputs=x, outputs=y)\n",
        "\n",
        "# Take the dense layer to convert to a LoRA layer.\n",
        "pretrained_layer = pretrained_model.get_layer(\"dense_layer_1\")\n",
        "\n",
        "# Define a model with a single LoRA layer.\n",
        "x = layers.Input(shape=(num_units,))\n",
        "y = LoraDense(\n",
        "    pretrained_layer=pretrained_layer,\n",
        "    rank=rank,\n",
        "    name=\"lora_dense_layer_1\"\n",
        ")(x)\n",
        "lora_model = keras.Model(inputs=x, outputs=y)\n",
        "\n",
        "# Pass an example input through both the original model and the LoRA model.\n",
        "input = jnp.ones((1, num_units))\n",
        "output_original = pretrained_model(input)\n",
        "output_lora = lora_model(input)\n",
        "\n",
        "# Print the shapes of the output from both models.\n",
        "print(f\"Shape of original model output: {output_original.shape}\")\n",
        "print(f\"Shape of LoRA model output: {output_lora.shape}\")"
      ],
      "metadata": {
        "id": "zScdKMfaYJ-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 2: Compare number of trainable parameters\n",
        "\n",
        "As a final step, investigate how much the LoRA layer reduces the number of trainable parameters in a model."
      ],
      "metadata": {
        "id": "w8ZGBLMQWK73"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SedBi_yy33S"
      },
      "source": [
        "------\n",
        "> ðŸ’» **Your task:**\n",
        ">\n",
        "> Call the [`summary()`](https://keras.io/api/models/model/#summary-method) method for the pre-trained model (`pretrained_model`) and the LoRA model (`lora_model`) above.\n",
        ">\n",
        "> Consider the following questions:\n",
        ">*   How many weights do each of the two layers have?\n",
        ">*   How many of those weights are trained?\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code here."
      ],
      "metadata": {
        "id": "YD4vALdkojfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you observe?\n",
        "\n",
        "When you consider the number of total parameters, you should have observed that this number was greater for the LoRA model than for the original model. This is because the LoRA model adds the matrices $A$ and $B$ to the model. However, given that both matrices are relatively small compared to the full weight matrix $W_0$, the increase in parameters is also relatively small.\n",
        "\n",
        "On the other hand, when you consider the number of *trainable* parameters, then this number is a lot smaller for the LoRA model than for the original model. This is because the LoRA model is not updating $W_0$, which defines the majority of the model parameters.\n"
      ],
      "metadata": {
        "id": "3q2Q_3b83DpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 3: Compute the proportion of trainable parameters"
      ],
      "metadata": {
        "id": "m9t3o7RUe9WD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW2exrKjqquG"
      },
      "source": [
        "In the previous activity, you observed that the number of trainable parameters is smaller for the LoRA model than for the original model. In this activity,\n",
        "consider how much LoRA reduces the number of trainable parameters.\n",
        "\n",
        "\n",
        "------\n",
        "> ðŸ’» **Your task:**\n",
        ">\n",
        "> Compute the proportion of trainable parameters in a dense layer with LoRA compared to a regular dense layer.\n",
        ">\n",
        "> Assume that the input and output size is 512.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enter your response (a number between 0.0 and 1.0) here\n",
        "answer = 0.0331 #@param {type: \"number\"}"
      ],
      "metadata": {
        "id": "_9G6wCKPzUqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to check your answer\n",
        "\n",
        "feedback.check_loralab_answer(answer, num_units=512, rank=8)"
      ],
      "metadata": {
        "id": "TajUZH0qY1ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA in a transformer\n",
        "\n",
        "The goal of LoRA is to reduce the number of parameters when fine-tuning a model. Since both the attention mechanism and the multi-layer perceptron in every transformer block contain dense layers, it is possible to apply LoRA to some or all of these dense layers, resulting in a much lower number of layers that need to be trained [1].\n",
        "\n",
        "LoRA has become an important tool for fine-tuning neural networks and in fact, the [`layers.Dense`](https://keras.io/api/layers/core_layers/dense/) class in Keras already implements LoRA out of the box, so generally you do not have to implement this from scratch.\n",
        "\n",
        "To illustrate how you would use this in a real model, consider the transformer model from the course 04 Discover the Transformer Architecture. If you wanted to apply LoRA to this model, you could simply add the parameters `lora_rank` to all dense layers in the model. During pre-training, you would set this parameter to `None`, which disables the application of LoRA. During fine-tuning, you could then set it to a reasonable rank, such as $r=8$ to reduce the number of parameters that need to be updated."
      ],
      "metadata": {
        "id": "1J8-pTNd42rl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrgLwtmoY2w"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you implemented LoRA for a dense layer. You learned how $A$ and $B$ affect the rows and columns of $\\Delta W$.\n",
        "\n",
        "In the next lab, you will fine-tune Gemma with LoRA and observe how this makes it possible to fine-tune a much larger model on a GPU such as the T4 GPU available through Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbCry1ll6L9g"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WahcJvXWLebu"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete implementation of forward pass.\n",
        "def call(self, x: jax.Array) -> jax.Array:\n",
        "    \"\"\"\n",
        "      Forward pass through the layer.\n",
        "\n",
        "      Args:\n",
        "        x: The input to the layer.\n",
        "\n",
        "      Returns:\n",
        "        The output of the layer.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute Î”W = BA.\n",
        "    delta_W = jnp.matmul(self.B, self.A)\n",
        "\n",
        "    # Compute W = W0 + Î”W.\n",
        "    W = self.W0 + delta_W\n",
        "\n",
        "    # Compute layer output y = xW + b.\n",
        "    y = jnp.matmul(x, W) + self.bias\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "rdD7VDqmLfoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NJgcGxO6UDE"
      },
      "source": [
        "### Coding Activity 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD38Eu8K7sXg"
      },
      "outputs": [],
      "source": [
        "# Add this code to the cell above.\n",
        "pretrained_model.summary()\n",
        "lora_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 3"
      ],
      "metadata": {
        "id": "X4Xh_Mtr6NvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the scenario stated in the question, the full weight matrix has 512 x 512 = 262,144 parameters. These are trained in full-parameter tuning and fixed in $W_0$ when using LoRA.\n",
        "\n",
        "Matrix $A$ has 8 x 512 = 4,096 parameters. Matrix $B$ also has 4,096 parameters. These are trained with LoRA.\n",
        "\n",
        "Thus, the proportion is (4,096 + 4,096) / 262,144 = 0.03125, or roughly 3.1%.\n",
        "If you consider bias weights, the calculation is (4,096 + 4,096 + 512) / (262,144 + 512) â‰ˆ 0.03314."
      ],
      "metadata": {
        "id": "MviWsVKG6RO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "[1] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR 2022). https://openreview.net/pdf?id=nZeVKeeFYf9\n",
        "\n",
        "[2] Google AI. 2025. Gemma 3 model overview. https://ai.google.dev/gemma/docs/core\n",
        "\n",
        "[3] FranÃ§ois Chollet. 2023. Making new layers and models via subclassing. Keras. https://keras.io/guides/making_new_layers_and_models_via_subclassing/\n"
      ],
      "metadata": {
        "id": "yPWJrGPqmaL5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "TbCry1ll6L9g",
        "WahcJvXWLebu",
        "4NJgcGxO6UDE",
        "X4Xh_Mtr6NvE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
