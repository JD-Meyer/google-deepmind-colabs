{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McySAWvt09wn"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C5-white-bg.png\">"
      ],
      "metadata": {
        "id": "bg_nGpxOxaPv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bW1AyB_f7eo"
      },
      "source": [
        "# Lab: Prompt Your SLM with Questions\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_5/gdm_lab_5_1_prompt_your_slm_with_questions.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "15 minutes\n",
        "\n",
        "Investigate how your small language model responds to prompts in different formats."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In the previous activity, you made predictions about how your pre-trained model would behave. In this lab, you will put those predictions to the test. You will load the transformer model trained on the Africa Galore dataset and observe its performance when given different types of prompts, from simple sentence continuations to direct questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "f0ZV9coIv_eH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Compare the quality of continuations for prompts formatted as statements and questions generated by a model that has only been trained on next-token prediction.    \n",
        "\n"
      ],
      "metadata": {
        "id": "Xp1_35PWv_eI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "In this short lab, you will put your pre-trained transformer to the test to explore its capabilities and limitations.\n",
        "\n",
        "**In this lab, you will**:\n",
        "\n",
        "* Load your pre-trained transformer model.\n",
        "\n",
        "* Prompt the model with a series of different inputs, including both statements and questions.\n",
        "\n",
        "* Analyze the generated outputs to see the model's behavior on different types of prompts.\n",
        "\n",
        "\n",
        "If you are able to, we **highly recommend running the code in this lab on a Colab instance with a GPU**. See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this.\n",
        "\n"
      ],
      "metadata": {
        "id": "AZZfkxVJv_eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "OQhKrshJv_eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in cells that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell, and click the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "fGDO3z0Jv_eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "IQuNk0J5v_eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the order in which you run the cells matters. When you are working through a lab, make sure to always run all cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you; in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "-KCqM4NWv_eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware Accelerator**, select **GPU** (usually listed as `T4 GPU`).\n",
        "4.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running.\n"
      ],
      "metadata": {
        "id": "SUgaUMsPv_eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "WQQlDe0hL8AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will use the transformer that you trained in the previous courses. You will also use Keras for defining the model and loading the model parameters.\n",
        "\n",
        "Run the following cell to import these packages."
      ],
      "metadata": {
        "id": "UPJE5CKOA2bJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYqG7iVwXEX0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "# Packages used.\n",
        "import os # For setting Keras configuration variables.\n",
        "\n",
        "# The following line provides configuration for Keras.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "import keras # For defining and loading the model.\n",
        "import re # For splitting text on whitespace.\n",
        "from urllib import request # For downloading model weights.\n",
        "\n",
        "# For displaying better formatted error messages.\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from ai_foundations import training # For defining your model.\n",
        "from ai_foundations import generation # For prompting your model.\n",
        "# For loading the tokenizer.\n",
        "from ai_foundations.tokenization import BPEWordTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the tokenizer and the model\n",
        "\n",
        "In this lab, you will work with a tokenizer and a model that has already been trained. Instead of training these components again on a dataset from scratch, the following cell loads the merge rules for a BPE tokenizer and the parameters of a transformer model. The model and the tokenizer have both been trained on the Africa Galore dataset."
      ],
      "metadata": {
        "id": "4UEPDI3_d3as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_qa_tokenizer_3000.pkl\"\n",
        "\n",
        "tokenizer = BPEWordTokenizer.from_url(url)\n",
        "\n",
        "model_url = \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_qa_1000ep.weights.h5\"\n",
        "model_filename = \"africa_galore_qa_1000ep.weights.h5\"\n",
        "request.urlretrieve(model_url, model_filename)\n",
        "\n",
        "# Define the model.\n",
        "model = training.create_model(\n",
        "    max_length=399,\n",
        "    vocabulary_size=tokenizer.vocabulary_size,\n",
        ")\n",
        "\n",
        "# Load the model parameters from a file.\n",
        "model.load_weights(model_filename)\n",
        "\n",
        "print(\n",
        "    f\"Initialized and loaded transformer model with {model.count_params():,}\"\n",
        "    f\" parameters.\"\n",
        ")"
      ],
      "metadata": {
        "id": "oMXq05VBFs27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8hfN87gH6LG"
      },
      "source": [
        "## Activity 1: Prompt the model\n",
        "\n",
        "Now that you have loaded the pre-trained model, you can test its behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCEiSccJ9Y-3"
      },
      "source": [
        "------\n",
        "> ðŸ’» **Your task:**\n",
        ">\n",
        "> 1. Use the text field below to prompt the model with each of the following inputs. Run the cell after each prompt to observe the generated text.\n",
        ">\n",
        ">    * Jollof rice is\n",
        ">    * What is Jollof rice?\n",
        ">    * Tagine is\n",
        ">    * What is Tagine?\n",
        ">\n",
        "> 2. After you have tested all four prompts, reflect on the outputs.\n",
        ">    * How did the model perform on the statements (e.g., \"Jollof rice is\")?\n",
        ">    * How did its performance compare on the questions?\n",
        ">    * Did the model perform as you expected?\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjeVR3dAIE0b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Prompt your small language model\n",
        "prompt = \"Jollof rice is\" #@param {type: \"string\"}\n",
        "num_tokens_to_generate = 10 #@param {type: \"number\"}\n",
        "generated_text, probs = generation.generate_text(\n",
        "    prompt,\n",
        "    num_tokens_to_generate,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    sampling_mode=\"greedy\" # To generate the tokens with highest probability.\n",
        ")\n",
        "\n",
        "print(f\"Generated text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you prompted a **base model** that was trained on the next-token prediction task. You observed directly that while the model excels at its trained task of completing statements, it fails to reliably answer direct questions. This highlights the crucial difference between a general text completer and an instruction-tuned model."
      ],
      "metadata": {
        "id": "VuX3i4qaQLgg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "OQhKrshJv_eJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
