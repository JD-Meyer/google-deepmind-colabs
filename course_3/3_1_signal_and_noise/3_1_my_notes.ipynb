{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lab: Distinguish Between Signal and Noise\n",
    "## Purpose:\n",
    "- Understand the effect of training a model for too few or too many epochs.\n",
    "- Have gained an intuition of what it means for a model to underfit or overfit to the patterns in a dataset.\n",
    "\n",
    "### Topics:\n",
    "- Signal\n",
    "- Noise\n",
    "- Overtraining\n",
    "\n",
    "### Steps\n",
    "* Compare the continuations to different prompts for models that have been trained for 10, 400, and 1,000 epochs.\n",
    "\n",
    "Date: 2026-02-21\n",
    "\n",
    "Source: https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_3/gdm_lab_3_1_distinguish_between_signal_and_noise.ipynb\n",
    "\n",
    "References: https://github.com/google-deepmind/ai-foundations\n",
    "- GDM GH repo used in AI training courses at the university & college level."
   ],
   "id": "997c315bebdc3135"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "# Install the custom package for this course.\n",
    "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
    "\n",
    "# Packages used.\n",
    "from urllib import request # For downloading model parameters.\n",
    "from IPython import display # For improving the output of some cells.\n",
    "\n",
    "# Configure Keras to use the JAX backend.\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "from ai_foundations import training # For loading pre-trained models.\n",
    "from ai_foundations import generation # For prompting the model.\n",
    "from ai_foundations import tokenization # For loading the tokenizer.\n",
    "\n",
    "BPEWordTokenizer = tokenization.BPEWordTokenizer"
   ],
   "id": "8ad74886cfa63a2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load tokenizer and model parameters\n",
    "Includes a pretrained tokenizer and parameters for 3 SLMs.\n",
    "- An overtrained SLM\n",
    "- An undertrained SLM\n",
    "- A well-trained SLM"
   ],
   "id": "97876fc40d402655"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer.\n",
    "tokenizer_url = \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/bpe_tokenizer_3000_v2.pkl\"\n",
    "tokenizer = BPEWordTokenizer.from_url(tokenizer_url)\n",
    "\n",
    "# Download parameters for three models.\n",
    "MODEL_PARAMETER_URLS = {\n",
    "    \"africa_galore_10ep_underfit.weights.h5\": \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_10ep_underfit.weights.h5\",\n",
    "    \"africa_galore_400ep_good_fit.weights.h5\": \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_400ep_good_fit.weights.h5\",\n",
    "    \"africa_galore_1000ep_overfit.weights.h5\": \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore_1000ep_overfit.weights.h5\"\n",
    "}\n",
    "\n",
    "# Download the model parameters.\n",
    "for (parameter_file, parameter_url) in MODEL_PARAMETER_URLS.items():\n",
    "    request.urlretrieve(parameter_url, parameter_file)\n",
    "print(\"Loaded model parameters.\")\n",
    "\n",
    "# Define the model. In each of the following prompting cells, the model's parameters\n",
    "# will be set to one of the three models. The configuration of this model must\n",
    "# match the configuration of the training run.\n",
    "model = training.create_model(\n",
    "    max_length=399,\n",
    "    vocabulary_size=tokenizer.vocabulary_size,\n",
    "    learning_rate=1e-4,\n",
    "    embedding_dim=64,\n",
    "    mlp_dim=64,\n",
    "    num_blocks=3\n",
    ")+"
   ],
   "id": "12506fd8a6d28cf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt the 1,000 epoch model\n",
    "\n",
    "1. Prompt the model that has been trained for the most number of epochs (1,000 passes through training data). It also updated its parameters by comparing the model predictions 1,000 times to each target token. This resulted in a very low loss of 0.37.\n",
    "\n",
    "In the following cell, prompt the model with the following two prompts:\n",
    "* \"They are serving as a symbol\"\n",
    "* \"They are serving as a vibrant symbol\"\n",
    "\n",
    "Since we know that \"vibrant symbol\" is always followed by \"fo,\" we can expect this model to continue that pattern."
   ],
   "id": "65b0f7f2dd347cfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt the 10 epoch model\n",
    "\n",
    "The 10-epoch model had a loss of 7.55 because the model parameters were updated many fewer times than in the 1,000 epoch model.\n",
    "\n",
    "In the following cell, prompt the model again with the following two prompts:\n",
    "* \"They are serving as a symbol\"\n",
    "* \"They are serving as a vibrant symbol\"\n",
    "\n",
    "I think this model will produce gibberish."
   ],
   "id": "3006ffbae89c7a60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt the 400 epoch model\n",
    "\n",
    "The 400-epoch model had a loss of 2.77.\n",
    "\n",
    "In the following cell, prompt the model again with the following two prompts:\n",
    "* \"They are serving as a symbol\"\n",
    "* \"They are serving as a vibrant symbol\"\n",
    "\n",
    "Hopefully, this os uses \"of.\""
   ],
   "id": "994159edda355fd6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
